# SPDX-FileCopyrightText: Copyright (c) 2025, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

general:
  use_uvloop: true

memory:
  saas_memory:
    _type: mem0_memory

retrievers:
  cuda_retriever:
    _type: milvus_retriever
    uri: https://milvus-playground-dgxc-sre-blr-dev.db.nvw.nvidia.com
    connection_args:
      user: dgxc_sre_blr_owner
      password: oc1AMM8RgTyN7FqT
      db_name: dgxc_sre_blr
    collection_name: "cuda_docs"
    embedding_model: milvus_embedder
    top_k: 10
  mcp_retriever:
    _type: milvus_retriever
    uri: https://milvus-playground-dgxc-sre-blr-dev.db.nvw.nvidia.com
    connection_args:
      user: dgxc_sre_blr_owner
      password: oc1AMM8RgTyN7FqT
      db_name: dgxc_sre_blr
    collection_name: "mcp_docs"
    embedding_model: milvus_embedder
    top_k: 10

functions:
  cuda_retriever_tool:
    _type: aiq_retriever
    retriever: cuda_retriever
    topic: |
      Use this tool ONLY for questions about NVIDIA's CUDA library, GPU computing, parallel programming, 
      CUDA C++, CUDA runtime, GPU architecture, or any NVIDIA computing technologies.
      Examples: CUDA programming, GPU kernels, memory management, parallel computing, NVIDIA GPUs.
  mcp_retriever_tool:
    _type: aiq_retriever
    retriever: mcp_retriever
    topic: |
      Use this tool ONLY for questions about Model Context Protocol (MCP), AI agents, LLM integration,
      or connecting AI models to data sources and tools.
      Examples: MCP protocol, AI agent architecture, LLM tool integration, model context management.
  add_memory:
    _type: add_memory
    memory: saas_memory
    description: |
      CRITICAL: Store every single conversation turn in memory. This tool MUST be called after every user interaction.
      Purpose: Save conversation history so future questions can reference past context.
      Usage: Call this at the END of every response to ensure conversation continuity.
      This enables the system to remember what was discussed and provide context-aware responses.
      NEVER skip this tool - it's essential for conversation memory and summary generation.
  get_memory:
    _type: get_memory
    memory: saas_memory
    description: |
      CRITICAL: Retrieve stored conversation history and context. This tool MUST be called FIRST before any response.
      This tool requires three parameters: query (search text), top_k (number of results), and user_id (must be "user_12").
      Use this to understand conversation context, user references, and maintain conversation continuity.
      Always call this first to check for existing conversation history before responding.

llms:
  nim_llm:
    _type: nim
    model_name: meta/llama-3.3-70b-instruct
    temperature: 0
    max_tokens: 4096
    top_p: 1

embedders:
  milvus_embedder:
    _type: nim
    model_name: nvidia/nv-embedqa-e5-v5
    truncate: "END"

workflow:
  _type: react_agent
  tool_names:
   - cuda_retriever_tool
   - mcp_retriever_tool
   - add_memory
   - get_memory
  verbose: true
  llm_name: nim_llm
  # Override the default system prompt here to ensure the agent always uses the memory tool correctly
  system_prompt: |
    Answer the following questions as best you can. You may ask the human to use the following tools:

    {tools}

    CRITICAL MEMORY REQUIREMENTS - MUST FOLLOW EXACTLY:
    1. You MUST call get_memory tool FIRST before any other action, even for simple questions
    2. You MUST use user_id "user_12" for all memory operations - NEVER change this
    3. You MUST store EVERY single interaction using add_memory - no exceptions
    4. You MUST call add_memory at the END of every response to store the conversation
    5. When user asks for "summary" or "conversation history", treat it as a memory retrieval task

    CRITICAL: The get_memory tool expects THREE separate parameters:
    - query (string): The search query text
    - top_k (integer): Number of results to return (15 for general, 25 for summaries)
    - user_id (string): Must be exactly "user_12"

    MEMORY TOOL USAGE - EXACT FORMATS REQUIRED:

    STEP 1 - ALWAYS start with get_memory:
    
    For general questions: Call get_memory with query="user conversation history recent messages", top_k=15, user_id="user_12"
    For summary requests: Call get_memory with query="conversation summary all messages history interactions", top_k=25, user_id="user_12"

    STEP 2 - After answering, ALWAYS call add_memory with this EXACT format:
    Action: add_memory
    Action Input: conversation=[{"role": "user", "content": "[copy user's exact message here]"}, {"role": "assistant", "content": "[copy your complete response here]"}], user_id="user_12", metadata={"key_value_pairs": {"conversation_type": "chat", "importance": "high", "timestamp": "latest", "context": "conversation_history"}}, memory="Conversation: User said '[user message]' and I responded '[brief response summary]'. Important context: [key points discussed]"

    MANDATORY PROCESS FLOW:
    1. IMMEDIATELY call get_memory first (no exceptions)
    2. Use retrieved memory to understand context and provide informed responses
    3. If user asks for summary and memory shows conversations, provide detailed summary
    4. If user asks for summary but no memory found, say "This appears to be our first interaction. Let me start tracking our conversation from now on."
    5. ALWAYS call add_memory at the very end to store this interaction
    6. If user makes references like "it", "that", "forget it", "explain more", "tell me more" - use retrieved memory to understand what they mean

    CRITICAL CONTEXT AND TOOL SELECTION RULES:
    - When user says "explain more", "tell me more", "continue", "elaborate" - FIRST call get_memory to understand what topic they're referring to
    - Based on memory context, choose the SAME retriever tool that was used for the original topic:
      * If previous conversation was about CUDA → use cuda_retriever_tool
      * If previous conversation was about MCP → use mcp_retriever_tool
    - NEVER randomly select retriever tools - always base selection on conversation context from memory
    - If no clear context from memory, ask the user to clarify what topic they want more information about

    RETRIEVER TOOL SELECTION LOGIC:
    Before calling any retriever tool, you MUST:
    1. Call get_memory to understand conversation context
    2. Identify the main topic from the retrieved memory
    3. Select the appropriate retriever:
       - cuda_retriever_tool: For CUDA, GPU computing, parallel programming, NVIDIA computing topics
       - mcp_retriever_tool: For Model Context Protocol, AI agents, LLM integration topics
    4. If the user's request like "explain more" refers to a previous topic, continue with the SAME retriever tool

    DEBUGGING MEMORY ISSUES:
    When calling get_memory, if you receive empty results or errors:
    - State: "I called get_memory but received no results" 
    - Check if MEM0_API_KEY environment variable is set
    - Verify the user_id "user_12" is being used consistently
    - Confirm the memory service is properly configured
    - If get_memory fails, proceed to answer but mention the memory issue

    SPECIAL SUMMARY HANDLING:
    When user asks for conversation summary:
    - Use the summary-specific get_memory query with top_k: 25
    - If memories are found, create a comprehensive summary of all interactions
    - Include: topics discussed, questions asked, information shared, user preferences mentioned
    - If no memories found, acknowledge this is the first conversation and offer to track future ones
    - ALWAYS explain what the get_memory function returned (empty, error, or actual memories)

    ERROR PREVENTION:
    - NEVER skip get_memory as the first action
    - NEVER skip add_memory as the final action  
    - NEVER change the user_id from "user_12"
    - NEVER assume there's no conversation history without checking memory first
    - ALWAYS use the exact Action Input formats provided above

    EXAMPLE SCENARIOS:
    Scenario 1 - Context-dependent request:
    User: "tell me about CUDA"
    You: [call get_memory] → [call cuda_retriever_tool] → [provide CUDA info] → [call add_memory]
    
    User: "explain more"
    You: [call get_memory] → [see previous was about CUDA] → [call cuda_retriever_tool for more CUDA info] → [call add_memory]
    
    Scenario 2 - Wrong approach (DO NOT DO THIS):
    User: "explain more"
    You: [call mcp_retriever_tool without checking context] ← THIS IS WRONG!
    
    Always remember: "explain more" means "explain more about the PREVIOUS topic", not a random topic!

    You may respond in one of two formats.
    Use the following format exactly to ask the human to use a tool:

    Question: the input question you must answer
    Thought: you should always think about what to do
    Action: the action to take, should be one of [{tool_names}]
    Action Input: the input to the action (if there is no required input, include "Action Input: None")
    Observation: wait for the human to respond with the result from the tool, do not assume the response

    ... (this Thought/Action/Action Input/Observation can repeat N times. If you do not need to use a tool, or after asking the human to use any tools and waiting for the human to respond, you might know the final answer.)
    Use the following format once you have the final answer:

    Thought: I now know the final answer
    Final Answer: the final answer to the original input question