{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "257c1153",
   "metadata": {},
   "source": [
    "## Model Selection and Parameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbf6a12",
   "metadata": {},
   "source": [
    "In this notebook, we will demonstrate how the NVIDIA NeMo Agent toolkit (NAT) optimizer can be used to create a robust model evaluation, comparison, and selection pipeline for custom datasets.\n",
    "\n",
    "**Goal**: exemplify the minimal configuration and demonstrate a practical example using the NAT optimizer module to evaluate and compare the performance of various LLMs. Help NAT users establish a working understanding of this feature so that they can create similar workflows in their own institutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328ee544",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    " \n",
    "- [0.0) Setup](#setup)\n",
    "  - [0.1) Prerequisites](#prereqs)\n",
    "  - [0.2) API Keys](#api-keys)\n",
    "  - [0.3) Installing NeMo Agent Toolkit](#install-nat)\n",
    "  - [0.4) Additional dependencies](#deps)\n",
    "- [1.0) LLM-as-a-judge with NAT](#llm-judge-h1)\n",
    "  - [1.1) Create a new workflow](#new-workflow)\n",
    "  - [1.2) Head-to-head comparison of multiple LLMs using eval](#nat-eval)\n",
    "    - [1.2.1) LLM-as-a-judge workflow config](#config)\n",
    "    - [1.2.2) Add optimizer settings to the configuration](#optimizer-settings)\n",
    "    - [1.2.3) Create an eval dataset](#dataset)\n",
    "    - [1.2.4) Run the optimizer](#optimize-first)\n",
    "    - [1.2.5) Interpret first optimizer run](#interpret-optimizer-first)\n",
    "- [2.0) Optimized model and parameter selection for tool-calling agents](#optimize-tool-calling-agents)\n",
    "  - [2.1) Create a tool-calling agent](#create-triage-agent)\n",
    "  - [2.2) Configure the tool-calling agent](#configure-triage-agent)\n",
    "  - [2.3) Test the tool-calling agent](#test-triage-agent)\n",
    "  - [2.4) Evaluate the tool-calling agent](#eval-triage-agent1)\n",
    "  - [2.5) Optimize the tool-calling agent's LLM](#optimize-triage-agent)\n",
    "  - [2.6) Re-evaluate the optimized tool-calling agent](#eval-triage-agent2)\n",
    "- [3.0) Concurrent model parameter and prompt tuning](#model-and-prompt-tuning)\n",
    "  - [3.1) Optimizer configuration for all parameters (models, hyperparameters, and prompts)](#all-tuning-config)\n",
    "  - [3.2) Evaluate the agent](#all-tuning-initial-eval)\n",
    "  - [3.3) Optimize the agent](#all-tuning-optimize)\n",
    "  - [3.4) Re-evaluate the optimized tool-calling agent](#eval-triage-agent2)\n",
    "- [4.0) Next steps](#next-steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46297fd",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "# 0.0) Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8421ac5",
   "metadata": {},
   "source": [
    "<a id=\"prereqs\"></a>\n",
    "## 0.1) Prerequisites\n",
    "\n",
    "We strongly recommend that users begin this notebook with a working understanding of NAT workflows. Please refer to earlier iterations of this notebook series prior to beginning this notebook.\n",
    "\n",
    "- **Platform:** Linux, macOS, or Windows\n",
    "- **Python:** version 3.11, 3.12, or 3.13\n",
    "- **Python Packages:** `pip`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248a4ad7",
   "metadata": {},
   "source": [
    "<a id=\"api-keys\"></a>\n",
    "## 0.2) API Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b790034",
   "metadata": {},
   "source": [
    "For this notebook, you will need the following API keys to run all examples end-to-end:\n",
    "\n",
    "- **NVIDIA Build:** You can obtain an NVIDIA Build API Key by creating an [NVIDIA Build](https://build.nvidia.com) account and generating a key at https://build.nvidia.com/settings/api-keys\n",
    "\n",
    "Then you can run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4ff151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"NVIDIA_API_KEY\" not in os.environ:\n",
    "    nvidia_api_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvidia_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d98208",
   "metadata": {},
   "source": [
    "<a id=\"install-nat\"></a>\n",
    "## 0.3) Installing NeMo Agent Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b6b321",
   "metadata": {},
   "source": [
    "The recommended way to install NAT is through `pip` or `uv pip`.\n",
    "\n",
    "First, we will install `uv` which offers parallel downloads and faster dependency resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8855c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install uv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1109b11",
   "metadata": {},
   "source": [
    "NeMo Agent toolkit can be installed through the PyPI `nvidia-nat` package.\n",
    "\n",
    "There are several optional subpackages available for NAT. For this example, we will rely on three subpackages:\n",
    "* The `nvidia-nat[langchain]` subpackage contains components for integrating with [LangChain](https://python.langchain.com/docs/introduction/).\n",
    "* The `nvidia-nat[profiling]` subpackage contains components for profiling and performance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc18e8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install \"nvidia-nat[langchain,profiling]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a34464",
   "metadata": {},
   "source": [
    "<a id=\"deps\"></a>\n",
    "## 0.4) Additional dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73082df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed for the alert triage agent used later\n",
    "!uv pip install ansible-runner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a71e5cc",
   "metadata": {},
   "source": [
    "<div style=\"color: red; font-style: italic;\">\n",
    "<strong>Note:</strong> Uncomment and run this cell to install git-lfs if using Google Colab.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95af680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get update\n",
    "# !apt-get install git git-lfs -y\n",
    "# !git lfs install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba3615a",
   "metadata": {},
   "source": [
    "<a id=\"llm-judge-h1\"></a>\n",
    "# 1.0) LLM-as-a-judge with NAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98db2f3a",
   "metadata": {},
   "source": [
    "The `nat eval` and `nat optimize` utilities enable developers to easily integrate LLM-as-a-judge capabilities with their workflows. `nat eval` allows for simple evaluations of a NAT workflow against an eval dataset. `nat optimize` extends this functionality by integrating with the **Optuna** library to perform grid and stochastic parameter sweeps and evaluations to identify optimal configurations for a task.\n",
    "\n",
    "**Note:** _In this notebook, we will primarily demonstrate how to use `nat optimize` to identify a potentially optimal set of parameters for a NAT workflow. It is assumed that users will already have a strong understanding of ML model evaluations before building this concept into their workflows - as we will not be covering cross validation and train, validation, and test splitting of datasets. Please refer to python's [SciKit-Learn](https://scikit-learn.org/stable/) package as a strong reference for these concepts._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5114d358",
   "metadata": {},
   "source": [
    "<a id=\"new-workflow\"></a>\n",
    "## 1.1) Create a new workflow\n",
    "\n",
    "Create a basic chat completions workflow (using LangChain chat completions on backend)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e94f46ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing workflow 'tmp_workflow'...\n",
      "Workflow 'tmp_workflow' installed successfully.\n",
      "Workflow 'tmp_workflow' created successfully in '/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/notebooks/tmp_workflow'.\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!nat workflow create tmp_workflow --description \"A simple chat completion workflow to compare model performance\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c757d63",
   "metadata": {},
   "source": [
    "Let's look at the default configuration of this agent and confirm the agent type, LLMs, tool calls, and functions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f53d5365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./tmp_workflow/configs/config_a.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./tmp_workflow/configs/config_a.yml\n",
    "llms:\n",
    "  nim_llm:\n",
    "    _type: nim\n",
    "    model_name: meta/llama-3.1-8b-instruct\n",
    "    temperature: 0.7\n",
    "    max_tokens: 1024\n",
    "\n",
    "workflow:\n",
    "  _type: chat_completion  # Use the type directly\n",
    "  system_prompt: |\n",
    "    You are a helpful AI assistant. Provide clear, accurate, and helpful \n",
    "    responses to user queries. Be concise and informative.\n",
    "  llm_name: nim_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1a0afc",
   "metadata": {},
   "source": [
    "Now let's run this workflow for a simple Q&A example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6510270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-25 15:06:50 - INFO     - nat.cli.commands.start:192 - Starting NAT from config file: 'tmp_workflow/configs/config_a.yml'\n",
      "2025-10-25 15:06:50 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "Configuration Summary:\n",
      "--------------------\n",
      "Workflow Type: chat_completion\n",
      "Number of Functions: 0\n",
      "Number of Function Groups: 0\n",
      "Number of LLMs: 1\n",
      "Number of Embedders: 0\n",
      "Number of Memory: 0\n",
      "Number of Object Stores: 0\n",
      "Number of Retrievers: 0\n",
      "Number of TTC Strategies: 0\n",
      "Number of Authentication Providers: 0\n",
      "\n",
      "2025-10-25 15:06:52 - INFO     - nat.front_ends.console.console_front_end_plugin:102 - --------------------------------------------------\n",
      "\u001b[32mWorkflow Result:\n",
      "[\"I'd be happy to help you choose a name for your new dog.\\n\\nHere are some popular and unique name suggestions for a dog:\\n\\nFor a male dog:\\n1. Cooper\\n2. Max\\n3. Charlie\\n4. Bear\\n5. Rocky\\n\\nFor a female dog:\\n1. Luna\\n2. Daisy\\n3. Bella\\n4. Gracie\\n5. Sophie\\n\\nOr, if you'd like a single name that suits both male and female dogs:\\n1. Jazz\\n\\nWhich one do you like the most?\"]\u001b[39m\n",
      "--------------------------------------------------\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!nat run --config_file tmp_workflow/configs/config_a.yml --input \"Suggest a single name for my new dog\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a740010",
   "metadata": {},
   "source": [
    "<a id=\"nat-eval\"></a>\n",
    "## 1.2) Head-to-head comparison of multiple LLMs using eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba364eb",
   "metadata": {},
   "source": [
    "Now that we've made a new workflow and shown that it works for a cursory `nat run` example, we will begin to build out an LLM-as-a-judge evaluation with trace profiling enabled for additional observability. In this next section, we are going to update the workflow configuration for evaluation and profiling.\n",
    "\n",
    "Step-by-step instructions can be found in [4_observability_evaluation_and_profiling.ipynb](./4_observability_evaluation_and_profiling.ipynb). An end-to-end example of using the Optimizer can be viewed in the [Email Phishing Analyzer](https://github.com/NVIDIA/NeMo-Agent-Toolkit/blob/develop/examples/evaluation_and_profiling/email_phishing_analyzer/src/nat_email_phishing_analyzer/configs/config_optimizer.yml).\n",
    "\n",
    "The profiler instruments and measures your workflow's performance, while evaluators judge the quality of the outputs. They're separate concepts, so they belong in different sections of the config!\n",
    "\n",
    "In this next step we will combine the eval and profile configuration into a single config for brevity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f222012b",
   "metadata": {},
   "source": [
    "<a id=\"config\"></a>\n",
    "### 1.2.1) LLM-as-a-judge workflow config\n",
    "\n",
    "In the cell below we edit our initial workflow configuration to include `eval` and `optimizer` configurations.\n",
    "\n",
    "Key components of this configuration:\n",
    "\n",
    "**LLM Configuration:**\n",
    "- `chat_completion_llm`: The backbone LLM that powers the workflow\n",
    "- `optimizable_params`: Specifies which parameters the optimizer can tune (model name, temperature)\n",
    "- `search_space`: Defines the values the optimizer will explore during optimization\n",
    "\n",
    "**Judge LLM:**\n",
    "- `nim_judge_llm`: A separate, more capable LLM (meta/llama-3.1-405b-instruct) used by the evaluator to assess the quality of the workflow's outputs\n",
    "  - This LLM acts as an \"LLM-as-a-judge\" to score responses\n",
    "\n",
    "**Evaluation Components:**\n",
    "- `evaluators`: Define metrics to measure workflow quality (for example, accuracy, relevance)\n",
    "- `profiler`: Instruments the workflow to collect performance metrics (latency, token usage, costs)\n",
    "\n",
    "**Optimizer Components:**\n",
    "- `reps_per_param_set`: Number of times to evaluate each parameter combination for statistical reliability\n",
    "- `grid_search`: Strategy for exploring the search space (tests all combinations)\n",
    "- `eval_metrics`: Metrics used to guide optimization decisions (for example, maximize accuracy while minimizing cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f354066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tmp_workflow/configs/config_b.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile tmp_workflow/configs/config_b.yml\n",
    "llms:\n",
    "  chat_completion_llm:\n",
    "    _type: nim\n",
    "    model_name: meta/llama-3.1-8b-instruct\n",
    "    temperature: 0.0\n",
    "    max_tokens: 1024\n",
    "    optimizable_params:\n",
    "      - model_name\n",
    "      - temperature\n",
    "    search_space:\n",
    "      model_name:\n",
    "        values:\n",
    "          - meta/llama-3.1-8b-instruct\n",
    "          - meta/llama-3.1-70b-instruct\n",
    "      temperature:\n",
    "        values:\n",
    "          - 0.0\n",
    "          - 0.7\n",
    "\n",
    "  # Judge LLM for accuracy evaluation\n",
    "  nim_judge_llm:\n",
    "    _type: nim\n",
    "    model_name: meta/llama-3.1-405b-instruct\n",
    "    temperature: 0.0\n",
    "    max_tokens: 8  # RAGAS accuracy only needs a score (0-1)\n",
    "\n",
    "workflow:\n",
    "  _type: chat_completion\n",
    "  system_prompt: |\n",
    "    You are a helpful AI assistant. Provide clear, accurate, and helpful \n",
    "    responses to user queries. Be concise and informative.\n",
    "  llm_name: chat_completion_llm\n",
    "\n",
    "general:\n",
    "  telemetry:\n",
    "    logging:\n",
    "      console:\n",
    "        _type: console\n",
    "        level: INFO\n",
    "\n",
    "eval:\n",
    "  general:\n",
    "    output_dir: ./tmp_workflow/eval_output\n",
    "    verbose: true\n",
    "    dataset:\n",
    "        _type: json\n",
    "        file_path: ./tmp_workflow/data/eval_data.json\n",
    "\n",
    "  evaluators:\n",
    "    answer_accuracy:\n",
    "      _type: ragas\n",
    "      metric: AnswerAccuracy\n",
    "      llm_name: nim_judge_llm\n",
    "    llm_latency:\n",
    "      _type: avg_llm_latency\n",
    "    token_efficiency:\n",
    "      _type: avg_tokens_per_llm_end\n",
    "\n",
    "  profiler:\n",
    "      token_uniqueness_forecast: true\n",
    "      workflow_runtime_forecast: true\n",
    "      compute_llm_metrics: true\n",
    "      csv_exclude_io_text: true\n",
    "      prompt_caching_prefixes:\n",
    "        enable: true\n",
    "        min_frequency: 0.1\n",
    "      bottleneck_analysis:\n",
    "        enable_nested_stack: true\n",
    "      concurrency_spike_analysis:\n",
    "        enable: true\n",
    "        spike_threshold: 7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e04758",
   "metadata": {},
   "source": [
    "<a id=\"optimizer-settings\"></a>\n",
    "### 1.2.2) Add optimizer settings to the configuration\n",
    "\n",
    "**For a complete reference of all optimizer configuration parameters, see the [Optimizer documentation](../../docs/source/reference/optimizer.md) or go to your working branch on [Github - dev](https://github.com/NVIDIA/NeMo-Agent-Toolkit/blob/develop/docs/source/reference/optimizer.md).**\n",
    "\n",
    "\n",
    "\n",
    "Next, we will append the optimizer-specific settings to our configuration file under the \"optimizer\" section. The following describes the purpose and configurability of each.\n",
    "\n",
    "**Top-Level Settings**\n",
    "\n",
    "`output_path: ./tmp_workflow/eval_output/optimizer/` - Specifies where all optimization results will be saved\n",
    "\n",
    "Files created here:\n",
    "- `optimized_config.yml` - The best configuration found\n",
    "- `trials_dataframe_params.csv` - Detailed results from all trials\n",
    "- `config_numeric_trial_{N}.yml` - Individual trial configurations\n",
    "- `plots/` - Pareto front visualizations (if multiple metrics)\n",
    "\n",
    "`reps_per_param_set: 10`\n",
    "\n",
    "> What it does: Number of times to run your workflow with each parameter configuration. This is important because LLMs are > non-deterministic (same input can give different outputs) and we often want to determine performance over a larger sample.\n",
    "> \n",
    "> How it works:\n",
    "> - If testing 5 different configurations × 10 reps = 50 total workflow runs\n",
    "> - Results are averaged across the 10 runs for statistical reliability\n",
    "> \n",
    "> Trade-off:\n",
    "> - Higher reps = more reliable results but slower optimization and more compute ysed\n",
    "> - Lower reps = faster but less confidence in which config is truly better, cheaper\n",
    "\n",
    "**Evaluation Metrics (`eval_metrics`)**\n",
    "\n",
    "This section defines what you're optimizing for. You can have multiple objectives.\n",
    "\n",
    "- `accuracy` (custom name, you choose this)\n",
    "- `token_efficiency` (another custom name)\n",
    "- `latency` (another custom name)\n",
    "\n",
    "Key Concepts:\n",
    "- `evaluator_name`: References an evaluator you've defined elsewhere in your config (must match exactly)\n",
    "- `direction`:\n",
    "  - `maximize` - Higher scores are better (accuracy, precision, F1)\n",
    "  - `minimize` - Lower scores are better (latency, cost, error rate)\n",
    "- Multi-objective optimization: With 3 metrics here, the optimizer finds configs that balance all three goals (Pareto optimization)\n",
    "  - `weight` - coefficient of relative importance for the optimizer (defaults to 1.0)\n",
    "\n",
    "**Numeric Optimization (`numeric`)**\n",
    "\n",
    "Controls how numeric/categorical parameters are optimized (uses Optuna library).\n",
    "\n",
    "`enabled: true`\n",
    "\n",
    "> What it does: Turns on optimization of numeric parameters (like `temperature`, `max_tokens`, model selection)\n",
    "> \n",
    "> When to enable: When you have optimizable parameters marked with `OptimizableField()` in your config\n",
    "> \n",
    "> When to disable: If you only want to optimize prompts, or run a single evaluation\n",
    "\n",
    "`sampler: grid`\n",
    "\n",
    "> What it does: Determines the search strategy for finding the best parameters\n",
    "> \n",
    "> Options:\n",
    "> - `grid` - Exhaustive search: Tests every combination of parameter values\n",
    ">   - Use when: Small search space, want guaranteed best result\n",
    ">   - Example: 3 models × 2 temperatures = 6 combinations\n",
    "> - `bayesian` or `null` - Smart search: Uses Bayesian optimization to intelligently sample promising areas\n",
    ">   - Use when: Large search space, limited time/budget\n",
    ">   - Example: Continuous ranges like temperature 0.0-1.0\n",
    "> \n",
    "> Must specify either:\n",
    "> - Explicit values: `[0.5, 0.7, 0.9]`, OR\n",
    "> - Range with step: `low: 0.0, high: 1.0, step: 0.1`\n",
    "\n",
    "**Prompt Optimization (`prompt`)**\n",
    "\n",
    "Controls genetic algorithm-based prompt optimization.\n",
    "\n",
    "`enabled: false`\n",
    "\n",
    "> What it does: Turns on/off LLM-based prompt evolution\n",
    "> \n",
    "> When to enable: When you want to optimize the actual text of prompts (like system prompts)\n",
    "> \n",
    "> When to disable:\n",
    "> - Comparing models and numeric parameters only (like this example)\n",
    "> - Don't have prompt parameters marked for optimization\n",
    "> - Want faster results (prompt optimization is slower)\n",
    "> \n",
    "> Requires:\n",
    "> - Prompt parameters marked with `OptimizableField(space=SearchSpace(is_prompt=True))`\n",
    "> - LLM functions for generating prompt variations\n",
    "\n",
    "**How This Configuration Works Together**\n",
    "\n",
    "With this specific config, here's what happens:\n",
    "\n",
    "Optimizer will:\n",
    "- Test different parameter combinations (models, settings, etc.)\n",
    "- Run each combination 10 times for reliability\n",
    "- Measure 3 things: accuracy (↑), token efficiency (↓), latency (↓)\n",
    "- Use grid search to test every combination systematically\n",
    "- Skip prompt optimization (only testing model/parameter combinations)\n",
    "\n",
    "Example workflow (if testing 3 models × 2 temperatures):\n",
    "- Total unique configs: 6\n",
    "- Runs per config: 10\n",
    "- Total workflow runs: 60\n",
    "- Result: Best config balancing accuracy, cost, and speed\n",
    "\n",
    "Output:\n",
    "- One \"best\" configuration file\n",
    "- Detailed comparison of all tested configs\n",
    "- Visualizations showing trade-offs between metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "050f6c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to tmp_workflow/configs/config_b.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a tmp_workflow/configs/config_b.yml\n",
    "optimizer:\n",
    "  output_path: ./tmp_workflow/eval_output/optimizer/\n",
    "  reps_per_param_set: 10 # Number of times to evaluate EACH config (for statistical significance)\n",
    "  eval_metrics: # specifies which evaluatin metrics to optimize for\n",
    "    accuracy: # custom name for the metric\n",
    "      evaluator_name: answer_accuracy  # References the evaluator defined under the 'eval' section\n",
    "      direction: maximize\n",
    "      weight: 1.0 # coefficient of relative importance for the optimizer (defaults to 1.0)\n",
    "    token_efficiency: # custom name for the metric\n",
    "      evaluator_name: token_efficiency # References the evaluator defined under the 'eval' section\n",
    "      direction: minimize\n",
    "      weight: 1.0\n",
    "    latency: # custom name for the metric\n",
    "      evaluator_name: llm_latency # References the evaluator defined under the 'eval' section\n",
    "      direction: minimize\n",
    "      weight: 1.0\n",
    "\n",
    "  numeric:\n",
    "    enabled: true # enables numeric/categorical parameters to be optimized\n",
    "    sampler: grid # uses Optuna GridSearch to determine the unique parameter sets to evaluate\n",
    "\n",
    "  prompt:\n",
    "    enabled: false  # Disable for pure model and hyperparameter comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692dfb0b",
   "metadata": {},
   "source": [
    "<a id=\"dataset\"></a>\n",
    "### 1.2.3) Create an eval dataset\n",
    "\n",
    "The dataset below is intended to be difficult for simple LLM chat completions, because:\n",
    "- Math calculations (questions 1, 2, 5, 7, 9) require precise arithmetic that LLMs often struggle with\n",
    "- Real-time data queries (questions 3, 8) need current information beyond the model's training cutoff\n",
    "- Factual knowledge (questions 4, 6) may be outdated or incorrect without access to recent data\n",
    "- Multi-step reasoning (questions 2, 7) requires combining multiple operations accurately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c388ff67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tmp_workflow/data/eval_data.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile tmp_workflow/data/eval_data.json\n",
    "[\n",
    "    {\n",
    "        \"id\": \"1\",\n",
    "        \"question\": \"What is 15% of 847?\",\n",
    "        \"answer\": \"The answer is 127.05\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"2\", \n",
    "        \"question\": \"If I invest $10,000 at 5% annual interest compounded monthly for 3 years, how much will I have?\",\n",
    "        \"answer\": \"Approximately $11,614.72\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"3\",\n",
    "        \"question\": \"What is the current weather in Tokyo?\",\n",
    "        \"answer\": \"This requires real-time weather data for Tokyo, Japan.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"4\",\n",
    "        \"question\": \"Who won the FIFA World Cup in 2022 and where was it held?\",\n",
    "        \"answer\": \"Argentina won the 2022 FIFA World Cup, which was held in Qatar.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"5\",\n",
    "        \"question\": \"Calculate the average of these numbers: 23, 45, 67, 89, 12, 34\",\n",
    "        \"answer\": \"The average is 45\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"6\",\n",
    "        \"question\": \"What is the capital of Australia and what is its approximate population?\",\n",
    "        \"answer\": \"Canberra is the capital of Australia with a population of approximately 460,000 people.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"7\",\n",
    "        \"question\": \"If a train travels 120 miles in 2 hours, then 180 miles in 3 hours, what is its average speed over the entire journey?\",\n",
    "        \"answer\": \"The average speed is 60 miles per hour (300 miles / 5 hours).\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"8\",\n",
    "        \"question\": \"Search for information about the latest NASA Mars mission and summarize the key findings.\",\n",
    "        \"answer\": \"Requires web search for current NASA Mars mission information and synthesis of findings.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"9\",\n",
    "        \"question\": \"What is 2 to the power of 10?\",\n",
    "        \"answer\": \"1024\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"10\",\n",
    "        \"question\": \"Who is the current CEO of Microsoft and when did they take the position?\",\n",
    "        \"answer\": \"Satya Nadella has been CEO of Microsoft since February 2014.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b71b0b",
   "metadata": {},
   "source": [
    "<a id=\"optimize-first\"></a>\n",
    "### 1.2.4) Run the optimizer\n",
    "\n",
    "<div style=\"color: red; font-style: italic;\">\n",
    "<strong>Developer warning:</strong> Running the optimizer can take significant time (~30 minutes for search space of n=10) and  LLM inference tokens. Double check your config for unneeded search parameters or reduce the number of samples in the evaluation dataset to reduce cost.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71420933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-25 15:06:55 - WARNING  - nat.experimental.decorators.experimental_warning_decorator:59 - The Optimizer feature is experimental and the API may change in future releases. Future versions may introduce breaking changes without notice. Function: nat.profiler.parameter_optimization.optimizer_runtime.optimize_config\n",
      "2025-10-25 15:06:58 - WARNING  - nat.experimental.decorators.experimental_warning_decorator:59 - The Optimizer feature is experimental and the API may change in future releases. Future versions may introduce breaking changes without notice. Function: nat.profiler.parameter_optimization.parameter_optimizer.optimize_parameters\n",
      "2025-10-25 15:06:58 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:70 - Using Grid sampler for numeric optimization\n",
      "\u001b[32m[I 2025-10-25 15:06:58,998]\u001b[0m A new study created in memory with name: no-name-90c95b7e-9c77-41db-a024-592c4ef0e4a6\u001b[0m\n",
      "2025-10-25 15:06:58 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:125 - Starting numeric / enum parameter optimization...\n",
      "2025-10-25 15:07:04 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:07:04 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]2025-10-25 15:07:05 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:07:05 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A2025-10-25 15:07:05 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:07:05 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A2025-10-25 15:07:05 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:07:05 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A2025-10-25 15:07:05 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:07:05 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:07:05 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:07:05 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:07:05 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:07:05 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:07:05 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:07:05 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:07:05 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:07:05 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:07:05 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:07:05 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:00<00:03,  2.43it/s]\u001b[A2025-10-25 15:07:05 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:00<00:04,  1.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "Running workflow:  60%|███████████████▌          | 6/10 [00:00<00:00, 11.57it/s]\u001b[A\u001b[A2025-10-25 15:07:05 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:00<00:05,  1.54it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running workflow:  40%|██████████▍               | 4/10 [00:00<00:00,  6.40it/s]\u001b[A\u001b[A\u001b[A2025-10-25 15:07:06 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:00<00:08,  1.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  60%|███████████████▌          | 6/10 [00:01<00:00,  7.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:07:06 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:01<00:09,  1.03s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:01<00:11,  1.30s/it]2025-10-25 15:07:06 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:01<00:11,  1.29s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running workflow:  50%|█████████████             | 5/10 [00:02<00:02,  2.22it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:02<00:20,  2.30s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  20%|█████▏                    | 2/10 [00:03<00:13,  1.70s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  40%|██████████▍               | 4/10 [00:03<00:04,  1.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  30%|███████▊                  | 3/10 [00:04<00:08,  1.20s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:04<00:39,  4.44s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running workflow:  60%|███████████████▌          | 6/10 [00:05<00:04,  1.19s/it]\u001b[A\n",
      "Running workflow:  50%|█████████████             | 5/10 [00:06<00:07,  1.49s/it]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  40%|██████████▍               | 4/10 [00:07<00:11,  1.93s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  50%|█████████████             | 5/10 [00:07<00:06,  1.28s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  20%|█████▏                    | 2/10 [00:07<00:28,  3.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:09<01:21,  9.03s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running workflow:  80%|████████████████████▊     | 8/10 [00:09<00:03,  1.57s/it]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  40%|██████████▍               | 4/10 [00:09<00:11,  1.97s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  50%|█████████████             | 5/10 [00:09<00:07,  1.50s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  60%|███████████████▌          | 6/10 [00:10<00:09,  2.36s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running workflow:  90%|███████████████████████▍  | 9/10 [00:11<00:01,  1.67s/it]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  60%|███████████████▌          | 6/10 [00:11<00:09,  2.47s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  30%|███████▊                  | 3/10 [00:11<00:22,  3.15s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  60%|███████████████▌          | 6/10 [00:12<00:10,  2.64s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  40%|██████████▍               | 4/10 [00:13<00:14,  2.42s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running workflow:  80%|████████████████████▊     | 8/10 [00:13<00:04,  2.06s/it]\u001b[A\u001b[A\u001b[A\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:14<00:00,  1.40s/it]\u001b[A\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|█████████████| 10/10 [00:00<00:00, 1096.61it/s]\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|██████████| 10/10 [00:00<00:00, 1102.66it/s]\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  90%|███████████████████████▍  | 9/10 [00:14<00:01,  1.76s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running workflow:  90%|███████████████████████▍  | 9/10 [00:14<00:02,  2.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  60%|███████████████▌          | 6/10 [00:14<00:09,  2.45s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  50%|█████████████             | 5/10 [00:14<00:11,  2.22s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  80%|████████████████████▊     | 8/10 [00:15<00:04,  2.12s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  90%|███████████████████████▍  | 9/10 [00:15<00:02,  2.13s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:15<00:00,  1.55s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|█████████████| 10/10 [00:00<00:00, 1232.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|██████████| 10/10 [00:00<00:00, 1234.93it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  70%|██████████████████▏       | 7/10 [00:15<00:08,  2.92s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  90%|███████████████████████▍  | 9/10 [00:16<00:01,  1.90s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  70%|██████████████████▏       | 7/10 [00:16<00:10,  3.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  80%|████████████████████▊     | 8/10 [00:16<00:04,  2.23s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:16<00:00,  1.70s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|█████████████| 10/10 [00:00<00:00, 1269.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|██████████| 10/10 [00:00<00:00, 1273.28it/s]\n",
      "Running workflow:  80%|████████████████████▊     | 8/10 [00:17<00:05,  2.62s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  70%|██████████████████▏       | 7/10 [00:17<00:07,  2.61s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:17<00:00,  1.77s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|█████████████| 10/10 [00:00<00:00, 1239.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|██████████| 10/10 [00:00<00:00, 1240.99it/s]\n",
      "Running workflow:  90%|███████████████████████▍  | 9/10 [00:17<00:01,  1.97s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  60%|███████████████▌          | 6/10 [00:17<00:09,  2.39s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  80%|████████████████████▊     | 8/10 [00:17<00:03,  1.90s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:18<00:00,  1.81s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|█████████████| 10/10 [00:00<00:00, 1252.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|██████████| 10/10 [00:00<00:00, 1333.18it/s]\n",
      "2025-10-25 15:07:23 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:04<00:39,  4.43s/it]2025-10-25 15:07:23 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:23 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:02<00:26,  2.91s/it]\u001b[A2025-10-25 15:07:23 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  20%|██▊           | 2/10 [00:03<00:10,  1.27s/it]\u001b[A2025-10-25 15:07:23 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:23 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:23 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:23 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  60%|████████▍     | 6/10 [00:03<00:01,  3.23it/s]\u001b[A2025-10-25 15:07:23 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:23 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:23 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:01<00:15,  1.71s/it]\u001b[A\u001b[A2025-10-25 15:07:23 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  40%|█████▌        | 4/10 [00:01<00:02,  2.83it/s]\u001b[A\u001b[A2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:24 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:01<00:11,  1.25s/it]\u001b[A\u001b[A\u001b[A2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  50%|███████       | 5/10 [00:01<00:01,  4.59it/s]\u001b[A\u001b[A\u001b[A2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:24 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  80%|████████████████████▊     | 8/10 [00:19<00:05,  3.00s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:00<00:08,  1.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  60%|████████▍     | 6/10 [00:01<00:00,  7.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:07:24 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  20%|██▊           | 2/10 [00:05<00:18,  2.34s/it]2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  90%|███████████████████████▍  | 9/10 [00:19<00:02,  2.26s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  90%|████████████▌ | 9/10 [00:03<00:00,  3.48it/s]\u001b[A2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  80%|███████████▏  | 8/10 [00:05<00:00,  2.46it/s]2025-10-25 15:07:24 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  60%|████████▍     | 6/10 [00:02<00:01,  2.93it/s]\u001b[A\u001b[A2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  80%|███████████▏  | 8/10 [00:01<00:00,  5.61it/s]\u001b[A\u001b[A\u001b[A2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:24 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  80%|███████████▏  | 8/10 [00:02<00:00,  4.34it/s]\u001b[A\u001b[A2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:24 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  90%|████████████▌ | 9/10 [00:01<00:00,  7.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:07:24 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:24 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:24 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  70%|██████████████████▏       | 7/10 [00:19<00:07,  2.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:07:25 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:25 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:25 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:25 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:25 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  90%|███████████████████████▍  | 9/10 [00:20<00:02,  2.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  90%|███████████████████████▍  | 9/10 [00:20<00:02,  2.67s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:07:26 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:26 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:26 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:26 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:26 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  80%|████████████████████▊     | 8/10 [00:21<00:04,  2.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:07:27 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:27 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:28 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:28 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:28 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:05<00:00,  1.30it/s]\u001b[A\u001b[A\u001b[A2025-10-25 15:07:29 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:05<00:00,  1.67it/s]\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:06<00:00,  1.50it/s]\n",
      "2025-10-25 15:07:29 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:07:29 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:07:29 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:07:29 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:07:29 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:07:29 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:07:29 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:07:29 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:07:29 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:07:29 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:07:29 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:07:29 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:07:29 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:07:29 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:07:29 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:07:29 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:07:29 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:07:29 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:07:29 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:07:29 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:07:29 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:07:29 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:07:29 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:07:29 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:07:29 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:07:29 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:29 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:29 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  90%|███████████████████████▍  | 9/10 [00:24<00:02,  2.28s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:08<00:00,  1.24it/s]\u001b[A\u001b[A\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:09<00:00,  1.03it/s]\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:11<00:00,  1.15s/it]\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:07:30 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:25<00:00,  2.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|██████████████| 10/10 [00:00<00:00, 977.81it/s]\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|███████████| 10/10 [00:00<00:00, 979.82it/s]\n",
      "2025-10-25 15:07:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:00<00:02,  3.01it/s]2025-10-25 15:07:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:31 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:31 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:31 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  70%|█████████▊    | 7/10 [00:00<00:00, 18.39it/s]2025-10-25 15:07:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:31 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:31 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:31 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:32 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:32 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:32 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:32 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:32 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:32 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:03<00:00,  3.05it/s]\n",
      "2025-10-25 15:07:34 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:07:34 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:07:34 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:07:34 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:07:34 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:07:34 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:07:34 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:07:34 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:07:34 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:07:34 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:07:34 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:07:34 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:07:34 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:07:34 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:07:34 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:41<00:00,  4.12s/it]\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|██████████████| 10/10 [00:00<00:00, 784.20it/s]\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|███████████| 10/10 [00:00<00:00, 786.54it/s]\n",
      "2025-10-25 15:07:46 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:46 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:46 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:00<00:03,  2.80it/s]2025-10-25 15:07:46 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:46 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:46 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:46 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:46 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:46 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:46 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:46 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:46 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:46 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:46 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:46 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:46 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:46 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:46 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:46 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:46 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:46 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:46 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:46 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:46 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  80%|███████████▏  | 8/10 [00:00<00:00, 20.39it/s]2025-10-25 15:07:46 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:46 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:46 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:41<00:00,  4.17s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|██████████████| 10/10 [00:00<00:00, 898.25it/s]\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|███████████| 10/10 [00:00<00:00, 898.56it/s]\n",
      "2025-10-25 15:07:47 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:47 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:47 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:47 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:47 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:47 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:00<00:03,  2.94it/s]\u001b[A2025-10-25 15:07:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  80%|███████████▏  | 8/10 [00:00<00:00, 21.37it/s]\u001b[A2025-10-25 15:07:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:47 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:47 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:47 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:47 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:47 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:47 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:48 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:48 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:48 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:43<00:00,  4.38s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|█████████████| 10/10 [00:00<00:00, 1008.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|██████████| 10/10 [00:00<00:00, 1008.39it/s]\n",
      "2025-10-25 15:07:49 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:49 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:49 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:00<00:03,  2.88it/s]\u001b[A\u001b[A2025-10-25 15:07:49 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:49 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:49 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:49 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:49 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:49 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:49 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:49 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:49 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:49 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:49 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:49 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:49 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:49 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:49 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:49 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:49 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:49 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:49 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:49 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:49 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:49 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:49 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:49 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  70%|█████████▊    | 7/10 [00:00<00:00, 13.76it/s]\u001b[A\u001b[A2025-10-25 15:07:49 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:49 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:49 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:49 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:49 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:49 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  90%|████████████▌ | 9/10 [00:00<00:00, 15.05it/s]\u001b[A\u001b[A2025-10-25 15:07:49 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:49 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:49 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:50 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:50 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:50 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:51 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:51 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:51 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:51 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:51 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:51 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:52 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:52 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:52 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:52 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:52 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:52 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:53 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:53 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:53 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:04<00:00,  2.24it/s]\n",
      "2025-10-25 15:07:53 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:07:53 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:07:53 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:07:53 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:07:53 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:07:53 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:07:53 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:07:53 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:07:53 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:07:53 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:07:53 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:07:53 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:07:53 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:07:53 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:07:53 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:07:53 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:07:53 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:07:53 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:07:53 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:07:53 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:07<00:00,  1.27it/s]\n",
      "2025-10-25 15:07:54 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:07:54 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:07:54 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:07:54 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:07:54 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:07:54 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:07:54 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:07:54 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:07:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:07:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:07:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:07:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:07:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:07:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:07:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:07:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:07:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:07:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:07:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:07:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:08<00:00,  1.15it/s]\n",
      "2025-10-25 15:07:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:07:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:07:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:07:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:07:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:07:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:07:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:07:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:07:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:07:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:07:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:07:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:07:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:07:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:07:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:07:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:07:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:07:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:07:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:07:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:49<00:00,  4.97s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|█████████████| 10/10 [00:00<00:00, 1097.35it/s]\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|██████████| 10/10 [00:00<00:00, 1097.81it/s]\n",
      "2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:00<00:03,  2.92it/s]2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:55 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:55 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:55 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:55 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  70%|█████████▊    | 7/10 [00:00<00:00, 18.25it/s]2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:55 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:07:55 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:55 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:55 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:55 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:56 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:56 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:56 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:56 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:57 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:57 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:57 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:57 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:03<00:00,  2.99it/s]\n",
      "2025-10-25 15:07:58 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:07:58 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:07:58 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:07:58 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:07:58 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:07:58 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:07:58 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:07:58 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:07:58 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:07:58 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:07:58 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:07:58 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:07:58 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:07:58 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:07:58 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:07:58 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:07:58 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:07:58 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:07:58 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:07:58 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "\u001b[32m[I 2025-10-25 15:07:58,563]\u001b[0m Trial 0 finished with values: [0.07, 95.28, 8.8] and parameters: {'llms.chat_completion_llm.temperature': 0.0, 'llms.chat_completion_llm.model_name': 'meta/llama-3.1-70b-instruct'}.\u001b[0m\n",
      "2025-10-25 15:07:58 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.7, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:07:58 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.7, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:07:58 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.7, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:07:58 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.7, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:07:58 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]2025-10-25 15:07:58 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.7, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:07:58 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A2025-10-25 15:07:58 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.7, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:07:58 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A2025-10-25 15:07:58 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.7, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:07:58 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A2025-10-25 15:07:58 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.7, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:07:58 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:07:58 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.7, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:07:58 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:07:58 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.7, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:07:58 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:07:58 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.7, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:07:58 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:07:58 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.7, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:07:58 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:07:58 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.7, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:07:58 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:00<00:02,  4.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:00<00:02,  4.27it/s]\u001b[A2025-10-25 15:07:58 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:58 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:00<00:03,  2.66it/s]\u001b[A\u001b[A2025-10-25 15:07:58 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:00<00:03,  2.64it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running workflow:  50%|█████████████             | 5/10 [00:00<00:00, 12.32it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:00<00:04,  2.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:07:59 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:00<00:05,  1.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:07:59 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:00<00:05,  1.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:07:59 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  30%|███████▊                  | 3/10 [00:00<00:01,  4.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:07:59 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:00<00:06,  1.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  60%|███████████████▌          | 6/10 [00:00<00:00,  9.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:07:59 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  60%|███████████████▌          | 6/10 [00:00<00:00,  6.66it/s]2025-10-25 15:07:59 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  70%|██████████████████▏       | 7/10 [00:01<00:00,  6.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running workflow:  50%|█████████████             | 5/10 [00:01<00:01,  4.82it/s]\u001b[A\n",
      "\n",
      "Running workflow:  70%|██████████████████▏       | 7/10 [00:01<00:00,  7.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "Running workflow:  80%|████████████████████▊     | 8/10 [00:01<00:00,  7.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running workflow:  80%|████████████████████▊     | 8/10 [00:01<00:00,  6.91it/s]\u001b[A\u001b[A\u001b[A2025-10-25 15:07:59 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:59 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  80%|████████████████████▊     | 8/10 [00:01<00:00,  7.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  80%|████████████████████▊     | 8/10 [00:01<00:00,  8.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:07:59 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  80%|████████████████████▊     | 8/10 [00:01<00:00,  8.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  80%|████████████████████▊     | 8/10 [00:01<00:00,  8.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:07:59 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:59 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:07:59 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  80%|████████████████████▊     | 8/10 [00:01<00:00,  7.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running workflow:  70%|██████████████████▏       | 7/10 [00:01<00:00,  5.36it/s]\u001b[A2025-10-25 15:08:00 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:00 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:00 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:00 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:00 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:00 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:02<00:00,  4.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|█████████████| 10/10 [00:00<00:00, 1896.85it/s]\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|██████████| 10/10 [00:00<00:00, 1900.89it/s]\n",
      "2025-10-25 15:08:01 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:01 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:00<00:03,  2.94it/s]2025-10-25 15:08:01 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:01 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:01 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:01 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:01 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:01 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  60%|████████▍     | 6/10 [00:00<00:00, 15.63it/s]\n",
      "\n",
      "Running workflow:  90%|███████████████████████▍  | 9/10 [00:02<00:00,  2.32it/s]\u001b[A\u001b[A2025-10-25 15:08:01 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  90%|███████████████████████▍  | 9/10 [00:02<00:00,  2.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:01 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  90%|████████████▌ | 9/10 [00:00<00:00, 14.43it/s]2025-10-25 15:08:01 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:03<00:00,  3.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|██████████████| 10/10 [00:00<00:00, 951.78it/s]\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|███████████| 10/10 [00:00<00:00, 954.27it/s]\n",
      "2025-10-25 15:08:02 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:02 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:00<00:03,  2.47it/s]\u001b[A2025-10-25 15:08:02 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:02 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:02 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:02 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:02 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  40%|█████▌        | 4/10 [00:00<00:00,  8.30it/s]\u001b[A2025-10-25 15:08:02 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:03<00:00,  2.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|██████████████| 10/10 [00:00<00:00, 909.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|███████████| 10/10 [00:00<00:00, 910.34it/s]\n",
      "2025-10-25 15:08:02 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:02 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:02 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:02 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  70%|█████████▊    | 7/10 [00:00<00:00,  9.08it/s]\u001b[A2025-10-25 15:08:02 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:00<00:02,  3.08it/s]\u001b[A\u001b[A2025-10-25 15:08:02 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:02 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:02 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:02 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:02 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:02 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:02 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  70%|█████████▊    | 7/10 [00:00<00:00, 18.93it/s]\u001b[A\u001b[A2025-10-25 15:08:02 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:03 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:03 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "Running workflow:  80%|████████████████████▊     | 8/10 [00:04<00:01,  1.16it/s]\u001b[A2025-10-25 15:08:03 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:03 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:03 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "Running workflow:  90%|███████████████████████▍  | 9/10 [00:05<00:00,  1.18it/s]\u001b[A2025-10-25 15:08:04 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:05<00:00,  1.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|██████████████| 10/10 [00:00<00:00, 965.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|███████████| 10/10 [00:00<00:00, 965.36it/s]\n",
      "Running workflow:  90%|███████████████████████▍  | 9/10 [00:05<00:00,  1.02it/s]2025-10-25 15:08:04 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:04 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:00<00:03,  2.97it/s]\u001b[A\u001b[A\u001b[A2025-10-25 15:08:04 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:04 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:04 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:04 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:04 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:04 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:04 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  70%|█████████▊    | 7/10 [00:00<00:00, 17.63it/s]\u001b[A\u001b[A\u001b[A2025-10-25 15:08:04 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:04 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:04 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:05 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:05 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:05 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:05 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:06 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:06 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:07<00:00,  1.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|█████████████| 10/10 [00:00<00:00, 1115.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|██████████| 10/10 [00:00<00:00, 1122.25it/s]\n",
      "2025-10-25 15:08:06 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:00<00:03,  2.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:06 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:06 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:06 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:06 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:06 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:06 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:06 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:08<00:00,  1.18it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|██████████████| 10/10 [00:00<00:00, 956.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|███████████| 10/10 [00:00<00:00, 951.65it/s]\n",
      "2025-10-25 15:08:07 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:07 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:08<00:00,  1.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|██████████████| 10/10 [00:00<00:00, 812.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|███████████| 10/10 [00:00<00:00, 813.26it/s]\n",
      "2025-10-25 15:08:07 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:07 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:07 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:07 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:07 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:07 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:07 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:07 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:07 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:07 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:07 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:07 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:00<00:03,  2.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:04<00:00,  1.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  90%|████████████▌ | 9/10 [00:00<00:00, 10.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:07 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:00<00:03,  2.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:07 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:07 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:07 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:07 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:07 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:07 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:07 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  80%|███████████▏  | 8/10 [00:00<00:00, 16.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:07 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:07 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:07 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  80%|███████████▏  | 8/10 [00:00<00:00, 20.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:07 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:07 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:08 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:08 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:09<00:00,  1.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|█████████████| 10/10 [00:00<00:00, 1295.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|██████████| 10/10 [00:00<00:00, 1295.86it/s]\n",
      "2025-10-25 15:08:08 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:08 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:08 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:06<00:00,  1.12it/s]\u001b[A2025-10-25 15:08:08 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:00<00:03,  2.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:08 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:08 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:08 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:08 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:08 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:08 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:08 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:08 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:08 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  80%|███████████▏  | 8/10 [00:00<00:00, 21.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:08 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:08 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:09 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:09 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:09 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:09 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:09 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:10 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:10 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:10 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:06<00:00,  1.31it/s]\u001b[A\u001b[A\u001b[A2025-10-25 15:08:10 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:10 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:12 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:12 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:14<00:00,  1.48s/it]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|█████████████| 10/10 [00:00<00:00, 1025.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|██████████| 10/10 [00:00<00:00, 1020.61it/s]\n",
      "2025-10-25 15:08:13 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:13 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:00<00:03,  2.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:13 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:13 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:13 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:13 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:13 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:13 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:13 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:13 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:13 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  80%|███████████▏  | 8/10 [00:00<00:00, 20.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:13 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:14 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:14 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:14 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:02<00:00,  3.68it/s]\n",
      "2025-10-25 15:08:16 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:16 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:16 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:16 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:16 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:16 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:16 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:16 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:16 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:16 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:16 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:16 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:10<00:00,  1.07s/it]\n",
      "2025-10-25 15:08:18 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:18 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:18 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:18 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:18 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:18 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:18 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:18 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:18 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:18 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:18 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:18 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:11<00:00,  1.19s/it]\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:12<00:00,  1.23s/it]\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:13<00:00,  1.30s/it]\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:19 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:15<00:00,  1.56s/it]\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:17<00:00,  1.75s/it]\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:18<00:00,  1.84s/it]\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:19<00:00,  1.96s/it]\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:20 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:21<00:00,  2.19s/it]\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|█████████████| 10/10 [00:00<00:00, 1098.33it/s]\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|██████████| 10/10 [00:00<00:00, 1099.97it/s]\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:00<00:03,  2.94it/s]2025-10-25 15:08:20 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:20 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:20 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:20 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:20 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:20 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:20 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:20 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  70%|█████████▊    | 7/10 [00:00<00:00, 18.37it/s]2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:20 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:21 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:21 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:21 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:21 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:21 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:21 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:21 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:21 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:21 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:21 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:21 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:21 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:21 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:21 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:21 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:21 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:22 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:22 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:22 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:22 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:22 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:22 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:22 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:22 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:22 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:22 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:22 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:22 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:22 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:22 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:22 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:22 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:03<00:00,  3.00it/s]\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "\u001b[32m[I 2025-10-25 15:08:23,847]\u001b[0m Trial 1 finished with values: [0.05, 21.779999999999998, 0.967] and parameters: {'llms.chat_completion_llm.temperature': 0.7, 'llms.chat_completion_llm.model_name': 'meta/llama-3.1-70b-instruct'}.\u001b[0m\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.7, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.7, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.7, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.7, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.7, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.7, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.7, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.7, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.7, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.7, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.7, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:23 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.7, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:23 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.7, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:23 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.7, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:23 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.7, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:23 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.7, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:23 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.7, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:23 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.7, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:23 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.7, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:23 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:23 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.7, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:23 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:00<00:02,  4.17it/s]\u001b[A\n",
      "\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:00<00:02,  4.47it/s]\u001b[A\u001b[A2025-10-25 15:08:24 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "Running workflow:  20%|█████▏                    | 2/10 [00:00<00:01,  6.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:00<00:04,  2.17it/s]\u001b[A\u001b[A\u001b[A2025-10-25 15:08:24 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  60%|███████████████▌          | 6/10 [00:00<00:00, 12.78it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:00<00:05,  1.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:24 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  40%|██████████▍               | 4/10 [00:00<00:00,  7.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:24 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  30%|███████▊                  | 3/10 [00:00<00:02,  3.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  20%|█████▏                    | 2/10 [00:00<00:02,  2.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:00<00:08,  1.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:24 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  70%|██████████████████▏       | 7/10 [00:01<00:00,  8.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:24 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:01<00:09,  1.06s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:01<00:10,  1.19s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:25 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "Running workflow:  30%|███████▊                  | 3/10 [00:01<00:03,  2.08it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  40%|██████████▍               | 4/10 [00:01<00:01,  3.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:25 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "Running workflow:  40%|██████████▍               | 4/10 [00:01<00:02,  2.47it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:01<00:12,  1.42s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running workflow:  50%|█████████████             | 5/10 [00:01<00:01,  3.10it/s]\u001b[A\n",
      "\n",
      "Running workflow:  80%|████████████████████▊     | 8/10 [00:01<00:00,  4.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running workflow:  90%|███████████████████████▍  | 9/10 [00:01<00:00,  4.83it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  80%|████████████████████▊     | 8/10 [00:01<00:00,  4.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:25 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  80%|████████████████████▊     | 8/10 [00:01<00:00,  5.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:25 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:25 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:25 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:25 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  60%|███████████████▌          | 6/10 [00:01<00:01,  3.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  70%|██████████████████▏       | 7/10 [00:01<00:00,  4.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:25 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  80%|████████████████████▊     | 8/10 [00:01<00:00,  5.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:25 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:25 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:02<00:00,  4.13it/s]\u001b[A\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|██████████████| 10/10 [00:00<00:00, 636.38it/s]\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|███████████| 10/10 [00:00<00:00, 636.86it/s]\n",
      "Running workflow:  80%|████████████████████▊     | 8/10 [00:02<00:00,  3.01it/s]2025-10-25 15:08:26 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:26 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:26 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:00<00:03,  2.81it/s]2025-10-25 15:08:26 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:26 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:26 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:26 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:26 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:26 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:26 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:26 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:26 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  70%|█████████▊    | 7/10 [00:00<00:00, 17.74it/s]2025-10-25 15:08:26 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:26 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:27 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:03<00:00,  3.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|██████████████| 10/10 [00:00<00:00, 957.76it/s]\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|███████████| 10/10 [00:00<00:00, 959.69it/s]\n",
      "2025-10-25 15:08:27 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:03<00:00,  2.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:03<00:00,  3.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|█████████████| 10/10 [00:00<00:00, 1261.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|██████████| 10/10 [00:00<00:00, 1322.21it/s]\n",
      "2025-10-25 15:08:27 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:27 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:27 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:27 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:27 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:27 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:27 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:27 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:00<00:03,  2.42it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:03<00:00,  2.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|██████████████| 10/10 [00:00<00:00, 810.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|███████████| 10/10 [00:00<00:00, 792.89it/s]\n",
      "2025-10-25 15:08:27 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:27 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:27 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:27 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:27 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:27 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:27 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:27 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:27 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:27 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:00<00:04,  2.08it/s]\u001b[A\u001b[A\n",
      "Evaluating Ragas nv_accuracy:  80%|███████████▏  | 8/10 [00:00<00:00, 13.41it/s]\u001b[A2025-10-25 15:08:27 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:00<00:02,  3.06it/s]\u001b[A\u001b[A\u001b[A2025-10-25 15:08:27 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:27 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:27 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:27 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:27 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:27 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:27 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  80%|███████████▏  | 8/10 [00:00<00:00, 15.41it/s]\u001b[A\u001b[A2025-10-25 15:08:27 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:27 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:27 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:28 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:28 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:28 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:28 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  70%|█████████▊    | 7/10 [00:00<00:00, 10.22it/s]\u001b[A\u001b[A\u001b[A2025-10-25 15:08:28 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:28 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:28 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:04<00:00,  2.21it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|█████████████| 10/10 [00:00<00:00, 1063.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|██████████| 10/10 [00:00<00:00, 1069.16it/s]\n",
      "2025-10-25 15:08:28 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:04<00:00,  2.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:04<00:00,  1.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:04<00:00,  2.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|███████████████| 10/10 [00:00<00:00, 49.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|████████████| 10/10 [00:00<00:00, 49.63it/s]\n",
      "Evaluating Avg LLM Latency: 100%|██████████████| 10/10 [00:00<00:00, 761.95it/s]\n",
      "Evaluating Avg Tokens/LLM_END: 100%|███████████| 10/10 [00:00<00:00, 762.73it/s]\n",
      "2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:29 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:00<00:05,  1.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:05<00:00,  1.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|█████████████| 10/10 [00:00<00:00, 1715.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|██████████| 10/10 [00:00<00:00, 1716.44it/s]\n",
      "2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:29 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:29 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:00<00:05,  1.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:00<00:03,  2.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:29 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:29 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  80%|███████████▏  | 8/10 [00:01<00:00,  8.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:00<00:04,  2.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:29 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  70%|█████████▊    | 7/10 [00:00<00:00,  9.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  80%|███████████▏  | 8/10 [00:00<00:00, 13.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:29 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:29 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:29 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  80%|███████████▏  | 8/10 [00:00<00:00, 17.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:29 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  90%|███████████████████████▍  | 9/10 [00:05<00:00,  1.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:29 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:29 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:29 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:30 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:30 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:30 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:30 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:30 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:30 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:31 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:31 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:07<00:00,  1.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|█████████████| 10/10 [00:00<00:00, 1418.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|██████████| 10/10 [00:00<00:00, 1410.85it/s]\n",
      "2025-10-25 15:08:31 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:00<00:02,  3.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:07<00:00,  1.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|█████████████| 10/10 [00:00<00:00, 1011.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|██████████| 10/10 [00:00<00:00, 1012.77it/s]\n",
      "2025-10-25 15:08:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:31 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:05<00:00,  1.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:00<00:03,  3.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:31 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:31 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:31 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:31 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  90%|████████████▌ | 9/10 [00:00<00:00, 12.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:31 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:31 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:32 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:32 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:32 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  80%|███████████▏  | 8/10 [00:00<00:00, 15.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:32 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:32 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:05<00:00,  1.51it/s]\u001b[A2025-10-25 15:08:32 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:32 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:32 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:32 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:32 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:32 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:32 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:33 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:33 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:33 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:34 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:34 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:34 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:05<00:00,  1.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:34 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:34 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:35 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:35 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:06<00:00,  1.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:07<00:00,  1.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:36 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:36 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:06<00:00,  1.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:07<00:00,  1.35it/s]\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:09<00:00,  1.04it/s]\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:10<00:00,  1.01s/it]\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:38 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:10<00:00,  1.05s/it]\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:10<00:00,  1.09s/it]\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:11<00:00,  1.20s/it]\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:12<00:00,  1.24s/it]\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:12<00:00,  1.29s/it]\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:13<00:00,  1.38s/it]\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "\u001b[32m[I 2025-10-25 15:08:40,149]\u001b[0m Trial 2 finished with values: [0.015000000000000003, 36.0, 0.454] and parameters: {'llms.chat_completion_llm.temperature': 0.7, 'llms.chat_completion_llm.model_name': 'meta/llama-3.1-8b-instruct'}.\u001b[0m\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:40 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:40 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:40 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:40 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:40 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:40 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:40 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:40 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:40 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:   0%|                                  | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:40 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={'console': ConsoleLoggingMethodConfig(level='INFO')}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={} function_groups={} llms={'chat_completion_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=1024), 'nim_judge_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-405b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/eval_output/optimizer'), eval_metrics={'accuracy': OptimizerMetric(evaluator_name='answer_accuracy', direction='maximize', weight=1.0), 'token_efficiency': OptimizerMetric(evaluator_name='token_efficiency', direction='minimize', weight=1.0), 'latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=10, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=ChatCompletionConfig(system_prompt='You are a helpful AI assistant. Provide clear, accurate, and helpful \\nresponses to user queries. Be concise and informative.\\n', llm_name='chat_completion_llm') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/eval_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='./tmp_workflow/data/eval_data.json'), profiler=None), evaluators={'answer_accuracy': RagasEvaluatorConfig(llm_name='nim_judge_llm', metric='AnswerAccuracy', input_obj_field=None), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:08:40 - WARNING  - nat.profiler.utils:137 - Discovered frameworks: {<LLMFrameworkEnum.LANGCHAIN: 'langchain'>} in function register_chat_completion by inspecting source. It is recommended and more reliable to instead add the used LLMFrameworkEnum types in the framework_wrappers argument when calling @register_function.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:00<00:02,  4.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:00<00:02,  4.44it/s]\u001b[A2025-10-25 15:08:40 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:40 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:40 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:00<00:02,  3.02it/s]\u001b[A\u001b[A2025-10-25 15:08:40 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:00<00:03,  2.86it/s]\u001b[A\u001b[A\u001b[A2025-10-25 15:08:40 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:00<00:03,  2.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:40 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:00<00:04,  2.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:00<00:05,  1.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:40 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:40 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:00<00:05,  1.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  60%|███████████████▌          | 6/10 [00:00<00:00, 10.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:00<00:06,  1.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:40 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  10%|██▌                       | 1/10 [00:00<00:07,  1.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:41 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "Running workflow:  60%|███████████████▌          | 6/10 [00:00<00:00,  6.39it/s]\n",
      "Running workflow:  80%|████████████████████▊     | 8/10 [00:00<00:00,  8.68it/s]\u001b[A\n",
      "\n",
      "Running workflow:  70%|██████████████████▏       | 7/10 [00:00<00:00,  7.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  80%|████████████████████▊     | 8/10 [00:00<00:00,  9.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Running workflow:  80%|████████████████████▊     | 8/10 [00:00<00:00,  9.05it/s]\u001b[A\u001b[A\u001b[A2025-10-25 15:08:41 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:41 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  80%|████████████████████▊     | 8/10 [00:00<00:00,  9.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:41 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  80%|████████████████████▊     | 8/10 [00:01<00:00,  9.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:41 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:41 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  90%|███████████████████████▍  | 9/10 [00:01<00:00, 10.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  80%|████████████████████▊     | 8/10 [00:01<00:00,  9.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:41 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow:  80%|████████████████████▊     | 8/10 [00:01<00:00,  9.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:41 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:41 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:41 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "Running workflow:  80%|████████████████████▊     | 8/10 [00:01<00:00,  4.61it/s]2025-10-25 15:08:41 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:41 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:41 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:41 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:41 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:41 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:41 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:02<00:00,  4.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:02<00:00,  4.61it/s]\u001b[A\u001b[A\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|█████████████| 10/10 [00:00<00:00, 1147.87it/s]\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|██████████| 10/10 [00:00<00:00, 1143.92it/s]\n",
      "Running workflow:  90%|███████████████████████▍  | 9/10 [00:02<00:00,  3.02it/s]\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:02<00:00,  4.11it/s]\u001b[A\n",
      "\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|█████████████| 10/10 [00:00<00:00, 1521.11it/s]\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|██████████| 10/10 [00:00<00:00, 1522.32it/s]\n",
      "2025-10-25 15:08:42 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:42 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:42 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:42 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:42 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:42 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:42 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:02<00:00,  3.76it/s]\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|█████████████| 10/10 [00:00<00:00, 1284.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|██████████| 10/10 [00:00<00:00, 1284.63it/s]\n",
      "2025-10-25 15:08:43 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:43 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:43 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:43 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:43 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:43 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:43 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:43 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:43 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:43 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  70%|█████████▊    | 7/10 [00:00<00:00, 13.28it/s]\u001b[A2025-10-25 15:08:43 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:00<00:03,  2.89it/s]\u001b[A\u001b[A2025-10-25 15:08:43 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:43 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:43 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:43 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:43 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:43 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:43 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:43 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:43 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:43 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:43 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:43 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:43 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:43 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  90%|████████████▌ | 9/10 [00:01<00:00,  5.43it/s]\u001b[A\u001b[A\n",
      "Evaluating Ragas nv_accuracy:  80%|███████████▏  | 8/10 [00:01<00:00,  6.55it/s]\u001b[A2025-10-25 15:08:43 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:43 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:43 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:03<00:00,  2.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg Tokens/LLM_END:   0%|                     | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|█████████████| 10/10 [00:00<00:00, 1602.78it/s]\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|██████████| 10/10 [00:00<00:00, 1577.81it/s]\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:03<00:00,  2.55it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|█████████████| 10/10 [00:00<00:00, 1345.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|██████████| 10/10 [00:00<00:00, 1342.48it/s]\n",
      "2025-10-25 15:08:44 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:44 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:44 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:44 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:44 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:44 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:44 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:44 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:00<00:03,  2.48it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:04<00:00,  2.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|█████████████| 10/10 [00:00<00:00, 1756.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|██████████| 10/10 [00:00<00:00, 1736.99it/s]\n",
      "2025-10-25 15:08:44 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:44 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:44 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:44 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:44 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:44 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:44 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:44 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:44 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:44 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:44 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:44 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:00<00:03,  2.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  80%|███████████▏  | 8/10 [00:00<00:00, 16.13it/s]\u001b[A\u001b[A\u001b[A2025-10-25 15:08:44 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:00<00:02,  3.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:44 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:44 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:44 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:44 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:44 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:44 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:44 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:44 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  80%|███████████▏  | 8/10 [00:00<00:00, 17.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:44 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:44 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:44 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:44 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:44 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  80%|███████████▏  | 8/10 [00:00<00:00, 21.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:44 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:44 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:45 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:45 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:45 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:45 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:45 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:45 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:45 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:46 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:05<00:00,  1.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|█████████████| 10/10 [00:00<00:00, 1323.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|██████████| 10/10 [00:00<00:00, 1322.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:06<00:00,  1.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|█████████████| 10/10 [00:00<00:00, 1231.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|██████████| 10/10 [00:00<00:00, 1229.14it/s]\n",
      "2025-10-25 15:08:46 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:46 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:46 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:46 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:46 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:46 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:46 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:46 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:46 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:00<00:05,  1.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:06<00:00,  1.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Running workflow: 100%|█████████████████████████| 10/10 [00:06<00:00,  1.55it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg Tokens/LLM_END:   0%|                     | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:   0%|                      | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg Tokens/LLM_END:   0%|                     | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency:  10%|█▌              | 1/10 [00:00<00:01,  5.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency: 100%|███████████████| 10/10 [00:00<00:00, 48.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg Tokens/LLM_END: 100%|████████████| 10/10 [00:00<00:00, 48.73it/s]\n",
      "Evaluating Avg LLM Latency: 100%|██████████████| 10/10 [00:00<00:00, 707.16it/s]\n",
      "Evaluating Avg Tokens/LLM_END: 100%|███████████| 10/10 [00:00<00:00, 708.82it/s]\n",
      "2025-10-25 15:08:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:47 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:47 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:47 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:47 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:01<00:11,  1.32s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  70%|█████████▊    | 7/10 [00:01<00:00,  4.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:47 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:47 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:00<00:06,  1.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  10%|█▍            | 1/10 [00:00<00:08,  1.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:47 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:47 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  80%|███████████▏  | 8/10 [00:01<00:00,  6.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  70%|█████████▊    | 7/10 [00:01<00:00,  7.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  70%|█████████▊    | 7/10 [00:00<00:00,  8.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:47 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:47 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:47 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  90%|████████████▌ | 9/10 [00:01<00:00,  8.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  90%|████████████▌ | 9/10 [00:01<00:00,  9.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:48 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:48 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:48 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:48 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:48 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:48 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:48 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:48 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:05<00:00,  1.50it/s]\u001b[A\u001b[A2025-10-25 15:08:48 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:49 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:49 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:49 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:49 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:49 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:49 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:07<00:00,  1.12it/s]\u001b[A2025-10-25 15:08:49 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:49 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:50 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:50 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:50 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:50 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:50 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:51 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:51 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:51 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:05<00:00,  1.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A2025-10-25 15:08:52 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:52 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:08:52 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:08:52 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:05<00:00,  1.77it/s]\n",
      "2025-10-25 15:08:52 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:52 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:52 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:52 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:52 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:52 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:52 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:52 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:52 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:52 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:06<00:00,  1.65it/s]\n",
      "2025-10-25 15:08:52 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:52 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:52 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:52 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:52 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:52 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:52 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:52 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:52 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:52 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:52 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:52 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:52 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:52 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:52 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:07<00:00,  1.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:08<00:00,  1.22it/s]\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:10<00:00,  1.01s/it]\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:10<00:00,  1.06s/it]\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:10<00:00,  1.10s/it]\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:54 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:12<00:00,  1.23s/it]\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:12<00:00,  1.27s/it]\n",
      "Evaluating Ragas nv_accuracy: 100%|█████████████| 10/10 [00:13<00:00,  1.31s/it]\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/eval_output/workflow_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/llm_latency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/token_efficiency_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "2025-10-25 15:08:55 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/eval_output/answer_accuracy_output.json\n",
      "\u001b[32m[I 2025-10-25 15:08:55,548]\u001b[0m Trial 3 finished with values: [0.02, 14.790000000000001, 0.154] and parameters: {'llms.chat_completion_llm.temperature': 0.0, 'llms.chat_completion_llm.model_name': 'meta/llama-3.1-8b-instruct'}.\u001b[0m\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:127 - Numeric optimization finished\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:127 - Numeric optimization finished\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:127 - Numeric optimization finished\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:127 - Numeric optimization finished\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:127 - Numeric optimization finished\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:127 - Numeric optimization finished\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:127 - Numeric optimization finished\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:127 - Numeric optimization finished\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:127 - Numeric optimization finished\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:127 - Numeric optimization finished\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:127 - Numeric optimization finished\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:127 - Numeric optimization finished\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:127 - Numeric optimization finished\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:127 - Numeric optimization finished\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:127 - Numeric optimization finished\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:127 - Numeric optimization finished\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:127 - Numeric optimization finished\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:127 - Numeric optimization finished\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:127 - Numeric optimization finished\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:173 - Generating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:173 - Generating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:173 - Generating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:173 - Generating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:173 - Generating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:173 - Generating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:173 - Generating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:173 - Generating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:173 - Generating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:173 - Generating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:173 - Generating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:173 - Generating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:173 - Generating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:173 - Generating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:173 - Generating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:173 - Generating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:173 - Generating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:173 - Generating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:173 - Generating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:408 - Creating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:408 - Creating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:408 - Creating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:408 - Creating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:408 - Creating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:408 - Creating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:408 - Creating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:408 - Creating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:408 - Creating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:408 - Creating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:408 - Creating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:408 - Creating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:408 - Creating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:408 - Creating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:408 - Creating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:408 - Creating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:408 - Creating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:408 - Creating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:408 - Creating Pareto front visualizations...\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:409 - Total trials: 4\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:409 - Total trials: 4\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:409 - Total trials: 4\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:409 - Total trials: 4\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:409 - Total trials: 4\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:409 - Total trials: 4\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:409 - Total trials: 4\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:409 - Total trials: 4\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:409 - Total trials: 4\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:409 - Total trials: 4\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:409 - Total trials: 4\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:409 - Total trials: 4\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:409 - Total trials: 4\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:409 - Total trials: 4\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:409 - Total trials: 4\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:409 - Total trials: 4\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:409 - Total trials: 4\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:409 - Total trials: 4\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:409 - Total trials: 4\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:410 - Pareto optimal trials: 3\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:410 - Pareto optimal trials: 3\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:410 - Pareto optimal trials: 3\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:410 - Pareto optimal trials: 3\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:410 - Pareto optimal trials: 3\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:410 - Pareto optimal trials: 3\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:410 - Pareto optimal trials: 3\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:410 - Pareto optimal trials: 3\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:410 - Pareto optimal trials: 3\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:410 - Pareto optimal trials: 3\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:410 - Pareto optimal trials: 3\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:410 - Pareto optimal trials: 3\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:410 - Pareto optimal trials: 3\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:410 - Pareto optimal trials: 3\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:410 - Pareto optimal trials: 3\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:410 - Pareto optimal trials: 3\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:410 - Pareto optimal trials: 3\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:410 - Pareto optimal trials: 3\n",
      "2025-10-25 15:08:55 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:410 - Pareto optimal trials: 3\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:231 - Parallel coordinates plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_parallel_coordinates.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:231 - Parallel coordinates plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_parallel_coordinates.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:231 - Parallel coordinates plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_parallel_coordinates.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:231 - Parallel coordinates plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_parallel_coordinates.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:231 - Parallel coordinates plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_parallel_coordinates.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:231 - Parallel coordinates plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_parallel_coordinates.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:231 - Parallel coordinates plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_parallel_coordinates.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:231 - Parallel coordinates plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_parallel_coordinates.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:231 - Parallel coordinates plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_parallel_coordinates.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:231 - Parallel coordinates plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_parallel_coordinates.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:231 - Parallel coordinates plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_parallel_coordinates.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:231 - Parallel coordinates plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_parallel_coordinates.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:231 - Parallel coordinates plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_parallel_coordinates.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:231 - Parallel coordinates plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_parallel_coordinates.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:231 - Parallel coordinates plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_parallel_coordinates.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:231 - Parallel coordinates plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_parallel_coordinates.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:231 - Parallel coordinates plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_parallel_coordinates.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:231 - Parallel coordinates plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_parallel_coordinates.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:231 - Parallel coordinates plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_parallel_coordinates.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:325 - Pairwise matrix plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_pairwise_matrix.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:325 - Pairwise matrix plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_pairwise_matrix.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:325 - Pairwise matrix plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_pairwise_matrix.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:325 - Pairwise matrix plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_pairwise_matrix.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:325 - Pairwise matrix plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_pairwise_matrix.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:325 - Pairwise matrix plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_pairwise_matrix.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:325 - Pairwise matrix plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_pairwise_matrix.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:325 - Pairwise matrix plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_pairwise_matrix.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:325 - Pairwise matrix plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_pairwise_matrix.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:325 - Pairwise matrix plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_pairwise_matrix.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:325 - Pairwise matrix plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_pairwise_matrix.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:325 - Pairwise matrix plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_pairwise_matrix.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:325 - Pairwise matrix plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_pairwise_matrix.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:325 - Pairwise matrix plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_pairwise_matrix.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:325 - Pairwise matrix plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_pairwise_matrix.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:325 - Pairwise matrix plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_pairwise_matrix.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:325 - Pairwise matrix plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_pairwise_matrix.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:325 - Pairwise matrix plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_pairwise_matrix.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:325 - Pairwise matrix plot saved to: tmp_workflow/eval_output/optimizer/plots/pareto_pairwise_matrix.png\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:438 - Visualization complete!\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:438 - Visualization complete!\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:438 - Visualization complete!\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:438 - Visualization complete!\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:438 - Visualization complete!\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:438 - Visualization complete!\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:438 - Visualization complete!\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:438 - Visualization complete!\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:438 - Visualization complete!\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:438 - Visualization complete!\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:438 - Visualization complete!\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:438 - Visualization complete!\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:438 - Visualization complete!\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:438 - Visualization complete!\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:438 - Visualization complete!\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:438 - Visualization complete!\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:438 - Visualization complete!\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:438 - Visualization complete!\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:438 - Visualization complete!\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:440 - Plots saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:440 - Plots saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:440 - Plots saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:440 - Plots saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:440 - Plots saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:440 - Plots saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:440 - Plots saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:440 - Plots saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:440 - Plots saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:440 - Plots saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:440 - Plots saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:440 - Plots saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:440 - Plots saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:440 - Plots saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:440 - Plots saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:440 - Plots saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:440 - Plots saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:440 - Plots saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:440 - Plots saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:182 - Pareto visualizations saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:182 - Pareto visualizations saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:182 - Pareto visualizations saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:182 - Pareto visualizations saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:182 - Pareto visualizations saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:182 - Pareto visualizations saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:182 - Pareto visualizations saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:182 - Pareto visualizations saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:182 - Pareto visualizations saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:182 - Pareto visualizations saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:182 - Pareto visualizations saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:182 - Pareto visualizations saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:182 - Pareto visualizations saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:182 - Pareto visualizations saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:182 - Pareto visualizations saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:182 - Pareto visualizations saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:182 - Pareto visualizations saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:182 - Pareto visualizations saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:182 - Pareto visualizations saved to: tmp_workflow/eval_output/optimizer/plots\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.optimizer_runtime:66 - All optimization phases complete.\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.optimizer_runtime:66 - All optimization phases complete.\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.optimizer_runtime:66 - All optimization phases complete.\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.optimizer_runtime:66 - All optimization phases complete.\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.optimizer_runtime:66 - All optimization phases complete.\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.optimizer_runtime:66 - All optimization phases complete.\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.optimizer_runtime:66 - All optimization phases complete.\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.optimizer_runtime:66 - All optimization phases complete.\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.optimizer_runtime:66 - All optimization phases complete.\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.optimizer_runtime:66 - All optimization phases complete.\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.optimizer_runtime:66 - All optimization phases complete.\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.optimizer_runtime:66 - All optimization phases complete.\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.optimizer_runtime:66 - All optimization phases complete.\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.optimizer_runtime:66 - All optimization phases complete.\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.optimizer_runtime:66 - All optimization phases complete.\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.optimizer_runtime:66 - All optimization phases complete.\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.optimizer_runtime:66 - All optimization phases complete.\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.optimizer_runtime:66 - All optimization phases complete.\n",
      "2025-10-25 15:08:56 - INFO     - nat.profiler.parameter_optimization.optimizer_runtime:66 - All optimization phases complete.\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!nat optimize --config_file tmp_workflow/configs/config_b.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f79ce00",
   "metadata": {},
   "source": [
    "<a id=\"interpret-optimizer-first\"></a>\n",
    "### 1.2.5) Interpret first optimizer run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6029eeeb",
   "metadata": {},
   "source": [
    "**Understanding Evaluation Outputs**\n",
    "\n",
    "This evaluation will have generated two artifacts for analysis at the `output_dir` specified in `config_b.yml`:\n",
    " - **`answer_accuracy_output.json`**\n",
    " - **`workflow_output.json`**\n",
    " - **`llm_latency_output.json`**\n",
    " - **`token_efficiency_output.json`**\n",
    "\n",
    "**Interpreting `trajectory_accuracy_output.json`**\n",
    "\n",
    "The `trajectory_accuracy_output.json` file contains the results of agent trajectory evaluation.\n",
    "\n",
    "**Top-level fields:**\n",
    "- **`average_score`** - Mean trajectory accuracy score across all evaluated examples (0.0 to 1.0)\n",
    "- **`eval_output_items`** - Array of individual evaluation results for each test case\n",
    "\n",
    "**Per-item fields:**\n",
    "- **`id`** - Unique identifier for the test case\n",
    "- **`score`** - Trajectory accuracy score for this specific example (0.0 to 1.0)\n",
    "- **`reasoning`** - Evaluation reasoning, either:\n",
    "  - String containing error message if evaluation failed\n",
    "  - Object with:\n",
    "    - **`reasoning`** - LLM judge's explanation of the score\n",
    "    - **`trajectory`** - Array of [AgentAction, Output] pairs showing the agent's execution path\n",
    "\n",
    "The trajectory accuracy evaluator assesses whether the agent used appropriate tools, followed a logical sequence of steps, and efficiently reached the correct answer.\n",
    "\n",
    "**Interpreting `workflow_output.json`**\n",
    "\n",
    "The `workflow_output.json` file contains the raw execution results from running the workflow on each test case.\n",
    "\n",
    "**Top-level fields:**\n",
    "- **`output_items`** - Array of workflow execution results for each test case in the dataset\n",
    "\n",
    "**Per-item fields:**\n",
    "- **`id`** - Unique identifier matching the test case ID\n",
    "- **`input_obj`** - The input question or prompt sent to the workflow\n",
    "- **`output_obj`** - The final answer generated by the workflow\n",
    "- **`trajectory`** - Detailed execution trace containing:\n",
    "  - **`event_type`** - Type of event (e.g., `LLM_START`, `LLM_END`, `TOOL_START`, `TOOL_END`, `SPAN_START`, `SPAN_END`)\n",
    "  - **`event_timestamp`** - Unix timestamp of when the event occurred\n",
    "  - **`metadata`** - Event-specific data including:\n",
    "    - Tool names and inputs\n",
    "    - LLM prompts and responses\n",
    "    - Token counts (`prompt_tokens`, `completion_tokens`)\n",
    "    - Model names\n",
    "    - Function names\n",
    "    - Error information\n",
    "\n",
    "The workflow output provides complete observability into each execution, enabling detailed analysis of agent behavior, performance profiling, and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab620774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search Optimization Results\n",
      "================================================================================\n",
      "\n",
      "Trials Summary:\n",
      " number  values_accuracy  values_token_efficiency  values_latency             datetime_start          datetime_complete               duration params_llms.chat_completion_llm.model_name  params_llms.chat_completion_llm.temperature                                                                                                                                                                                              rep_scores  system_attrs_grid_id                                                                                                                                  system_attrs_search_space    state  pareto_optimal\n",
      "      0            0.070                    95.28           8.800 2025-10-25 15:06:58.998732 2025-10-25 15:07:58.563088 0 days 00:00:59.564356                meta/llama-3.1-70b-instruct                                          0.0 [[0.1, 161.1, 11.64], [0.0, 103.4, 4.78], [0.0, 47.6, 5.66], [0.1, 23.9, 4.16], [0.1, 23.9, 4.45], [0.0, 23.9, 4.59], [0.1, 88.3, 10.54], [0.1, 159.6, 12.76], [0.1, 160.3, 14.6], [0.1, 160.8, 14.82]]                     0 {'llms.chat_completion_llm.model_name': ['meta/llama-3.1-8b-instruct', 'meta/llama-3.1-70b-instruct'], 'llms.chat_completion_llm.temperature': [0.0, 0.7]} COMPLETE            True\n",
      "      1            0.050                    21.78           0.967 2025-10-25 15:07:58.563250 2025-10-25 15:08:23.847898 0 days 00:00:25.284648                meta/llama-3.1-70b-instruct                                          0.7                [[0.1, 40.3, 2.85], [0.1, 52.0, 2.33], [0.1, 30.7, 0.96], [0.0, 9.4, 0.57], [0.1, 9.4, 0.72], [0.0, 9.4, 0.24], [0.0, 9.4, 0.13], [0.1, 23.9, 0.9], [0.0, 17.7, 0.67], [0.0, 15.6, 0.3]]                     1 {'llms.chat_completion_llm.model_name': ['meta/llama-3.1-8b-instruct', 'meta/llama-3.1-70b-instruct'], 'llms.chat_completion_llm.temperature': [0.0, 0.7]} COMPLETE            True\n",
      "      2            0.015                    36.00           0.454 2025-10-25 15:08:23.848007 2025-10-25 15:08:40.149963 0 days 00:00:16.301956                 meta/llama-3.1-8b-instruct                                          0.7            [[0.1, 117.2, 1.42], [0.0, 87.2, 1.05], [0.0, 9.4, 0.09], [0.0, 9.4, 0.13], [0.0, 9.4, 0.1], [0.0, 9.4, 0.09], [0.0, 9.4, 0.09], [0.025, 66.9, 0.98], [0.0, 32.3, 0.47], [0.025, 9.4, 0.12]]                     2 {'llms.chat_completion_llm.model_name': ['meta/llama-3.1-8b-instruct', 'meta/llama-3.1-70b-instruct'], 'llms.chat_completion_llm.temperature': [0.0, 0.7]} COMPLETE           False\n",
      "      3            0.020                    14.79           0.154 2025-10-25 15:08:40.150077 2025-10-25 15:08:55.548497 0 days 00:00:15.398420                 meta/llama-3.1-8b-instruct                                          0.0                    [[0.0, 49.5, 0.51], [0.0, 9.4, 0.09], [0.0, 32.6, 0.33], [0.1, 9.4, 0.09], [0.0, 0.0, 0.0], [0.0, 9.4, 0.1], [0.0, 9.4, 0.13], [0.0, 9.4, 0.09], [0.1, 9.4, 0.09], [0.0, 9.4, 0.11]]                     3 {'llms.chat_completion_llm.model_name': ['meta/llama-3.1-8b-instruct', 'meta/llama-3.1-70b-instruct'], 'llms.chat_completion_llm.temperature': [0.0, 0.7]} COMPLETE            True\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the optimizer results\n",
    "trials_df_path = Path(\"tmp_workflow/eval_output/optimizer/trials_dataframe_params.csv\")\n",
    "\n",
    "if trials_df_path.exists():\n",
    "    trials_df = pd.read_csv(trials_df_path)\n",
    "\n",
    "    print(\"Grid Search Optimization Results\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nTrials Summary:\")\n",
    "    print(trials_df.to_string(index=False))\n",
    "    print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3836ec19",
   "metadata": {},
   "source": [
    "The results above show:\n",
    " \n",
    "**Grid Search Optimization Summary:**\n",
    "- The optimizer evaluated all combinations of models and temperatures defined in the search space\n",
    "- Each configuration was tested multiple times (repetitions) to account for variability\n",
    "- Three key metrics were tracked: accuracy, token efficiency (tokens used), and latency (response time)\n",
    " \n",
    " **Understanding the Statistics:**\n",
    "- **Mean**: Average performance across all repetitions for each model\n",
    "- **Standard Deviation (±)**: Measure of variability in performance\n",
    "- **95% Confidence Interval**: Range where we expect 95% of results to fall\n",
    "\n",
    "**Key Insights:**\n",
    " - Different models show different trade-offs between accuracy, efficiency, and speed\n",
    "- Temperature settings affect response variability and quality\n",
    "- The \"Best Configuration\" represents the optimal balance based on the weighted combination of all metrics\n",
    " \n",
    "**Interpreting Your Results:**\n",
    "When you run this optimization, look for:\n",
    "- Which model/temperature combination achieves the highest aggregated accuracy\n",
    "- How token efficiency varies between models (lower is more efficient)\n",
    "- Latency differences (lower is faster)\n",
    "- The confidence intervals to understand result stability\n",
    "\n",
    "The optimizer automatically selects the best configuration and saves it to `optimized_config.yml` for use in production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59876571",
   "metadata": {},
   "source": [
    "<a id=\"optimize-tool-calling-agents\"></a>\n",
    "# 2.0) Optimized model and parameter selection for tool-calling agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7223a3b2",
   "metadata": {},
   "source": [
    "<a id=\"create-triage-agent\"></a>\n",
    "## 2.1) Create a tool-calling agent\n",
    "As we explained above, in many real-world applications straightforward chat completions requests may not be adequate without agentic tool-calling integration. Therefore, for the next exercise we are going to build a similar optimize pipeline for an advanced tool calling agent: the [Alert Triage Agent](https://github.com/NVIDIA/NeMo-Agent-Toolkit/tree/develop/examples/advanced_agents/alert_triage_agent). This agent uses tool calling to automate the triage of server-monitoring alerts. It demonstrates how to build an intelligent troubleshooting workflow using NeMo Agent toolkit and LangGraph.\n",
    "\n",
    "The Alert Triage Agent is an advanced example that demonstrates:\n",
    "- **Multi-tool orchestration** - Dynamically selects and uses diagnostic tools\n",
    "- **Structured report generation** - Creates comprehensive analysis reports\n",
    "- **Root cause categorization** - Classifies alerts into predefined categories\n",
    "- **Offline evaluation mode** - Test with synthetic data before live deployment\n",
    "\n",
    "We aim to demonstrate the power of model evaluation and optimization on agentic AI platforms. There are many foundational models to choose as your agent's backbone and academic benchmarks are not always representative of potential performance on your institutional data (refer to training data leakage and data domain shift research for more motivation).\n",
    "\n",
    "<div style=\"color: red; font-style: italic;\">\n",
    "<strong>Note:</strong> As the Alert Triage Agent is not shipped with the NAT PyPi package, we will either clone it from GitHub (by selecting your branch of choice), or if the package was installed with the `-e` editable code flag, we can work locally. We will parameterize the path to this agent to easily alter the configuration in the next cell\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a101122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Alert Triage Agent Installation\n",
      "============================================================\n",
      "\n",
      "Options:\n",
      "  - Enter 'local' for editable install from local repository\n",
      "  - Enter a branch name (e.g., 'develop', 'main') for git install\n",
      "============================================================\n",
      "\n",
      "Installing alert triage agent in editable mode from local repository...\n",
      "Obtaining file:///Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of nat-alert-triage-agent to determine which version is compatible with other requirements. This could take a while.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement nvidia-nat~=1.4 (from nat-alert-triage-agent) (from versions: 1.1.0a20251020, 1.2.0a20250813, 1.2.0rc5, 1.2.0rc6, 1.2.0rc7, 1.2.0rc8, 1.2rc9, 1.2.0rc10, 1.2.0, 1.2.1rc1, 1.2.1, 1.3.dev0, 1.3.0.dev2, 1.3a0, 1.3a20250818, 1.3a20250819, 1.3.0a20250822, 1.3.0a20250823, 1.3.0a20250824, 1.3.0a20250826, 1.3.0a20250827, 1.3.0a20250828, 1.3.0a20250829, 1.3.0a20250830, 1.3.0a20250831, 1.3.0a20250901, 1.3.0a20250902, 1.3.0a20250904, 1.3.0a20250906, 1.3.0a20250909, 1.3.0a20250910, 1.3.0a20250917, 1.3.0a20250922, 1.3.0a20250923, 1.3.0a20250924, 1.3.0a20250925, 1.3.0a20250926, 1.3.0a20250928, 1.3.0a20250929, 1.3.0a20250930, 1.3.0a20251001, 1.3.0a20251002, 1.3.0a20251004, 1.3.0a20251005, 1.3.0a20251006, 1.3.0a20251007, 1.3.0a20251008, 1.3.0a20251009, 1.3.0a20251012, 1.3.0a20251013, 1.3.0a20251021, 1.3.0a20251022, 1.3.0a20251023, 1.3.0a20251024, 1.3.0rc1, 1.3.0rc2, 1.3.0rc3, 1.3.0rc4, 1.3.0rc5, 1.3.0rc6, 1.3.0, 1.4.0a20251008, 1.4.0a20251010, 1.4.0a20251011, 1.4.0a20251012, 1.4.0a20251013, 1.4.0a20251014, 1.4.0a20251015, 1.4.0a20251021, 1.4.0a20251022, 1.4.0a20251023, 1.4.0a20251024)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for nvidia-nat~=1.4\u001b[0m\u001b[31m\n",
      "\u001b[0m✓ Installed from local path: /Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/notebooks/../../examples/advanced_agents/alert_triage_agent\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Simple input prompt for branch selection\n",
    "print(\"=\" * 60)\n",
    "print(\"Alert Triage Agent Installation\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nOptions:\")\n",
    "print(\"  - Enter 'local' for editable install from local repository\")\n",
    "print(\"  - Enter a branch name (e.g., 'develop', 'main') for git install\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "branch_name = input(\"\\nEnter your choice: \").strip()\n",
    "\n",
    "if branch_name.lower() == 'local':\n",
    "    # Local editable install\n",
    "    print(\"\\nInstalling alert triage agent in editable mode from local repository...\")\n",
    "\n",
    "    # Try to find the local path relative to current directory\n",
    "    from pathlib import Path\n",
    "    # path-check-skip-next-line\n",
    "    local_path = Path('../../examples/advanced_agents/alert_triage_agent')\n",
    "\n",
    "    if local_path.exists():\n",
    "        get_ipython().system(f'pip install -e {local_path}')\n",
    "        print(f\"✓ Installed from local path: {local_path.absolute()}\")\n",
    "    else:\n",
    "        print(f\"✗ Error: Local path not found: {local_path.absolute()}\")\n",
    "        print(\"Make sure you're running this from the correct directory\")\n",
    "else:\n",
    "    # Git install from specified branch\n",
    "    print(f\"\\nInstalling alert triage agent from branch: {branch_name}\")\n",
    "    get_ipython().system(f'pip install --no-deps \"git+https://github.com/NVIDIA/NeMo-Agent-Toolkit.git@{branch_name}#subdirectory=examples/advanced_agents/alert_triage_agent\"')\n",
    "    print(f\"✓ Installed from git branch: {branch_name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1fc34f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package data directory: /Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data\n"
     ]
    }
   ],
   "source": [
    "import importlib.resources\n",
    "\n",
    "# Find the installed package data directory\n",
    "package_data = importlib.resources.files('nat_alert_triage_agent').joinpath('data')\n",
    "\n",
    "maintenance_csv = str(package_data / 'maintenance_static_dataset.csv')\n",
    "offline_csv = str(package_data / 'offline_data.csv')\n",
    "benign_json = str(package_data / 'benign_fallback_offline_data.json')\n",
    "offline_json = str(package_data / 'offline_data.json')\n",
    "\n",
    "print(f\"Package data directory: {package_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc40fd05",
   "metadata": {},
   "source": [
    "<a id=\"configure-triage-agent\"></a>\n",
    "## 2.2) Configure the tool-calling agent\n",
    "\n",
    "**Configuring the Alert Triage Agent**\n",
    "\n",
    "The Alert Triage Agent requires several components:\n",
    "\n",
    "1. **Diagnostic Tools** - Hardware checks, network connectivity, performance monitoring, telemetry analysis\n",
    "2. **Sub-agents** - Telemetry metrics analysis agent that coordinates multiple telemetry tools\n",
    "3. **Categorizer** - Classifies root causes into predefined categories\n",
    "4. **Maintenance Check** - Filters out alerts during maintenance windows\n",
    "\n",
    "We'll create a **local configuration file** and run in **offline mode** using synthetic data.\n",
    "\n",
    "In the configuration file, you can see the list of LLMs that we have predefined to be compared when the optimizer runs. We will only run the initial search across two models, for brevity and token efficiency. However, you can uncomment the entire list of 11 models (or add [more models](https://catalog.ngc.nvidia.com/)) to run a more robust search. This model will be used as the agent's backbone LLM for reasoning steps. The `tool_reasoning_llm` and `nim_rag_eval_llm` remain fixed to `meta/llama-3.1-70b-instruct`, but in a modified evaluation these models could be evaluated as well. \n",
    "```\n",
    "- Meta: llama-3.1-8b-instruct\n",
    "- Meta: llama-3.1-70b-instruct\n",
    "- Meta: llama-3.1-405b-instruct\n",
    "- Meta: llama-3.3-3b-instruct\n",
    "- Meta: llama-3.3-70b-instruct\n",
    "- Meta: llama-4-scout-17b-16e-instruct\n",
    "- OpenAI: gpt-oss-20b\n",
    "- OpenAI: gpt-oss-120b\n",
    "- IBM: granite-3.3-8b-instruct\n",
    "- MistralAI: mistral-small-3.1-24b-instruct-2503\n",
    "- MistralAI: mistral-medium-3-instruct\n",
    "```\n",
    "\n",
    "We additionally provide two different vlaues for `temperature` to exemplify concurrent model and parameter searches:\n",
    "```\n",
    "- 0.0\n",
    "- 0.5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97b2e49",
   "metadata": {},
   "source": [
    "<div style=\"color: red; font-style: italic;\">\n",
    "<strong>Developer warning:</strong> Running the optimizer can consume a significant amount of LLM inference tokens. To protect users from unexpected costs only 2 models have been left uncommented in the config below. Uncomment models to increase the search space.\n",
    "</div>\n",
    "\n",
    "We will create a YAML configuration file using Python code rather than a static file. This approach allows us to dynamically reference the package data directory and ensures the configuration is created in the notebook's working directory, making it easier to modify and experiment with different settings for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e8f1940e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./tmp_workflow/configs/alert_triage_config_model_selection.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./tmp_workflow/configs/alert_triage_config_model_selection.yml\n",
    "# path-check-skip-begin\n",
    "functions:\n",
    "  hardware_check:\n",
    "    _type: hardware_check\n",
    "    llm_name: tool_reasoning_llm\n",
    "    offline_mode: true\n",
    "  host_performance_check:\n",
    "    _type: host_performance_check\n",
    "    llm_name: tool_reasoning_llm\n",
    "    offline_mode: true\n",
    "  monitoring_process_check:\n",
    "    _type: monitoring_process_check\n",
    "    llm_name: tool_reasoning_llm\n",
    "    offline_mode: true\n",
    "  network_connectivity_check:\n",
    "    _type: network_connectivity_check\n",
    "    llm_name: tool_reasoning_llm\n",
    "    offline_mode: true\n",
    "  telemetry_metrics_host_heartbeat_check:\n",
    "    _type: telemetry_metrics_host_heartbeat_check\n",
    "    llm_name: tool_reasoning_llm\n",
    "    offline_mode: true\n",
    "  telemetry_metrics_host_performance_check:\n",
    "    _type: telemetry_metrics_host_performance_check\n",
    "    llm_name: tool_reasoning_llm\n",
    "    offline_mode: true\n",
    "  telemetry_metrics_analysis_agent:\n",
    "    _type: telemetry_metrics_analysis_agent\n",
    "    tool_names:\n",
    "      - telemetry_metrics_host_heartbeat_check\n",
    "      - telemetry_metrics_host_performance_check\n",
    "    llm_name: agent_llm\n",
    "  maintenance_check:\n",
    "    _type: maintenance_check\n",
    "    llm_name: agent_llm\n",
    "    static_data_path: PLACEHOLDER_maintenance_static_dataset.csv\n",
    "  categorizer:\n",
    "    _type: categorizer\n",
    "    llm_name: agent_llm\n",
    "\n",
    "workflow:\n",
    "  _type: alert_triage_agent\n",
    "  tool_names:\n",
    "    - hardware_check\n",
    "    - host_performance_check\n",
    "    - monitoring_process_check\n",
    "    - network_connectivity_check\n",
    "    - telemetry_metrics_analysis_agent\n",
    "  llm_name: agent_llm\n",
    "  offline_mode: true\n",
    "  offline_data_path: PLACEHOLDER_offline_data.csv\n",
    "  benign_fallback_data_path: PLACEHOLDER_benign_fallback_offline_data.json\n",
    "\n",
    "llms:\n",
    "  agent_llm:\n",
    "    _type: nim\n",
    "    model_name: meta/llama-3.1-8b-instruct\n",
    "    temperature: 0.0\n",
    "    max_tokens: 2048\n",
    "    optimizable_params:\n",
    "      - model_name\n",
    "      - temperature\n",
    "    search_space:\n",
    "      model_name:\n",
    "        values:\n",
    "          - meta/llama-3.1-8b-instruct\n",
    "          - meta/llama-3.1-70b-instruct\n",
    "          # - meta/llama-3.1-405b-instruct\n",
    "          # - meta/llama-3.3-3b-instruct\n",
    "          # - meta/llama-3.3-70b-instruct\n",
    "          # - meta/llama-4-scout-17b-16e-instruct\n",
    "          # - openai/gpt-oss-20b\n",
    "          # - openai/gpt-oss-120b\n",
    "          # - ibm/granite-3.3-8b-instruct\n",
    "          # - mistralai/mistral-small-3.1-24b-instruct-2503\n",
    "          # - mistralai/mistral-medium-3-instruct\n",
    "      temperature:\n",
    "        values:\n",
    "          - 0.0\n",
    "          - 0.5\n",
    "  tool_reasoning_llm:\n",
    "    _type: nim\n",
    "    model_name: meta/llama-3.1-70b-instruct\n",
    "    temperature: 0.2\n",
    "    max_tokens: 2048\n",
    "  nim_rag_eval_llm:\n",
    "    _type: nim\n",
    "    model_name: meta/llama-3.1-70b-instruct\n",
    "    max_tokens: 8\n",
    "\n",
    "eval:\n",
    "  general:\n",
    "    output_dir: ./tmp_workflow/alert_triage_model_selection_output/\n",
    "    dataset:\n",
    "      _type: json\n",
    "      file_path: PLACEHOLDER_offline_data.json\n",
    "  evaluators:\n",
    "    accuracy:\n",
    "      _type: ragas\n",
    "      metric: AnswerAccuracy\n",
    "      llm_name: nim_rag_eval_llm\n",
    "    groundedness:\n",
    "      _type: ragas\n",
    "      metric: ResponseGroundedness\n",
    "      llm_name: nim_rag_eval_llm\n",
    "    relevance:\n",
    "      _type: ragas\n",
    "      metric: ContextRelevance\n",
    "      llm_name: nim_rag_eval_llm\n",
    "    classification_accuracy:\n",
    "      _type: classification_accuracy\n",
    "    llm_latency:\n",
    "      _type: avg_llm_latency\n",
    "    token_efficiency:\n",
    "      _type: avg_tokens_per_llm_end\n",
    "  profiler:\n",
    "    token_uniqueness_forecast: true\n",
    "    workflow_runtime_forecast: true\n",
    "    compute_llm_metrics: true\n",
    "    csv_exclude_io_text: true\n",
    "    prompt_caching_prefixes:\n",
    "      enable: true\n",
    "      min_frequency: 0.1\n",
    "    bottleneck_analysis:\n",
    "      enable_nested_stack: true\n",
    "    concurrency_spike_analysis:\n",
    "      enable: true\n",
    "      spike_threshold: 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5e86bf",
   "metadata": {},
   "source": [
    "Above we have defined the `SearchSpace` to include two different LLMs (variants of Meta's llama 3.1 model), and temperature of 0.0 and 0.5 (making 4 unique combinations via grid search).\n",
    "\n",
    "Next, let's append sime simple optimizer settings to our configuration. We will optimize specifically for the predefined `classification_accuracy` evaluator, use a grid search sampler, and **disable prompt optimization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0c5d0fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./tmp_workflow/configs/alert_triage_config_model_selection.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./tmp_workflow/configs/alert_triage_config_model_selection.yml\n",
    "optimizer:\n",
    "  output_path: ./tmp_workflow/alert_triage_model_selection_output/optimizer/\n",
    "  reps_per_param_set: 1\n",
    "  eval_metrics:\n",
    "    classification_accuracy:\n",
    "      evaluator_name: classification_accuracy\n",
    "      direction: maximize\n",
    "    llm_latency:\n",
    "      evaluator_name: llm_latency\n",
    "      direction: minimize\n",
    "  numeric:\n",
    "    enabled: true\n",
    "    sampler: grid\n",
    "  prompt:\n",
    "    enabled: false\n",
    "# path-check-skip-end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099c4dc5",
   "metadata": {},
   "source": [
    "Before running, let's replace the placeholder paths in our config, depending on where we have installed the Alert Traige Agent. This step is only needed for compatibility of this notebook to source NAT in multiple ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "20ec99f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Config written with data paths from: /Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data\n"
     ]
    }
   ],
   "source": [
    "# Replace placeholder paths with actual package data paths\n",
    "import importlib.resources\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the package data path\n",
    "package_data = importlib.resources.files('nat_alert_triage_agent').joinpath('data')\n",
    "\n",
    "# Read the YAML file\n",
    "config_path = Path('./tmp_workflow/configs/alert_triage_config_model_selection.yml')\n",
    "with open(config_path) as f:\n",
    "    config_content = f.read()\n",
    "\n",
    "# Replace placeholders with actual paths\n",
    "replacements = {\n",
    "    'PLACEHOLDER_maintenance_static_dataset.csv': str(package_data / 'maintenance_static_dataset.csv'),\n",
    "    'PLACEHOLDER_offline_data.csv': str(package_data / 'offline_data.csv'),\n",
    "    'PLACEHOLDER_benign_fallback_offline_data.json': str(package_data / 'benign_fallback_offline_data.json'),\n",
    "    'PLACEHOLDER_offline_data.json': str(package_data / 'offline_data.json')\n",
    "}\n",
    "\n",
    "for placeholder, actual_path in replacements.items():\n",
    "    config_content = config_content.replace(placeholder, actual_path)\n",
    "\n",
    "# Write back to file\n",
    "with open(config_path, 'w') as f:\n",
    "    f.write(config_content)\n",
    "\n",
    "print(f\"✓ Config written with data paths from: {package_data}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299df6c9",
   "metadata": {},
   "source": [
    "<a id=\"test-triage-agent\"></a>\n",
    "## 2.3) Test the tool-calling agent\n",
    "\n",
    "Let's test the Alert Triage Agent with a single alert. This alert is an \"InstanceDown\" alert that, according to the offline dataset, is actually a false positive (the system is healthy).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34b468a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-25 15:25:04 - INFO     - nat.cli.commands.start:192 - Starting NAT from config file: 'tmp_workflow/configs/alert_triage_config_model_selection.yml'\n",
      "2025-10-25 15:25:04 - INFO     - nat_alert_triage_agent:104 - Preloaded test data from: /Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/offline_data.csv\n",
      "2025-10-25 15:25:04 - INFO     - nat_alert_triage_agent:108 - Preloaded benign fallback data from: /Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/benign_fallback_offline_data.json\n",
      "2025-10-25 15:25:04 - INFO     - nat_alert_triage_agent:80 - ================================================Running in offline mode=================================================\n",
      "\n",
      "Configuration Summary:\n",
      "--------------------\n",
      "Workflow Type: alert_triage_agent\n",
      "Number of Functions: 9\n",
      "Number of Function Groups: 0\n",
      "Number of LLMs: 3\n",
      "Number of Embedders: 0\n",
      "Number of Memory: 0\n",
      "Number of Object Stores: 0\n",
      "Number of Retrievers: 0\n",
      "Number of TTC Strategies: 0\n",
      "Number of Authentication Providers: 0\n",
      "\n",
      "2025-10-25 15:25:04 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-0.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:27:15 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "2025-10-25 15:27:15 - INFO     - nat.front_ends.console.console_front_end_plugin:102 - --------------------------------------------------\n",
      "\u001b[32mWorkflow Result:\n",
      "['**Alert Summary**\\n----------------\\n\\n* Alert ID: 0\\n* Alert Name: InstanceDown\\n* Host ID: test-instance-0.example.com\\n* Severity: Critical\\n* Description: Instance test-instance-0.example.com is not available for scraping for the last 5m. Please check: - instance is up and running; - monitoring service is in place and running; - network connectivity is ok\\n* Summary: Instance test-instance-0.example.com is down\\n* Timestamp: 2025-04-28T05:00:00.000000\\n\\n**Collected Metrics**\\n--------------------\\n\\n* Hardware Check:\\n\\t+ Power Status: ON\\n\\t+ Hardware Health: Normal\\n\\t+ Observed Anomalies: None\\n* Network Connectivity Check:\\n\\t+ Ping Status: Successful\\n\\t+ Telnet Status: Connected\\n\\t+ Potential Cause of Connectivity Issue: No issue\\n* Telemetry Metrics Analysis:\\n\\t+ CPU Usage: Normal fluctuations, no obvious pattern of cyclic usage surges or anomalous peaks\\n\\t+ Maximum CPU Usage: Below 11%\\n\\n**Analysis**\\n------------\\n\\nBased on the collected metrics, it appears that the host \\'test-instance-0.example.com\\' is up and running, with no obvious issues with its CPU usage or network connectivity. However, the alert type is \\'InstanceDown\\', which suggests that the host may be down or not responding. This discrepancy between the telemetry data and the alert type may indicate that there is an issue with the monitoring service or the alerting system, rather than the host itself.\\n\\n**Recommended Actions**\\n-----------------------\\n\\n1. Verify the system\\'s workload and ambient temperature to determine if the high CPU temperature is expected.\\n2. Check the system\\'s cooling configuration and ensure that the fan is functioning correctly.\\n3. Monitor the system\\'s temperature and fan speed over time to see if the readings remain consistent or if there are any fluctuations.\\n4. Review system logs and other monitoring data to see if there are any other indicators of issues or anomalies.\\n5. If the alert persists, consider running additional diagnostic tests to identify any underlying hardware or software issues.\\n\\n**Alert Status**\\n----------------\\n\\n* Alert Status: Abnormal but benign\\n\\nNote: The alert status is set to \"Abnormal but benign\" because the host is up and running, but the alert type is inconsistent with the telemetry data. Further investigation is required to determine the root cause of the issue.\\n\\n## Root Cause Category\\nnetwork_connectivity\\n\\nThe report indicates that the host is up and running, with successful ping and telnet tests, suggesting that network connectivity is not the issue. However, the alert type is \\'InstanceDown\\', which suggests that the host may be down or not responding, indicating a discrepancy between the telemetry data and the alert type. This discrepancy may be due to an issue with the monitoring service or the alerting system rather than the host itself.']\u001b[39m\n",
      "--------------------------------------------------\n",
      "2025-10-25 15:27:15 - INFO     - nat_alert_triage_agent:164 - Cleaning up\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "alert = {\n",
    "    \"alert_id\": 0,\n",
    "    \"alert_name\": \"InstanceDown\",\n",
    "    \"host_id\": \"test-instance-0.example.com\",\n",
    "    \"severity\": \"critical\",\n",
    "    \"description\": (\n",
    "        \"Instance test-instance-0.example.com is not available for scraping for the last 5m. \"\n",
    "        \"Please check: - instance is up and running; - monitoring service is in place and running; \"\n",
    "        \"- network connectivity is ok\"\n",
    "    ),\n",
    "    \"summary\": \"Instance test-instance-0.example.com is down\",\n",
    "    \"timestamp\": \"2025-04-28T05:00:00.000000\"\n",
    "}\n",
    "\n",
    "!nat run --config_file tmp_workflow/configs/alert_triage_config_model_selection.yml --input '{json.dumps(alert)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ac49a9",
   "metadata": {},
   "source": [
    "After running the cell above, we have confirmed that the tool calling agent is properly configured and ready for a naive evaluation. This evaluation will be our performance baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730ef191",
   "metadata": {},
   "source": [
    "<a id=\"eval-triage-agent1\"></a>\n",
    "## 2.4) Evaluate the tool-calling agent (naive parameters)\n",
    "\n",
    "*using `nat eval`...*\n",
    "\n",
    "Now let's run a full evaluation on the Alert Triage Agent using the complete offline dataset. This dataset contains seven alerts with different root causes:\n",
    "\n",
    "- **False positives** - System appears healthy despite alert\n",
    "- **Hardware issues** - Hardware failures or degradation  \n",
    "- **Software issues** - Malfunctioning monitoring services\n",
    "- **Maintenance** - Scheduled maintenance windows\n",
    "- **Repetitive behavior** - Benign recurring patterns\n",
    "\n",
    "The evaluation will measure:\n",
    "1. **Classification Accuracy** - How well the agent categorizes root causes\n",
    "2. **Answer Accuracy** - How well the generated reports match expected outcomes (using RAGAS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55c4fcc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-25 15:27:31 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: tmp_workflow/configs/alert_triage_config_model_selection.yml\n",
      "2025-10-25 15:27:35 - INFO     - nat_alert_triage_agent:104 - Preloaded test data from: /Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/offline_data.csv\n",
      "2025-10-25 15:27:35 - INFO     - nat_alert_triage_agent:108 - Preloaded benign fallback data from: /Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/benign_fallback_offline_data.json\n",
      "2025-10-25 15:27:35 - INFO     - nat_alert_triage_agent:80 - ================================================Running in offline mode=================================================\n",
      "Running workflow:   0%|                                   | 0/7 [00:00<?, ?it/s]2025-10-25 15:27:36 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-0.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:27:36 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-1.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:27:36 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-2.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:27:36 - INFO     - nat_alert_triage_agent:258 - Host: [test-instance-3.example.com] is under maintenance according to the maintenance database\n",
      "2025-10-25 15:27:39 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-4.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:27:39 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-5.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:27:39 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-6.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:27:39 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  14%|███▊                       | 1/7 [00:03<00:18,  3.14s/it]2025-10-25 15:28:47 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  29%|███████▋                   | 2/7 [01:10<03:25, 41.04s/it]2025-10-25 15:28:55 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  43%|███████████▌               | 3/7 [01:19<01:44, 26.10s/it]2025-10-25 15:29:18 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  57%|███████████████▍           | 4/7 [01:42<01:14, 24.89s/it]2025-10-25 15:30:07 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  71%|███████████████████▎       | 5/7 [02:31<01:07, 33.62s/it]2025-10-25 15:30:31 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  86%|███████████████████████▏   | 6/7 [02:54<00:30, 30.23s/it]2025-10-25 15:30:38 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow: 100%|███████████████████████████| 7/7 [03:01<00:00, 25.95s/it]\n",
      "Evaluating Ragas nv_accuracy:   0%|                       | 0/7 [00:00<?, ?it/s]\n",
      "Evaluating Ragas nv_response_groundedness:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Evaluating Ragas nv_context_relevance:   0%|              | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating classification accuracy:   0%|                 | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                         | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating classification accuracy: 100%|████████| 7/7 [00:00<00:00, 209.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg LLM Latency: 100%|████████████████| 7/7 [00:00<00:00, 210.06it/s]\n",
      "Evaluating Avg Tokens/LLM_END: 100%|█████████████| 7/7 [00:00<00:00, 210.16it/s]\n",
      "\n",
      "Evaluating Ragas nv_response_groundedness:  14%|▎ | 1/7 [00:00<00:02,  2.39it/s]\u001b[AAn error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "Evaluating Ragas nv_response_groundedness:  29%|▌ | 2/7 [00:00<00:01,  3.87it/s]\u001b[AAn error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_context_relevance:  14%|▊     | 1/7 [00:00<00:02,  2.65it/s]\u001b[A\u001b[AAn error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_context_relevance:  71%|████▎ | 5/7 [00:00<00:00, 12.35it/s]\u001b[A\u001b[AAn error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:30:39 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  14%|██▏            | 1/7 [00:01<00:07,  1.27s/it]2025-10-25 15:30:39 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  29%|████▎          | 2/7 [00:01<00:02,  1.71it/s]2025-10-25 15:30:39 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "Evaluating Ragas nv_response_groundedness:  57%|█▏| 4/7 [00:01<00:00,  3.46it/s]\u001b[A2025-10-25 15:30:39 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:30:40 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  57%|████████▌      | 4/7 [00:01<00:00,  3.51it/s]An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "Evaluating Ragas nv_response_groundedness:  71%|█▍| 5/7 [00:01<00:00,  4.20it/s]\u001b[AAn error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:30:40 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:30:40 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  71%|██████████▋    | 5/7 [00:01<00:00,  4.16it/s]2025-10-25 15:30:40 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:30:40 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "Evaluating Ragas nv_accuracy: 100%|███████████████| 7/7 [00:02<00:00,  4.12it/s]\n",
      "Evaluating Ragas nv_context_relevance: 100%|██████| 7/7 [00:02<00:00,  2.37it/s]\u001b[A\n",
      "Evaluating Ragas nv_response_groundedness: 100%|██| 7/7 [00:03<00:00,  2.10it/s]\n",
      "Evaluating Ragas nv_accuracy: 100%|███████████████| 7/7 [00:03<00:00,  1.82it/s]\n",
      "2025-10-25 15:30:42 - INFO     - nat_alert_triage_agent:164 - Cleaning up\n",
      "2025-10-25 15:30:42 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:30:42 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/alert_triage_model_selection_output/workflow_output.json\n",
      "2025-10-25 15:30:42 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/classification_accuracy_output.json\n",
      "2025-10-25 15:30:42 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/llm_latency_output.json\n",
      "2025-10-25 15:30:42 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/token_efficiency_output.json\n",
      "2025-10-25 15:30:42 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/relevance_output.json\n",
      "2025-10-25 15:30:42 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/groundedness_output.json\n",
      "2025-10-25 15:30:42 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/accuracy_output.json\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!nat eval --config_file ./tmp_workflow/configs/alert_triage_config_model_selection.yml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe817e6",
   "metadata": {},
   "source": [
    "**Understanding Alert Triage Evaluation Results**\n",
    "\n",
    "The evaluation generates several output files in the `alert_triage_output` directory:\n",
    "\n",
    "1. **classification_accuracy_output.json** - Root cause classification metrics\n",
    "   - Shows accuracy, precision, recall, and F1 scores for each category\n",
    "   - Contains confusion matrix for detailed analysis\n",
    "   \n",
    "2. **rag_accuracy_output.json** - Answer quality metrics\n",
    "   - Measures how well generated reports match expected outcomes\n",
    "   - Uses LLM-as-a-judge to evaluate report quality\n",
    "\n",
    "3. **workflow_output.json** - Complete execution traces\n",
    "   - Contains full agent trajectories with tool calls\n",
    "   - Includes generated reports for each alert\n",
    "   - Shows token usage and performance metrics\n",
    "\n",
    "Let's examine the classification accuracy results:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754384df",
   "metadata": {},
   "source": [
    "We see that the classification accuracy results are around 43% based on RAG accuracy results of 46%.\n",
    "\n",
    "Next we will run the optimizer over a variety of models and some reasonable hyperparameters, then use that optimal configuration and run the evaluation again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ddbbe01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Alerts Evaluated: 7\n",
      "Classification Accuracy Average Score: 57.00%\n",
      "LLM Latency Average Score: 8.69sec\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load and display classification accuracy results\n",
    "# path-check-skip-next-line\n",
    "with open('./tmp_workflow/alert_triage_model_selection_output/classification_accuracy_output.json') as f:\n",
    "    classification_results = json.load(f)\n",
    "print(f\"Total Alerts Evaluated: {len(classification_results['eval_output_items'])}\")\n",
    "print(f\"Classification Accuracy Average Score: {classification_results['average_score']:.2%}\")\n",
    "\n",
    "# Load and display RAG accuracy results\n",
    "# path-check-skip-next-line\n",
    "with open('./tmp_workflow/alert_triage_model_selection_output/llm_latency_output.json') as f:\n",
    "    latency_results = json.load(f)\n",
    "\n",
    "print(f\"LLM Latency Average Score: {latency_results['average_score']}sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2c2cf1",
   "metadata": {},
   "source": [
    "<a id=\"optimize-triage-agent\"></a>\n",
    "## 2.5) Optimize the tool-calling agent's LLM\n",
    "\n",
    "*using `nat optimize`...*\n",
    "\n",
    "Next we will run `nat optimize` for the Alert Traige Agent using a GridSearch sweep over the `OptimizableField`s in `alert_triage_config.yml`. In this case, we are just comparing backbone LLM models for the core agent, not the `tool_reasoning_llm`. Optimizable fields have been previously explained in this notebook, but in this case we are going to run a similar optimization pass over a complex tool-calling agent to demonstrate the power of `nat optimize` at scale.\n",
    "\n",
    "<div style=\"color: red; font-style: italic;\">\n",
    "<strong>Developer warning:</strong> Running the optimizer can take significant time (~30 minutes for search space of n=10) and  LLM inference tokens. Double check your config for unneeded search parameters prior to running.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3ddd831f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-25 15:44:52 - WARNING  - nat.experimental.decorators.experimental_warning_decorator:59 - The Optimizer feature is experimental and the API may change in future releases. Future versions may introduce breaking changes without notice. Function: nat.profiler.parameter_optimization.optimizer_runtime.optimize_config\n",
      "2025-10-25 15:44:57 - WARNING  - nat.experimental.decorators.experimental_warning_decorator:59 - The Optimizer feature is experimental and the API may change in future releases. Future versions may introduce breaking changes without notice. Function: nat.profiler.parameter_optimization.parameter_optimizer.optimize_parameters\n",
      "2025-10-25 15:44:57 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:70 - Using Grid sampler for numeric optimization\n",
      "\u001b[32m[I 2025-10-25 15:44:57,266]\u001b[0m A new study created in memory with name: no-name-ab9d97c5-e04d-48ba-8a9c-f10e5b6f22b7\u001b[0m\n",
      "2025-10-25 15:44:57 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:125 - Starting numeric / enum parameter optimization...\n",
      "2025-10-25 15:45:01 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={'hardware_check': HardwareCheckToolConfig(description='This tool checks hardware health status using IPMI monitoring to detect power state, hardware degradation, and anomalies that could explain alerts. Args: host_id: str', llm_name='tool_reasoning_llm', prompt=\"You are analyzing IPMI metrics to support host monitoring and alert triage. Use the provided IPMI output to assess overall system status. Your goals are to:\\n\\n1. Determine the system's current power state.\\n2. Identify any signs of hardware degradation or failure.\\n3. Flag any anomalies that could explain why a monitoring alert was triggered.\\n\\nReview the data carefully and summarize your assessment in a clear and structured format.\\n\\nIPMI Output:\\n{input_data}\\n\\nFormat your response as follows:\\n\\nPower Status: ON / OFF\\nHardware Health: Normal / Issues Detected\\nObserved Anomalies: [List any irregularities or warning signs]\\nPossible Cause of Alert: [e.g., hardware issue, thermal spike, power fluctuation, no clear issue]\\nNext Steps: [Recommended actions or checks for further triage]\", offline_mode=True), 'host_performance_check': HostPerformanceCheckToolConfig(description='This tool retrieves CPU usage, memory usage, and hardware I/O usage details for a given host. Args: host_id: str', llm_name='tool_reasoning_llm', parsing_prompt='You are given system performance data captured from a host. Your task is to extract and organize the information into a clean, structured JSON format. The input contains system details and performance metrics, such as CPU, memory, and disk I/O.\\n\\nFollow these instructions:\\n\\n1. Identify metric categories dynamically based on the line prefixes or column headers (e.g., \"Mem:\", \"Swap:\", \"CPU:\", \"Device:\").\\n2. For each category, extract the numerical values and map them to meaningful field names.\\n3. Group related fields under sections such as \"memory_usage\", \"swap_usage\", \"cpu_usage\", \"disk_io\", etc.\\n4. Use consistent, readable key names for all fields.\\n5. Return **only** the final JSON object — no explanations or extra text.\\n\\nHere is the input data:\\n{input_data}', analysis_prompt='You are analyzing system metrics to assess CPU and memory usage. Use the output below to determine whether CPU or memory usage is abnormally high, identify which processes are consuming the most resources, and assess whether the usage patterns could explain a recent alert.\\n\\nInstructions:\\n1. Evaluate overall CPU and memory usage levels.\\n2. List the top resource-consuming processes, including their name, PID, %CPU, and %MEM.\\n3. Identify any potential causes of high usage (e.g., memory leak, runaway process, legitimate high load).\\n4. Recommend possible next steps for investigation or mitigation.\\n\\nFormat your response as a structured summary:\\n\\nCPU Usage: Normal / High (X% usage)\\nMemory Usage: Normal / High (X% usage)\\nTop Resource-Consuming Processes: [Process name, PID, %CPU, %MEM]\\nPotential Cause of High Usage: [e.g., runaway process, heavy load, memory leak]\\nNext Steps: [Suggested mitigation actions]\\n\\nSystem Metrics Output:\\n{input_data}\\n', offline_mode=True), 'monitoring_process_check': MonitoringProcessCheckToolConfig(description='This tool checks the status of critical monitoring processes and services on a target host by executing system commands. Args: host_id: str', llm_name='tool_reasoning_llm', prompt='You are checking whether the telegraf service is running on the server. Use the monitoring output below to verify its status. If it’s not running, identify possible reasons and assess the impact.\\n\\nInstructions:\\n1. Check if the telegraf process is present and active.\\n2. Evaluate the potential impact of telegraf not running on system availability or monitoring.\\n3. Identify likely causes for the process not running.\\n\\nFormat your response as a structured summary:\\n* **Telegraf Running:** Yes / No\\n* **Potential Impact:** [e.g., host seems down to the monitoring system, delayed alerting]\\n* **Possible Cause:** [e.g., process crash, misconfiguration, resource constraints]\\n* **Next Steps:** [e.g., restart telegraf, check logs]\\n\\nMonitoring Output:\\n{input_data}', offline_mode=True), 'network_connectivity_check': NetworkConnectivityCheckToolConfig(description='This tool checks network connectivity of a host by running ping and socket connection tests. Args: host_id: str', llm_name='tool_reasoning_llm', prompt='You are assisting with alert triage by checking the network connectivity status of a host. Use the outputs from `ping` and `telnet` commands to determine whether the host is reachable. If connectivity issues are detected, analyze the possible root causes and provide a structured summary of your findings.\\n\\nInstructions:\\n1. Interpret the `ping` and `telnet` results to assess host reachability.\\n2. Determine whether there is a connectivity issue.\\n3. Identify potential causes, such as network failure, firewall restrictions, or service unavailability.\\n4. Recommend appropriate next steps for troubleshooting or escalation.\\n\\nFormat your response as a structured summary:\\n\\nPing Status: Successful / Failed\\nTelnet Status: Connected / Failed\\nPotential Cause of Connectivity Issue: [e.g., network failure, firewall rules, service outage, no issue]\\nNext Steps: [e.g., check network logs, restart network services, escalate issue, or no action needed]\\n\\nPing Output:\\n{ping_data}\\n\\nTelnet Output:\\n{telnet_data}', offline_mode=True), 'telemetry_metrics_host_heartbeat_check': TelemetryMetricsHostHeartbeatCheckToolConfig(description=\"This tool checks if a host's telemetry monitoring service is reporting heartbeat metrics. This tells us if the host is up and running. Args: host_id: str\", llm_name='tool_reasoning_llm', prompt=\"The following is the telemetry metrics fetched for the host to see if it's been up and running (if result is empty, then the monitoring service on the host is down):\\n{data}\\nBased on the data, summarize the fetched data and provide a conclusion of the host's running status.\", offline_mode=True, metrics_url=''), 'telemetry_metrics_host_performance_check': TelemetryMetricsHostPerformanceCheckToolConfig(description='This tool checks the performance of the host by analyzing the CPU usage timeseries. Args: host_id: str', llm_name='tool_reasoning_llm', prompt=\"You are an expert on analyzing CPU usage timeseries. Periodic usage peaks are expected benign system behavior.\\nUser will provide data in the format of a list of lists, where each sublist contains two elements: timestamp and CPU usage percentage. User will also provide statistics on the timeseries. Write a markdown report about what was observed in the timeseries.\\n\\nExample format:\\n# CPU Usage Analysis Report\\nThe data analysis is performed on 14 days of CPU usage percentage data.\\n\\n## Data Statistics\\ndata start and end time, data point interval, CPU usage statistics\\n\\n## Observations\\nany patterns observed? Should be one of the below cases:\\n- Are there any cyclic usage surges?\\n  - What is the cycle?\\n  - What is the high and low CPU usage of the pattern?\\n- Is there one anomalous peak?\\n  - When did it happen?\\n  - What is it like before and after?\\n- No obvious pattern? A mix of patterns? => it's normal flutuation of the system (max usage less than 60%)\\n  - What is the fluctuation range?\\n\\n## Conclusion\\nSummarize the observation.\\nCategories:\\n- peak in the data means the high CPU usage is an anomaly and requires attention\\n- periodic behvior means the high usage is benign\\n- overall moderate (max usage less than 60%) usage means no issue in the system\\n\\n## Pattern Label\\nAnomalous Peak/Periodic Surges/Normal Fluctuations\\n\", offline_mode=True, metrics_url=''), 'telemetry_metrics_analysis_agent': TelemetryMetricsAnalysisAgentConfig(description='This is a telemetry metrics tool used to monitor remotely collected telemetry data. It checks server heartbeat data to determine whether the server is up and running and analyzes CPU usage patterns over the past 14 days to identify potential CPU issues. Args: host_id: str, alert_type: str', tool_names=['telemetry_metrics_host_heartbeat_check', 'telemetry_metrics_host_performance_check'], llm_name='agent_llm', prompt=\"You arg a helpful alert triage assistant. Your task is to investigate an alert that was just triggered on a specific host. You will be given two inputs:\\n- `host_id`: the identifier of the host where the alert occurred.\\n- `alert_type`: the type of alert that triggered.\\n\\nUse the tools provided below to collect relevant telemetry data for the specified host:\\n\\nTools:\\n- `telemetry_metrics_host_heartbeat_check`: Use this to check the server's heartbeat and determine if the host is currently up and responsive.\\n- `telemetry_metrics_host_performance_check`: Use this to analyze CPU usage trends over the past 14 days and identify abnormal patterns.\\n\\nInstructions:\\n1. Run the appropriate tools based on the host and alert type.\\n2. Collect and include all relevant output from the tools in your response.\\n3. Analyze the data and provide reasoning to help determine whether the telemetry supports or explains the triggered alert.\\n\\nYour response should include:\\n- Raw data from each tool\\n- A concise summary of findings\\n- Any insights or hypotheses that explain the alert\"), 'maintenance_check': MaintenanceCheckToolConfig(description='Check if a host is under maintenance during the time of an alert to help determine if the alert can be deprioritized.', llm_name='agent_llm', prompt='User will provide you with a system alert represented in JSON format. You know for a fact that there is maintenance happening for the host. Maintenance start time for this host is : [{maintenance_start_str}]; end time is: [{maintenance_end_str}] (end time empty means that there is not yet a set end time for the maintenance on the host)\\nGenerate a markdown report in the following format:\\n\\n## Alert Summary\\n(summary of what happened in the alert JSON data)\\n\\n## Collected Metrics\\n(lay out the maintenance information)\\n\\n## Analysis\\n(Describe the maintenance status of this host)\\n\\n## Recommended Actions\\n(Bullet point list: write how the user may not need to worry about this alert given that the host is under maintenance, and they could check if the issue persists afterward)\\n\\n## Alert Status\\n(can deprioritize the investigation of the alert, host under maintenance)', static_data_path='/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/maintenance_static_dataset.csv', skip_maintenance_check=False), 'categorizer': CategorizerToolConfig(description='This is a categorization tool used at the end of the pipeline.', llm_name='agent_llm', prompt='You will be given a system-generated alert triage report. Your job is to read the report carefully and determine the most likely root cause of the issue. Then, categorize the root cause into one of the following predefined categories:\\n\\n**Valid Categories**\\n- `software`: The alert was triggered due to a malfunctioning or inactive monitoring service (e.g., Telegraf not running).\\n- `network_connectivity`: The host is not reachable via ping or curl, or there are signs of connection issues due to blocked ports, broken services, or firewall rules (e.g., telnet fails).\\n- `hardware`: The alert is caused by a hardware failure or degradation.\\n- `repetitive_behavior`: The alert is triggered by a recurring or periodic behavior pattern (e.g., regular CPU spikes or memory surges).\\n- `false_positive`: No clear signs of failure or degradation; system appears healthy and no suspicious pattern is found.\\n- `need_investigation`: The report contains conflicting, ambiguous, or insufficient information to determine a clear root cause.\\n\\n**Response Format**\\n- Line 1: Output only the category name (e.g., `hardware`)\\n- Line 2: Briefly explain your reasoning based on the contents of the report.\\n- Example response:\\nnetwork_connectivity\\nPing and curl to the host both failed, and telnet to the monitored port timed out, indicating a likely connectivity or firewall issue.\\n\\n**Important Guidelines**\\n- Base your categorization only on evidence presented in the report.\\n- If no category clearly fits, default to `need_investigation`.')} function_groups={} llms={'agent_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=2048), 'tool_reasoning_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.2, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=2048), 'nim_rag_eval_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/alert_triage_model_selection_output/optimizer'), eval_metrics={'classification_accuracy': OptimizerMetric(evaluator_name='classification_accuracy', direction='maximize', weight=1.0), 'llm_latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=1, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=AlertTriageAgentWorkflowConfig(tool_names=['hardware_check', 'host_performance_check', 'monitoring_process_check', 'network_connectivity_check', 'telemetry_metrics_analysis_agent'], llm_name='agent_llm', offline_mode=True, offline_data_path='/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/offline_data.csv', benign_fallback_data_path='/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/benign_fallback_offline_data.json', agent_prompt='**Role**\\nYou are a Triage Agent responsible for diagnosing and troubleshooting system alerts in real time. Your goal is to determine whether an alert indicates a true issue, identify the root cause, and provide a clear, structured triage report to assist system analysts.\\n\\n\\n**Instructions**\\n\\n1. **Analyze the Alert**\\n   Begin by interpreting the incoming alert. Identify its type (e.g., *InstanceDown*, *HighCPUUsage*) and note any relevant details.\\n\\n2. **Select and Use Diagnostic Tools**\\n   Based on the alert type, choose the most relevant tools to gather system metrics. Use each tool only once per alert.\\n\\n   - `hardware_check`: Retrieves server power status and hardware health via IPMI. Useful for diagnosing instance down alerts or suspected hardware failures.\\n   - `host_performance_check`: Collects system-level CPU and memory usage using commands like `top` and `ps`. Use this to identify host\\'s resource (CPR and memory) usage bottlenecks.\\n   - `monitoring_process_check`: Checks whether critical processes are running on the host. Useful for verifying system functionality during instance down or degraded performance.\\n   - `network_connectivity_check`: Tests host connectivity through ping, telnet, and HTTP health checks. Helps determine if the server is reachable from the network.\\n   - `telemetry_metrics_analysis_agent`: Pulls telemetry metrics to check host status and analyze usage trends. Effective for validating instance uptime and system load over time.\\n\\n   Once you\\'ve received outputs from all selected tools, **pause to analyze them before proceeding further**.\\n\\n3. **Correlate Data and Determine Root Cause**\\n   - Evaluate the retrieved metrics against the alert details.\\n   - Determine if the alert reflects a real problem or is a false positive.\\n   - If an issue is detected, identify likely causes—such as hardware failure, performance bottlenecks, or network issues.\\n\\n4. **Generate a Structured Triage Report (in Markdown format)**\\n   Organize your findings clearly under these sections:\\n\\n   - **Alert Summary**: Brief description of the alert received.\\n   - **Collected Metrics**: Outputs from the diagnostic tools used.\\n   - **Analysis**: Interpretation of the data and how it relates to the alert.\\n   - **Recommended Actions**: Suggested next steps to mitigate or resolve the issue.\\n   - **Alert Status**: Choose one — \"Valid\", \"Abnormal but benign\", or \"False alarm\".\\n\\n\\n**Important Rules**\\n- Do not call the same tool more than once per alert.\\n- Analyze tool outputs before taking any additional action.\\n- Stay concise, structured, and actionable.') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/alert_triage_model_selection_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/offline_data.json'), profiler=None), evaluators={'accuracy': RagasEvaluatorConfig(llm_name='nim_rag_eval_llm', metric='AnswerAccuracy', input_obj_field=None), 'groundedness': RagasEvaluatorConfig(llm_name='nim_rag_eval_llm', metric='ResponseGroundedness', input_obj_field=None), 'relevance': RagasEvaluatorConfig(llm_name='nim_rag_eval_llm', metric='ContextRelevance', input_obj_field=None), 'classification_accuracy': ClassificationEvaluatorConfig(), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:45:01 - INFO     - nat_alert_triage_agent:104 - Preloaded test data from: /Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/offline_data.csv\n",
      "2025-10-25 15:45:01 - INFO     - nat_alert_triage_agent:108 - Preloaded benign fallback data from: /Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/benign_fallback_offline_data.json\n",
      "2025-10-25 15:45:01 - INFO     - nat_alert_triage_agent:80 - ================================================Running in offline mode=================================================\n",
      "Running workflow:   0%|                                   | 0/7 [00:00<?, ?it/s]2025-10-25 15:45:03 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-0.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:45:03 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-1.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:45:03 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-2.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:45:03 - INFO     - nat_alert_triage_agent:258 - Host: [test-instance-3.example.com] is under maintenance according to the maintenance database\n",
      "2025-10-25 15:45:07 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-4.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:45:07 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-5.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:45:07 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-6.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:45:07 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  14%|███▊                       | 1/7 [00:04<00:24,  4.12s/it]2025-10-25 15:45:21 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  29%|███████▋                   | 2/7 [00:18<00:49,  9.94s/it]2025-10-25 15:45:27 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  43%|███████████▌               | 3/7 [00:24<00:32,  8.23s/it]2025-10-25 15:45:34 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  57%|███████████████▍           | 4/7 [00:30<00:22,  7.47s/it]2025-10-25 15:45:40 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  71%|███████████████████▎       | 5/7 [00:36<00:14,  7.02s/it]2025-10-25 15:46:04 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  86%|███████████████████████▏   | 6/7 [01:00<00:12, 12.68s/it]2025-10-25 15:46:24 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow: 100%|███████████████████████████| 7/7 [01:20<00:00, 11.56s/it]\n",
      "Evaluating Ragas nv_accuracy:   0%|                       | 0/7 [00:00<?, ?it/s]\n",
      "Evaluating Ragas nv_response_groundedness:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Evaluating Ragas nv_context_relevance:   0%|              | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating classification accuracy:   0%|                 | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                         | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating classification accuracy: 100%|████████| 7/7 [00:00<00:00, 254.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg LLM Latency: 100%|████████████████| 7/7 [00:00<00:00, 255.33it/s]\n",
      "Evaluating Avg Tokens/LLM_END: 100%|█████████████| 7/7 [00:00<00:00, 255.88it/s]\n",
      "\n",
      "Evaluating Ragas nv_response_groundedness:  14%|▎ | 1/7 [00:00<00:02,  2.33it/s]\u001b[A2025-10-25 15:46:25 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:46:25 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  14%|██▏            | 1/7 [00:01<00:06,  1.07s/it]\n",
      "\n",
      "Evaluating Ragas nv_context_relevance:  14%|▊     | 1/7 [00:00<00:03,  1.90it/s]\u001b[A\u001b[A\n",
      "Evaluating Ragas nv_response_groundedness:  29%|▌ | 2/7 [00:00<00:01,  2.84it/s]\u001b[AAn error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_context_relevance:  29%|█▋    | 2/7 [00:00<00:01,  3.47it/s]\u001b[A\u001b[AAn error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:46:25 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:46:25 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  29%|████▎          | 2/7 [00:01<00:03,  1.58it/s]An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_context_relevance:  57%|███▍  | 4/7 [00:00<00:00,  5.56it/s]\u001b[A\u001b[A2025-10-25 15:46:25 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "Evaluating Ragas nv_response_groundedness:  57%|█▏| 4/7 [00:01<00:00,  3.67it/s]\u001b[A2025-10-25 15:46:25 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  57%|████████▌      | 4/7 [00:01<00:00,  3.61it/s]An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_context_relevance:  71%|████▎ | 5/7 [00:00<00:00,  6.31it/s]\u001b[A\u001b[AAn error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:46:25 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:46:26 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:46:26 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  86%|████████████▊  | 6/7 [00:01<00:00,  5.87it/s]2025-10-25 15:46:26 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "Evaluating Ragas nv_response_groundedness:  86%|█▋| 6/7 [00:01<00:00,  5.16it/s]\u001b[A\n",
      "\n",
      "Evaluating Ragas nv_context_relevance: 100%|██████| 7/7 [00:02<00:00,  2.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating Ragas nv_response_groundedness: 100%|██| 7/7 [00:03<00:00,  2.16it/s]\u001b[A\n",
      "Evaluating Ragas nv_accuracy: 100%|███████████████| 7/7 [00:03<00:00,  1.85it/s]\n",
      "2025-10-25 15:46:28 - INFO     - nat_alert_triage_agent:164 - Cleaning up\n",
      "2025-10-25 15:46:28 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:46:28 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/alert_triage_model_selection_output/workflow_output.json\n",
      "2025-10-25 15:46:28 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/classification_accuracy_output.json\n",
      "2025-10-25 15:46:28 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/llm_latency_output.json\n",
      "2025-10-25 15:46:28 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/token_efficiency_output.json\n",
      "2025-10-25 15:46:28 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/relevance_output.json\n",
      "2025-10-25 15:46:28 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/groundedness_output.json\n",
      "2025-10-25 15:46:28 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/accuracy_output.json\n",
      "\u001b[32m[I 2025-10-25 15:46:28,229]\u001b[0m Trial 0 finished with values: [0.29, 8.67] and parameters: {'llms.agent_llm.temperature': 0.0, 'llms.agent_llm.model_name': 'meta/llama-3.1-70b-instruct'}.\u001b[0m\n",
      "2025-10-25 15:46:28 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={'hardware_check': HardwareCheckToolConfig(description='This tool checks hardware health status using IPMI monitoring to detect power state, hardware degradation, and anomalies that could explain alerts. Args: host_id: str', llm_name='tool_reasoning_llm', prompt=\"You are analyzing IPMI metrics to support host monitoring and alert triage. Use the provided IPMI output to assess overall system status. Your goals are to:\\n\\n1. Determine the system's current power state.\\n2. Identify any signs of hardware degradation or failure.\\n3. Flag any anomalies that could explain why a monitoring alert was triggered.\\n\\nReview the data carefully and summarize your assessment in a clear and structured format.\\n\\nIPMI Output:\\n{input_data}\\n\\nFormat your response as follows:\\n\\nPower Status: ON / OFF\\nHardware Health: Normal / Issues Detected\\nObserved Anomalies: [List any irregularities or warning signs]\\nPossible Cause of Alert: [e.g., hardware issue, thermal spike, power fluctuation, no clear issue]\\nNext Steps: [Recommended actions or checks for further triage]\", offline_mode=True), 'host_performance_check': HostPerformanceCheckToolConfig(description='This tool retrieves CPU usage, memory usage, and hardware I/O usage details for a given host. Args: host_id: str', llm_name='tool_reasoning_llm', parsing_prompt='You are given system performance data captured from a host. Your task is to extract and organize the information into a clean, structured JSON format. The input contains system details and performance metrics, such as CPU, memory, and disk I/O.\\n\\nFollow these instructions:\\n\\n1. Identify metric categories dynamically based on the line prefixes or column headers (e.g., \"Mem:\", \"Swap:\", \"CPU:\", \"Device:\").\\n2. For each category, extract the numerical values and map them to meaningful field names.\\n3. Group related fields under sections such as \"memory_usage\", \"swap_usage\", \"cpu_usage\", \"disk_io\", etc.\\n4. Use consistent, readable key names for all fields.\\n5. Return **only** the final JSON object — no explanations or extra text.\\n\\nHere is the input data:\\n{input_data}', analysis_prompt='You are analyzing system metrics to assess CPU and memory usage. Use the output below to determine whether CPU or memory usage is abnormally high, identify which processes are consuming the most resources, and assess whether the usage patterns could explain a recent alert.\\n\\nInstructions:\\n1. Evaluate overall CPU and memory usage levels.\\n2. List the top resource-consuming processes, including their name, PID, %CPU, and %MEM.\\n3. Identify any potential causes of high usage (e.g., memory leak, runaway process, legitimate high load).\\n4. Recommend possible next steps for investigation or mitigation.\\n\\nFormat your response as a structured summary:\\n\\nCPU Usage: Normal / High (X% usage)\\nMemory Usage: Normal / High (X% usage)\\nTop Resource-Consuming Processes: [Process name, PID, %CPU, %MEM]\\nPotential Cause of High Usage: [e.g., runaway process, heavy load, memory leak]\\nNext Steps: [Suggested mitigation actions]\\n\\nSystem Metrics Output:\\n{input_data}\\n', offline_mode=True), 'monitoring_process_check': MonitoringProcessCheckToolConfig(description='This tool checks the status of critical monitoring processes and services on a target host by executing system commands. Args: host_id: str', llm_name='tool_reasoning_llm', prompt='You are checking whether the telegraf service is running on the server. Use the monitoring output below to verify its status. If it’s not running, identify possible reasons and assess the impact.\\n\\nInstructions:\\n1. Check if the telegraf process is present and active.\\n2. Evaluate the potential impact of telegraf not running on system availability or monitoring.\\n3. Identify likely causes for the process not running.\\n\\nFormat your response as a structured summary:\\n* **Telegraf Running:** Yes / No\\n* **Potential Impact:** [e.g., host seems down to the monitoring system, delayed alerting]\\n* **Possible Cause:** [e.g., process crash, misconfiguration, resource constraints]\\n* **Next Steps:** [e.g., restart telegraf, check logs]\\n\\nMonitoring Output:\\n{input_data}', offline_mode=True), 'network_connectivity_check': NetworkConnectivityCheckToolConfig(description='This tool checks network connectivity of a host by running ping and socket connection tests. Args: host_id: str', llm_name='tool_reasoning_llm', prompt='You are assisting with alert triage by checking the network connectivity status of a host. Use the outputs from `ping` and `telnet` commands to determine whether the host is reachable. If connectivity issues are detected, analyze the possible root causes and provide a structured summary of your findings.\\n\\nInstructions:\\n1. Interpret the `ping` and `telnet` results to assess host reachability.\\n2. Determine whether there is a connectivity issue.\\n3. Identify potential causes, such as network failure, firewall restrictions, or service unavailability.\\n4. Recommend appropriate next steps for troubleshooting or escalation.\\n\\nFormat your response as a structured summary:\\n\\nPing Status: Successful / Failed\\nTelnet Status: Connected / Failed\\nPotential Cause of Connectivity Issue: [e.g., network failure, firewall rules, service outage, no issue]\\nNext Steps: [e.g., check network logs, restart network services, escalate issue, or no action needed]\\n\\nPing Output:\\n{ping_data}\\n\\nTelnet Output:\\n{telnet_data}', offline_mode=True), 'telemetry_metrics_host_heartbeat_check': TelemetryMetricsHostHeartbeatCheckToolConfig(description=\"This tool checks if a host's telemetry monitoring service is reporting heartbeat metrics. This tells us if the host is up and running. Args: host_id: str\", llm_name='tool_reasoning_llm', prompt=\"The following is the telemetry metrics fetched for the host to see if it's been up and running (if result is empty, then the monitoring service on the host is down):\\n{data}\\nBased on the data, summarize the fetched data and provide a conclusion of the host's running status.\", offline_mode=True, metrics_url=''), 'telemetry_metrics_host_performance_check': TelemetryMetricsHostPerformanceCheckToolConfig(description='This tool checks the performance of the host by analyzing the CPU usage timeseries. Args: host_id: str', llm_name='tool_reasoning_llm', prompt=\"You are an expert on analyzing CPU usage timeseries. Periodic usage peaks are expected benign system behavior.\\nUser will provide data in the format of a list of lists, where each sublist contains two elements: timestamp and CPU usage percentage. User will also provide statistics on the timeseries. Write a markdown report about what was observed in the timeseries.\\n\\nExample format:\\n# CPU Usage Analysis Report\\nThe data analysis is performed on 14 days of CPU usage percentage data.\\n\\n## Data Statistics\\ndata start and end time, data point interval, CPU usage statistics\\n\\n## Observations\\nany patterns observed? Should be one of the below cases:\\n- Are there any cyclic usage surges?\\n  - What is the cycle?\\n  - What is the high and low CPU usage of the pattern?\\n- Is there one anomalous peak?\\n  - When did it happen?\\n  - What is it like before and after?\\n- No obvious pattern? A mix of patterns? => it's normal flutuation of the system (max usage less than 60%)\\n  - What is the fluctuation range?\\n\\n## Conclusion\\nSummarize the observation.\\nCategories:\\n- peak in the data means the high CPU usage is an anomaly and requires attention\\n- periodic behvior means the high usage is benign\\n- overall moderate (max usage less than 60%) usage means no issue in the system\\n\\n## Pattern Label\\nAnomalous Peak/Periodic Surges/Normal Fluctuations\\n\", offline_mode=True, metrics_url=''), 'telemetry_metrics_analysis_agent': TelemetryMetricsAnalysisAgentConfig(description='This is a telemetry metrics tool used to monitor remotely collected telemetry data. It checks server heartbeat data to determine whether the server is up and running and analyzes CPU usage patterns over the past 14 days to identify potential CPU issues. Args: host_id: str, alert_type: str', tool_names=['telemetry_metrics_host_heartbeat_check', 'telemetry_metrics_host_performance_check'], llm_name='agent_llm', prompt=\"You arg a helpful alert triage assistant. Your task is to investigate an alert that was just triggered on a specific host. You will be given two inputs:\\n- `host_id`: the identifier of the host where the alert occurred.\\n- `alert_type`: the type of alert that triggered.\\n\\nUse the tools provided below to collect relevant telemetry data for the specified host:\\n\\nTools:\\n- `telemetry_metrics_host_heartbeat_check`: Use this to check the server's heartbeat and determine if the host is currently up and responsive.\\n- `telemetry_metrics_host_performance_check`: Use this to analyze CPU usage trends over the past 14 days and identify abnormal patterns.\\n\\nInstructions:\\n1. Run the appropriate tools based on the host and alert type.\\n2. Collect and include all relevant output from the tools in your response.\\n3. Analyze the data and provide reasoning to help determine whether the telemetry supports or explains the triggered alert.\\n\\nYour response should include:\\n- Raw data from each tool\\n- A concise summary of findings\\n- Any insights or hypotheses that explain the alert\"), 'maintenance_check': MaintenanceCheckToolConfig(description='Check if a host is under maintenance during the time of an alert to help determine if the alert can be deprioritized.', llm_name='agent_llm', prompt='User will provide you with a system alert represented in JSON format. You know for a fact that there is maintenance happening for the host. Maintenance start time for this host is : [{maintenance_start_str}]; end time is: [{maintenance_end_str}] (end time empty means that there is not yet a set end time for the maintenance on the host)\\nGenerate a markdown report in the following format:\\n\\n## Alert Summary\\n(summary of what happened in the alert JSON data)\\n\\n## Collected Metrics\\n(lay out the maintenance information)\\n\\n## Analysis\\n(Describe the maintenance status of this host)\\n\\n## Recommended Actions\\n(Bullet point list: write how the user may not need to worry about this alert given that the host is under maintenance, and they could check if the issue persists afterward)\\n\\n## Alert Status\\n(can deprioritize the investigation of the alert, host under maintenance)', static_data_path='/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/maintenance_static_dataset.csv', skip_maintenance_check=False), 'categorizer': CategorizerToolConfig(description='This is a categorization tool used at the end of the pipeline.', llm_name='agent_llm', prompt='You will be given a system-generated alert triage report. Your job is to read the report carefully and determine the most likely root cause of the issue. Then, categorize the root cause into one of the following predefined categories:\\n\\n**Valid Categories**\\n- `software`: The alert was triggered due to a malfunctioning or inactive monitoring service (e.g., Telegraf not running).\\n- `network_connectivity`: The host is not reachable via ping or curl, or there are signs of connection issues due to blocked ports, broken services, or firewall rules (e.g., telnet fails).\\n- `hardware`: The alert is caused by a hardware failure or degradation.\\n- `repetitive_behavior`: The alert is triggered by a recurring or periodic behavior pattern (e.g., regular CPU spikes or memory surges).\\n- `false_positive`: No clear signs of failure or degradation; system appears healthy and no suspicious pattern is found.\\n- `need_investigation`: The report contains conflicting, ambiguous, or insufficient information to determine a clear root cause.\\n\\n**Response Format**\\n- Line 1: Output only the category name (e.g., `hardware`)\\n- Line 2: Briefly explain your reasoning based on the contents of the report.\\n- Example response:\\nnetwork_connectivity\\nPing and curl to the host both failed, and telnet to the monitored port timed out, indicating a likely connectivity or firewall issue.\\n\\n**Important Guidelines**\\n- Base your categorization only on evidence presented in the report.\\n- If no category clearly fits, default to `need_investigation`.')} function_groups={} llms={'agent_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.5, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=2048), 'tool_reasoning_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.2, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=2048), 'nim_rag_eval_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/alert_triage_model_selection_output/optimizer'), eval_metrics={'classification_accuracy': OptimizerMetric(evaluator_name='classification_accuracy', direction='maximize', weight=1.0), 'llm_latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=1, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=AlertTriageAgentWorkflowConfig(tool_names=['hardware_check', 'host_performance_check', 'monitoring_process_check', 'network_connectivity_check', 'telemetry_metrics_analysis_agent'], llm_name='agent_llm', offline_mode=True, offline_data_path='/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/offline_data.csv', benign_fallback_data_path='/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/benign_fallback_offline_data.json', agent_prompt='**Role**\\nYou are a Triage Agent responsible for diagnosing and troubleshooting system alerts in real time. Your goal is to determine whether an alert indicates a true issue, identify the root cause, and provide a clear, structured triage report to assist system analysts.\\n\\n\\n**Instructions**\\n\\n1. **Analyze the Alert**\\n   Begin by interpreting the incoming alert. Identify its type (e.g., *InstanceDown*, *HighCPUUsage*) and note any relevant details.\\n\\n2. **Select and Use Diagnostic Tools**\\n   Based on the alert type, choose the most relevant tools to gather system metrics. Use each tool only once per alert.\\n\\n   - `hardware_check`: Retrieves server power status and hardware health via IPMI. Useful for diagnosing instance down alerts or suspected hardware failures.\\n   - `host_performance_check`: Collects system-level CPU and memory usage using commands like `top` and `ps`. Use this to identify host\\'s resource (CPR and memory) usage bottlenecks.\\n   - `monitoring_process_check`: Checks whether critical processes are running on the host. Useful for verifying system functionality during instance down or degraded performance.\\n   - `network_connectivity_check`: Tests host connectivity through ping, telnet, and HTTP health checks. Helps determine if the server is reachable from the network.\\n   - `telemetry_metrics_analysis_agent`: Pulls telemetry metrics to check host status and analyze usage trends. Effective for validating instance uptime and system load over time.\\n\\n   Once you\\'ve received outputs from all selected tools, **pause to analyze them before proceeding further**.\\n\\n3. **Correlate Data and Determine Root Cause**\\n   - Evaluate the retrieved metrics against the alert details.\\n   - Determine if the alert reflects a real problem or is a false positive.\\n   - If an issue is detected, identify likely causes—such as hardware failure, performance bottlenecks, or network issues.\\n\\n4. **Generate a Structured Triage Report (in Markdown format)**\\n   Organize your findings clearly under these sections:\\n\\n   - **Alert Summary**: Brief description of the alert received.\\n   - **Collected Metrics**: Outputs from the diagnostic tools used.\\n   - **Analysis**: Interpretation of the data and how it relates to the alert.\\n   - **Recommended Actions**: Suggested next steps to mitigate or resolve the issue.\\n   - **Alert Status**: Choose one — \"Valid\", \"Abnormal but benign\", or \"False alarm\".\\n\\n\\n**Important Rules**\\n- Do not call the same tool more than once per alert.\\n- Analyze tool outputs before taking any additional action.\\n- Stay concise, structured, and actionable.') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/alert_triage_model_selection_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/offline_data.json'), profiler=None), evaluators={'accuracy': RagasEvaluatorConfig(llm_name='nim_rag_eval_llm', metric='AnswerAccuracy', input_obj_field=None), 'groundedness': RagasEvaluatorConfig(llm_name='nim_rag_eval_llm', metric='ResponseGroundedness', input_obj_field=None), 'relevance': RagasEvaluatorConfig(llm_name='nim_rag_eval_llm', metric='ContextRelevance', input_obj_field=None), 'classification_accuracy': ClassificationEvaluatorConfig(), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:46:28 - INFO     - nat_alert_triage_agent:104 - Preloaded test data from: /Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/offline_data.csv\n",
      "2025-10-25 15:46:28 - INFO     - nat_alert_triage_agent:108 - Preloaded benign fallback data from: /Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/benign_fallback_offline_data.json\n",
      "2025-10-25 15:46:28 - INFO     - nat_alert_triage_agent:80 - ================================================Running in offline mode=================================================\n",
      "Running workflow:   0%|                                   | 0/7 [00:00<?, ?it/s]2025-10-25 15:46:28 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-0.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:46:28 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-1.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:46:28 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-2.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:46:28 - INFO     - nat_alert_triage_agent:258 - Host: [test-instance-3.example.com] is under maintenance according to the maintenance database\n",
      "2025-10-25 15:46:54 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-4.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:46:54 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-5.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:46:54 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-6.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:46:54 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  14%|███▊                       | 1/7 [00:26<02:38, 26.34s/it]2025-10-25 15:47:05 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  29%|███████▋                   | 2/7 [00:37<01:27, 17.43s/it]2025-10-25 15:47:17 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  43%|███████████▌               | 3/7 [00:48<00:58, 14.63s/it]2025-10-25 15:47:21 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  57%|███████████████▍           | 4/7 [00:52<00:31, 10.41s/it]2025-10-25 15:47:43 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  71%|███████████████████▎       | 5/7 [01:15<00:29, 14.85s/it]2025-10-25 15:47:52 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  86%|███████████████████████▏   | 6/7 [01:23<00:12, 12.63s/it]2025-10-25 15:48:24 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow: 100%|███████████████████████████| 7/7 [01:56<00:00, 16.60s/it]\n",
      "Evaluating Ragas nv_accuracy:   0%|                       | 0/7 [00:00<?, ?it/s]\n",
      "Evaluating Ragas nv_response_groundedness:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Evaluating Ragas nv_context_relevance:   0%|              | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating classification accuracy:   0%|                 | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                         | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating classification accuracy: 100%|████████| 7/7 [00:00<00:00, 328.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg LLM Latency: 100%|████████████████| 7/7 [00:00<00:00, 330.90it/s]\n",
      "Evaluating Avg Tokens/LLM_END: 100%|█████████████| 7/7 [00:00<00:00, 332.51it/s]\n",
      "\n",
      "Evaluating Ragas nv_response_groundedness:  14%|▎ | 1/7 [00:00<00:02,  2.38it/s]\u001b[A2025-10-25 15:48:25 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:48:25 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_context_relevance:  14%|▊     | 1/7 [00:00<00:02,  2.54it/s]\u001b[A\u001b[AAn error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "Evaluating Ragas nv_response_groundedness:  29%|▌ | 2/7 [00:00<00:01,  3.60it/s]\u001b[AAn error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_context_relevance:  43%|██▌   | 3/7 [00:00<00:00,  6.72it/s]\u001b[A\u001b[AAn error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:48:25 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:48:25 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:48:25 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  14%|██▏            | 1/7 [00:01<00:07,  1.18s/it]2025-10-25 15:48:25 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:48:25 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  43%|██████▍        | 3/7 [00:01<00:01,  2.67it/s]2025-10-25 15:48:25 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_context_relevance:  71%|████▎ | 5/7 [00:00<00:00,  5.19it/s]\u001b[A\u001b[A2025-10-25 15:48:25 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "Evaluating Ragas nv_response_groundedness:  43%|▊ | 3/7 [00:01<00:01,  2.28it/s]\u001b[AAn error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:48:26 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  71%|██████████▋    | 5/7 [00:01<00:00,  4.44it/s]2025-10-25 15:48:26 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "Evaluating Ragas nv_accuracy: 100%|███████████████| 7/7 [00:02<00:00,  3.69it/s]\u001b[A\n",
      "\n",
      "Evaluating Ragas nv_context_relevance: 100%|██████| 7/7 [00:03<00:00,  2.12it/s]\u001b[A\u001b[A\n",
      "Evaluating Ragas nv_response_groundedness: 100%|██| 7/7 [00:03<00:00,  1.89it/s]\n",
      "Evaluating Ragas nv_accuracy: 100%|███████████████| 7/7 [00:04<00:00,  1.71it/s]\n",
      "2025-10-25 15:48:28 - INFO     - nat_alert_triage_agent:164 - Cleaning up\n",
      "2025-10-25 15:48:28 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:48:28 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/alert_triage_model_selection_output/workflow_output.json\n",
      "2025-10-25 15:48:28 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/classification_accuracy_output.json\n",
      "2025-10-25 15:48:28 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/llm_latency_output.json\n",
      "2025-10-25 15:48:28 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/token_efficiency_output.json\n",
      "2025-10-25 15:48:28 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/relevance_output.json\n",
      "2025-10-25 15:48:28 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/groundedness_output.json\n",
      "2025-10-25 15:48:28 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/accuracy_output.json\n",
      "\u001b[32m[I 2025-10-25 15:48:28,625]\u001b[0m Trial 1 finished with values: [0.14, 12.92] and parameters: {'llms.agent_llm.temperature': 0.5, 'llms.agent_llm.model_name': 'meta/llama-3.1-70b-instruct'}.\u001b[0m\n",
      "2025-10-25 15:48:28 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={'hardware_check': HardwareCheckToolConfig(description='This tool checks hardware health status using IPMI monitoring to detect power state, hardware degradation, and anomalies that could explain alerts. Args: host_id: str', llm_name='tool_reasoning_llm', prompt=\"You are analyzing IPMI metrics to support host monitoring and alert triage. Use the provided IPMI output to assess overall system status. Your goals are to:\\n\\n1. Determine the system's current power state.\\n2. Identify any signs of hardware degradation or failure.\\n3. Flag any anomalies that could explain why a monitoring alert was triggered.\\n\\nReview the data carefully and summarize your assessment in a clear and structured format.\\n\\nIPMI Output:\\n{input_data}\\n\\nFormat your response as follows:\\n\\nPower Status: ON / OFF\\nHardware Health: Normal / Issues Detected\\nObserved Anomalies: [List any irregularities or warning signs]\\nPossible Cause of Alert: [e.g., hardware issue, thermal spike, power fluctuation, no clear issue]\\nNext Steps: [Recommended actions or checks for further triage]\", offline_mode=True), 'host_performance_check': HostPerformanceCheckToolConfig(description='This tool retrieves CPU usage, memory usage, and hardware I/O usage details for a given host. Args: host_id: str', llm_name='tool_reasoning_llm', parsing_prompt='You are given system performance data captured from a host. Your task is to extract and organize the information into a clean, structured JSON format. The input contains system details and performance metrics, such as CPU, memory, and disk I/O.\\n\\nFollow these instructions:\\n\\n1. Identify metric categories dynamically based on the line prefixes or column headers (e.g., \"Mem:\", \"Swap:\", \"CPU:\", \"Device:\").\\n2. For each category, extract the numerical values and map them to meaningful field names.\\n3. Group related fields under sections such as \"memory_usage\", \"swap_usage\", \"cpu_usage\", \"disk_io\", etc.\\n4. Use consistent, readable key names for all fields.\\n5. Return **only** the final JSON object — no explanations or extra text.\\n\\nHere is the input data:\\n{input_data}', analysis_prompt='You are analyzing system metrics to assess CPU and memory usage. Use the output below to determine whether CPU or memory usage is abnormally high, identify which processes are consuming the most resources, and assess whether the usage patterns could explain a recent alert.\\n\\nInstructions:\\n1. Evaluate overall CPU and memory usage levels.\\n2. List the top resource-consuming processes, including their name, PID, %CPU, and %MEM.\\n3. Identify any potential causes of high usage (e.g., memory leak, runaway process, legitimate high load).\\n4. Recommend possible next steps for investigation or mitigation.\\n\\nFormat your response as a structured summary:\\n\\nCPU Usage: Normal / High (X% usage)\\nMemory Usage: Normal / High (X% usage)\\nTop Resource-Consuming Processes: [Process name, PID, %CPU, %MEM]\\nPotential Cause of High Usage: [e.g., runaway process, heavy load, memory leak]\\nNext Steps: [Suggested mitigation actions]\\n\\nSystem Metrics Output:\\n{input_data}\\n', offline_mode=True), 'monitoring_process_check': MonitoringProcessCheckToolConfig(description='This tool checks the status of critical monitoring processes and services on a target host by executing system commands. Args: host_id: str', llm_name='tool_reasoning_llm', prompt='You are checking whether the telegraf service is running on the server. Use the monitoring output below to verify its status. If it’s not running, identify possible reasons and assess the impact.\\n\\nInstructions:\\n1. Check if the telegraf process is present and active.\\n2. Evaluate the potential impact of telegraf not running on system availability or monitoring.\\n3. Identify likely causes for the process not running.\\n\\nFormat your response as a structured summary:\\n* **Telegraf Running:** Yes / No\\n* **Potential Impact:** [e.g., host seems down to the monitoring system, delayed alerting]\\n* **Possible Cause:** [e.g., process crash, misconfiguration, resource constraints]\\n* **Next Steps:** [e.g., restart telegraf, check logs]\\n\\nMonitoring Output:\\n{input_data}', offline_mode=True), 'network_connectivity_check': NetworkConnectivityCheckToolConfig(description='This tool checks network connectivity of a host by running ping and socket connection tests. Args: host_id: str', llm_name='tool_reasoning_llm', prompt='You are assisting with alert triage by checking the network connectivity status of a host. Use the outputs from `ping` and `telnet` commands to determine whether the host is reachable. If connectivity issues are detected, analyze the possible root causes and provide a structured summary of your findings.\\n\\nInstructions:\\n1. Interpret the `ping` and `telnet` results to assess host reachability.\\n2. Determine whether there is a connectivity issue.\\n3. Identify potential causes, such as network failure, firewall restrictions, or service unavailability.\\n4. Recommend appropriate next steps for troubleshooting or escalation.\\n\\nFormat your response as a structured summary:\\n\\nPing Status: Successful / Failed\\nTelnet Status: Connected / Failed\\nPotential Cause of Connectivity Issue: [e.g., network failure, firewall rules, service outage, no issue]\\nNext Steps: [e.g., check network logs, restart network services, escalate issue, or no action needed]\\n\\nPing Output:\\n{ping_data}\\n\\nTelnet Output:\\n{telnet_data}', offline_mode=True), 'telemetry_metrics_host_heartbeat_check': TelemetryMetricsHostHeartbeatCheckToolConfig(description=\"This tool checks if a host's telemetry monitoring service is reporting heartbeat metrics. This tells us if the host is up and running. Args: host_id: str\", llm_name='tool_reasoning_llm', prompt=\"The following is the telemetry metrics fetched for the host to see if it's been up and running (if result is empty, then the monitoring service on the host is down):\\n{data}\\nBased on the data, summarize the fetched data and provide a conclusion of the host's running status.\", offline_mode=True, metrics_url=''), 'telemetry_metrics_host_performance_check': TelemetryMetricsHostPerformanceCheckToolConfig(description='This tool checks the performance of the host by analyzing the CPU usage timeseries. Args: host_id: str', llm_name='tool_reasoning_llm', prompt=\"You are an expert on analyzing CPU usage timeseries. Periodic usage peaks are expected benign system behavior.\\nUser will provide data in the format of a list of lists, where each sublist contains two elements: timestamp and CPU usage percentage. User will also provide statistics on the timeseries. Write a markdown report about what was observed in the timeseries.\\n\\nExample format:\\n# CPU Usage Analysis Report\\nThe data analysis is performed on 14 days of CPU usage percentage data.\\n\\n## Data Statistics\\ndata start and end time, data point interval, CPU usage statistics\\n\\n## Observations\\nany patterns observed? Should be one of the below cases:\\n- Are there any cyclic usage surges?\\n  - What is the cycle?\\n  - What is the high and low CPU usage of the pattern?\\n- Is there one anomalous peak?\\n  - When did it happen?\\n  - What is it like before and after?\\n- No obvious pattern? A mix of patterns? => it's normal flutuation of the system (max usage less than 60%)\\n  - What is the fluctuation range?\\n\\n## Conclusion\\nSummarize the observation.\\nCategories:\\n- peak in the data means the high CPU usage is an anomaly and requires attention\\n- periodic behvior means the high usage is benign\\n- overall moderate (max usage less than 60%) usage means no issue in the system\\n\\n## Pattern Label\\nAnomalous Peak/Periodic Surges/Normal Fluctuations\\n\", offline_mode=True, metrics_url=''), 'telemetry_metrics_analysis_agent': TelemetryMetricsAnalysisAgentConfig(description='This is a telemetry metrics tool used to monitor remotely collected telemetry data. It checks server heartbeat data to determine whether the server is up and running and analyzes CPU usage patterns over the past 14 days to identify potential CPU issues. Args: host_id: str, alert_type: str', tool_names=['telemetry_metrics_host_heartbeat_check', 'telemetry_metrics_host_performance_check'], llm_name='agent_llm', prompt=\"You arg a helpful alert triage assistant. Your task is to investigate an alert that was just triggered on a specific host. You will be given two inputs:\\n- `host_id`: the identifier of the host where the alert occurred.\\n- `alert_type`: the type of alert that triggered.\\n\\nUse the tools provided below to collect relevant telemetry data for the specified host:\\n\\nTools:\\n- `telemetry_metrics_host_heartbeat_check`: Use this to check the server's heartbeat and determine if the host is currently up and responsive.\\n- `telemetry_metrics_host_performance_check`: Use this to analyze CPU usage trends over the past 14 days and identify abnormal patterns.\\n\\nInstructions:\\n1. Run the appropriate tools based on the host and alert type.\\n2. Collect and include all relevant output from the tools in your response.\\n3. Analyze the data and provide reasoning to help determine whether the telemetry supports or explains the triggered alert.\\n\\nYour response should include:\\n- Raw data from each tool\\n- A concise summary of findings\\n- Any insights or hypotheses that explain the alert\"), 'maintenance_check': MaintenanceCheckToolConfig(description='Check if a host is under maintenance during the time of an alert to help determine if the alert can be deprioritized.', llm_name='agent_llm', prompt='User will provide you with a system alert represented in JSON format. You know for a fact that there is maintenance happening for the host. Maintenance start time for this host is : [{maintenance_start_str}]; end time is: [{maintenance_end_str}] (end time empty means that there is not yet a set end time for the maintenance on the host)\\nGenerate a markdown report in the following format:\\n\\n## Alert Summary\\n(summary of what happened in the alert JSON data)\\n\\n## Collected Metrics\\n(lay out the maintenance information)\\n\\n## Analysis\\n(Describe the maintenance status of this host)\\n\\n## Recommended Actions\\n(Bullet point list: write how the user may not need to worry about this alert given that the host is under maintenance, and they could check if the issue persists afterward)\\n\\n## Alert Status\\n(can deprioritize the investigation of the alert, host under maintenance)', static_data_path='/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/maintenance_static_dataset.csv', skip_maintenance_check=False), 'categorizer': CategorizerToolConfig(description='This is a categorization tool used at the end of the pipeline.', llm_name='agent_llm', prompt='You will be given a system-generated alert triage report. Your job is to read the report carefully and determine the most likely root cause of the issue. Then, categorize the root cause into one of the following predefined categories:\\n\\n**Valid Categories**\\n- `software`: The alert was triggered due to a malfunctioning or inactive monitoring service (e.g., Telegraf not running).\\n- `network_connectivity`: The host is not reachable via ping or curl, or there are signs of connection issues due to blocked ports, broken services, or firewall rules (e.g., telnet fails).\\n- `hardware`: The alert is caused by a hardware failure or degradation.\\n- `repetitive_behavior`: The alert is triggered by a recurring or periodic behavior pattern (e.g., regular CPU spikes or memory surges).\\n- `false_positive`: No clear signs of failure or degradation; system appears healthy and no suspicious pattern is found.\\n- `need_investigation`: The report contains conflicting, ambiguous, or insufficient information to determine a clear root cause.\\n\\n**Response Format**\\n- Line 1: Output only the category name (e.g., `hardware`)\\n- Line 2: Briefly explain your reasoning based on the contents of the report.\\n- Example response:\\nnetwork_connectivity\\nPing and curl to the host both failed, and telnet to the monitored port timed out, indicating a likely connectivity or firewall issue.\\n\\n**Important Guidelines**\\n- Base your categorization only on evidence presented in the report.\\n- If no category clearly fits, default to `need_investigation`.')} function_groups={} llms={'agent_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.5, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=2048), 'tool_reasoning_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.2, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=2048), 'nim_rag_eval_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/alert_triage_model_selection_output/optimizer'), eval_metrics={'classification_accuracy': OptimizerMetric(evaluator_name='classification_accuracy', direction='maximize', weight=1.0), 'llm_latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=1, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=AlertTriageAgentWorkflowConfig(tool_names=['hardware_check', 'host_performance_check', 'monitoring_process_check', 'network_connectivity_check', 'telemetry_metrics_analysis_agent'], llm_name='agent_llm', offline_mode=True, offline_data_path='/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/offline_data.csv', benign_fallback_data_path='/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/benign_fallback_offline_data.json', agent_prompt='**Role**\\nYou are a Triage Agent responsible for diagnosing and troubleshooting system alerts in real time. Your goal is to determine whether an alert indicates a true issue, identify the root cause, and provide a clear, structured triage report to assist system analysts.\\n\\n\\n**Instructions**\\n\\n1. **Analyze the Alert**\\n   Begin by interpreting the incoming alert. Identify its type (e.g., *InstanceDown*, *HighCPUUsage*) and note any relevant details.\\n\\n2. **Select and Use Diagnostic Tools**\\n   Based on the alert type, choose the most relevant tools to gather system metrics. Use each tool only once per alert.\\n\\n   - `hardware_check`: Retrieves server power status and hardware health via IPMI. Useful for diagnosing instance down alerts or suspected hardware failures.\\n   - `host_performance_check`: Collects system-level CPU and memory usage using commands like `top` and `ps`. Use this to identify host\\'s resource (CPR and memory) usage bottlenecks.\\n   - `monitoring_process_check`: Checks whether critical processes are running on the host. Useful for verifying system functionality during instance down or degraded performance.\\n   - `network_connectivity_check`: Tests host connectivity through ping, telnet, and HTTP health checks. Helps determine if the server is reachable from the network.\\n   - `telemetry_metrics_analysis_agent`: Pulls telemetry metrics to check host status and analyze usage trends. Effective for validating instance uptime and system load over time.\\n\\n   Once you\\'ve received outputs from all selected tools, **pause to analyze them before proceeding further**.\\n\\n3. **Correlate Data and Determine Root Cause**\\n   - Evaluate the retrieved metrics against the alert details.\\n   - Determine if the alert reflects a real problem or is a false positive.\\n   - If an issue is detected, identify likely causes—such as hardware failure, performance bottlenecks, or network issues.\\n\\n4. **Generate a Structured Triage Report (in Markdown format)**\\n   Organize your findings clearly under these sections:\\n\\n   - **Alert Summary**: Brief description of the alert received.\\n   - **Collected Metrics**: Outputs from the diagnostic tools used.\\n   - **Analysis**: Interpretation of the data and how it relates to the alert.\\n   - **Recommended Actions**: Suggested next steps to mitigate or resolve the issue.\\n   - **Alert Status**: Choose one — \"Valid\", \"Abnormal but benign\", or \"False alarm\".\\n\\n\\n**Important Rules**\\n- Do not call the same tool more than once per alert.\\n- Analyze tool outputs before taking any additional action.\\n- Stay concise, structured, and actionable.') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/alert_triage_model_selection_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/offline_data.json'), profiler=None), evaluators={'accuracy': RagasEvaluatorConfig(llm_name='nim_rag_eval_llm', metric='AnswerAccuracy', input_obj_field=None), 'groundedness': RagasEvaluatorConfig(llm_name='nim_rag_eval_llm', metric='ResponseGroundedness', input_obj_field=None), 'relevance': RagasEvaluatorConfig(llm_name='nim_rag_eval_llm', metric='ContextRelevance', input_obj_field=None), 'classification_accuracy': ClassificationEvaluatorConfig(), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:48:28 - INFO     - nat_alert_triage_agent:104 - Preloaded test data from: /Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/offline_data.csv\n",
      "2025-10-25 15:48:28 - INFO     - nat_alert_triage_agent:108 - Preloaded benign fallback data from: /Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/benign_fallback_offline_data.json\n",
      "2025-10-25 15:48:28 - INFO     - nat_alert_triage_agent:80 - ================================================Running in offline mode=================================================\n",
      "Running workflow:   0%|                                   | 0/7 [00:00<?, ?it/s]2025-10-25 15:48:28 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-0.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:48:28 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-1.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:48:28 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-2.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:48:28 - INFO     - nat_alert_triage_agent:258 - Host: [test-instance-3.example.com] is under maintenance according to the maintenance database\n",
      "2025-10-25 15:48:32 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-4.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:48:32 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-5.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:48:32 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-6.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:48:32 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  14%|███▊                       | 1/7 [00:03<00:20,  3.42s/it]2025-10-25 15:48:32 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:49:33 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  29%|███████▋                   | 2/7 [01:04<03:07, 37.50s/it]2025-10-25 15:50:00 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  43%|███████████▌               | 3/7 [01:31<02:11, 32.80s/it]2025-10-25 15:50:09 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  57%|███████████████▍           | 4/7 [01:40<01:09, 23.25s/it]2025-10-25 15:50:10 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  71%|███████████████████▎       | 5/7 [01:41<00:30, 15.20s/it]2025-10-25 15:50:18 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  86%|███████████████████████▏   | 6/7 [01:49<00:12, 12.74s/it]2025-10-25 15:51:02 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow: 100%|███████████████████████████| 7/7 [02:33<00:00, 21.92s/it]\n",
      "Evaluating Ragas nv_accuracy:   0%|                       | 0/7 [00:00<?, ?it/s]\n",
      "Evaluating Ragas nv_response_groundedness:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Evaluating Ragas nv_context_relevance:   0%|              | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating classification accuracy:   0%|                 | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                         | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating classification accuracy: 100%|████████| 7/7 [00:00<00:00, 262.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg LLM Latency: 100%|████████████████| 7/7 [00:00<00:00, 263.72it/s]\n",
      "Evaluating Avg Tokens/LLM_END: 100%|█████████████| 7/7 [00:00<00:00, 259.75it/s]\n",
      "\n",
      "Evaluating Ragas nv_response_groundedness:  14%|▎ | 1/7 [00:00<00:02,  2.35it/s]\u001b[A2025-10-25 15:51:02 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_context_relevance:  14%|▊     | 1/7 [00:00<00:02,  2.49it/s]\u001b[A\u001b[AAn error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:51:02 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  14%|██▏            | 1/7 [00:00<00:04,  1.25it/s]An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_context_relevance:  43%|██▌   | 3/7 [00:00<00:00,  6.67it/s]\u001b[A\u001b[AAn error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:51:03 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  29%|████▎          | 2/7 [00:01<00:03,  1.49it/s]2025-10-25 15:51:03 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_context_relevance:  71%|████▎ | 5/7 [00:01<00:00,  5.12it/s]\u001b[A\u001b[A2025-10-25 15:51:03 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "Evaluating Ragas nv_response_groundedness:  29%|▌ | 2/7 [00:01<00:03,  1.56it/s]\u001b[AAn error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:51:03 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:51:03 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:51:03 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:51:03 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_context_relevance:  86%|█████▏| 6/7 [00:01<00:00,  4.18it/s]\u001b[A\u001b[A\n",
      "Evaluating Ragas nv_accuracy:  86%|████████████▊  | 6/7 [00:02<00:00,  2.85it/s]\u001b[A\n",
      "\n",
      "Evaluating Ragas nv_context_relevance: 100%|██████| 7/7 [00:02<00:00,  2.43it/s]\u001b[A\u001b[A\n",
      "Evaluating Ragas nv_response_groundedness: 100%|██| 7/7 [00:03<00:00,  2.14it/s]\n",
      "Evaluating Ragas nv_accuracy: 100%|███████████████| 7/7 [00:03<00:00,  1.81it/s]\n",
      "2025-10-25 15:51:05 - INFO     - nat_alert_triage_agent:164 - Cleaning up\n",
      "2025-10-25 15:51:05 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:51:06 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/alert_triage_model_selection_output/workflow_output.json\n",
      "2025-10-25 15:51:06 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/classification_accuracy_output.json\n",
      "2025-10-25 15:51:06 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/llm_latency_output.json\n",
      "2025-10-25 15:51:06 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/token_efficiency_output.json\n",
      "2025-10-25 15:51:06 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/relevance_output.json\n",
      "2025-10-25 15:51:06 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/groundedness_output.json\n",
      "2025-10-25 15:51:06 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/accuracy_output.json\n",
      "\u001b[32m[I 2025-10-25 15:51:06,034]\u001b[0m Trial 2 finished with values: [0.43, 7.88] and parameters: {'llms.agent_llm.temperature': 0.5, 'llms.agent_llm.model_name': 'meta/llama-3.1-8b-instruct'}.\u001b[0m\n",
      "2025-10-25 15:51:06 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={'hardware_check': HardwareCheckToolConfig(description='This tool checks hardware health status using IPMI monitoring to detect power state, hardware degradation, and anomalies that could explain alerts. Args: host_id: str', llm_name='tool_reasoning_llm', prompt=\"You are analyzing IPMI metrics to support host monitoring and alert triage. Use the provided IPMI output to assess overall system status. Your goals are to:\\n\\n1. Determine the system's current power state.\\n2. Identify any signs of hardware degradation or failure.\\n3. Flag any anomalies that could explain why a monitoring alert was triggered.\\n\\nReview the data carefully and summarize your assessment in a clear and structured format.\\n\\nIPMI Output:\\n{input_data}\\n\\nFormat your response as follows:\\n\\nPower Status: ON / OFF\\nHardware Health: Normal / Issues Detected\\nObserved Anomalies: [List any irregularities or warning signs]\\nPossible Cause of Alert: [e.g., hardware issue, thermal spike, power fluctuation, no clear issue]\\nNext Steps: [Recommended actions or checks for further triage]\", offline_mode=True), 'host_performance_check': HostPerformanceCheckToolConfig(description='This tool retrieves CPU usage, memory usage, and hardware I/O usage details for a given host. Args: host_id: str', llm_name='tool_reasoning_llm', parsing_prompt='You are given system performance data captured from a host. Your task is to extract and organize the information into a clean, structured JSON format. The input contains system details and performance metrics, such as CPU, memory, and disk I/O.\\n\\nFollow these instructions:\\n\\n1. Identify metric categories dynamically based on the line prefixes or column headers (e.g., \"Mem:\", \"Swap:\", \"CPU:\", \"Device:\").\\n2. For each category, extract the numerical values and map them to meaningful field names.\\n3. Group related fields under sections such as \"memory_usage\", \"swap_usage\", \"cpu_usage\", \"disk_io\", etc.\\n4. Use consistent, readable key names for all fields.\\n5. Return **only** the final JSON object — no explanations or extra text.\\n\\nHere is the input data:\\n{input_data}', analysis_prompt='You are analyzing system metrics to assess CPU and memory usage. Use the output below to determine whether CPU or memory usage is abnormally high, identify which processes are consuming the most resources, and assess whether the usage patterns could explain a recent alert.\\n\\nInstructions:\\n1. Evaluate overall CPU and memory usage levels.\\n2. List the top resource-consuming processes, including their name, PID, %CPU, and %MEM.\\n3. Identify any potential causes of high usage (e.g., memory leak, runaway process, legitimate high load).\\n4. Recommend possible next steps for investigation or mitigation.\\n\\nFormat your response as a structured summary:\\n\\nCPU Usage: Normal / High (X% usage)\\nMemory Usage: Normal / High (X% usage)\\nTop Resource-Consuming Processes: [Process name, PID, %CPU, %MEM]\\nPotential Cause of High Usage: [e.g., runaway process, heavy load, memory leak]\\nNext Steps: [Suggested mitigation actions]\\n\\nSystem Metrics Output:\\n{input_data}\\n', offline_mode=True), 'monitoring_process_check': MonitoringProcessCheckToolConfig(description='This tool checks the status of critical monitoring processes and services on a target host by executing system commands. Args: host_id: str', llm_name='tool_reasoning_llm', prompt='You are checking whether the telegraf service is running on the server. Use the monitoring output below to verify its status. If it’s not running, identify possible reasons and assess the impact.\\n\\nInstructions:\\n1. Check if the telegraf process is present and active.\\n2. Evaluate the potential impact of telegraf not running on system availability or monitoring.\\n3. Identify likely causes for the process not running.\\n\\nFormat your response as a structured summary:\\n* **Telegraf Running:** Yes / No\\n* **Potential Impact:** [e.g., host seems down to the monitoring system, delayed alerting]\\n* **Possible Cause:** [e.g., process crash, misconfiguration, resource constraints]\\n* **Next Steps:** [e.g., restart telegraf, check logs]\\n\\nMonitoring Output:\\n{input_data}', offline_mode=True), 'network_connectivity_check': NetworkConnectivityCheckToolConfig(description='This tool checks network connectivity of a host by running ping and socket connection tests. Args: host_id: str', llm_name='tool_reasoning_llm', prompt='You are assisting with alert triage by checking the network connectivity status of a host. Use the outputs from `ping` and `telnet` commands to determine whether the host is reachable. If connectivity issues are detected, analyze the possible root causes and provide a structured summary of your findings.\\n\\nInstructions:\\n1. Interpret the `ping` and `telnet` results to assess host reachability.\\n2. Determine whether there is a connectivity issue.\\n3. Identify potential causes, such as network failure, firewall restrictions, or service unavailability.\\n4. Recommend appropriate next steps for troubleshooting or escalation.\\n\\nFormat your response as a structured summary:\\n\\nPing Status: Successful / Failed\\nTelnet Status: Connected / Failed\\nPotential Cause of Connectivity Issue: [e.g., network failure, firewall rules, service outage, no issue]\\nNext Steps: [e.g., check network logs, restart network services, escalate issue, or no action needed]\\n\\nPing Output:\\n{ping_data}\\n\\nTelnet Output:\\n{telnet_data}', offline_mode=True), 'telemetry_metrics_host_heartbeat_check': TelemetryMetricsHostHeartbeatCheckToolConfig(description=\"This tool checks if a host's telemetry monitoring service is reporting heartbeat metrics. This tells us if the host is up and running. Args: host_id: str\", llm_name='tool_reasoning_llm', prompt=\"The following is the telemetry metrics fetched for the host to see if it's been up and running (if result is empty, then the monitoring service on the host is down):\\n{data}\\nBased on the data, summarize the fetched data and provide a conclusion of the host's running status.\", offline_mode=True, metrics_url=''), 'telemetry_metrics_host_performance_check': TelemetryMetricsHostPerformanceCheckToolConfig(description='This tool checks the performance of the host by analyzing the CPU usage timeseries. Args: host_id: str', llm_name='tool_reasoning_llm', prompt=\"You are an expert on analyzing CPU usage timeseries. Periodic usage peaks are expected benign system behavior.\\nUser will provide data in the format of a list of lists, where each sublist contains two elements: timestamp and CPU usage percentage. User will also provide statistics on the timeseries. Write a markdown report about what was observed in the timeseries.\\n\\nExample format:\\n# CPU Usage Analysis Report\\nThe data analysis is performed on 14 days of CPU usage percentage data.\\n\\n## Data Statistics\\ndata start and end time, data point interval, CPU usage statistics\\n\\n## Observations\\nany patterns observed? Should be one of the below cases:\\n- Are there any cyclic usage surges?\\n  - What is the cycle?\\n  - What is the high and low CPU usage of the pattern?\\n- Is there one anomalous peak?\\n  - When did it happen?\\n  - What is it like before and after?\\n- No obvious pattern? A mix of patterns? => it's normal flutuation of the system (max usage less than 60%)\\n  - What is the fluctuation range?\\n\\n## Conclusion\\nSummarize the observation.\\nCategories:\\n- peak in the data means the high CPU usage is an anomaly and requires attention\\n- periodic behvior means the high usage is benign\\n- overall moderate (max usage less than 60%) usage means no issue in the system\\n\\n## Pattern Label\\nAnomalous Peak/Periodic Surges/Normal Fluctuations\\n\", offline_mode=True, metrics_url=''), 'telemetry_metrics_analysis_agent': TelemetryMetricsAnalysisAgentConfig(description='This is a telemetry metrics tool used to monitor remotely collected telemetry data. It checks server heartbeat data to determine whether the server is up and running and analyzes CPU usage patterns over the past 14 days to identify potential CPU issues. Args: host_id: str, alert_type: str', tool_names=['telemetry_metrics_host_heartbeat_check', 'telemetry_metrics_host_performance_check'], llm_name='agent_llm', prompt=\"You arg a helpful alert triage assistant. Your task is to investigate an alert that was just triggered on a specific host. You will be given two inputs:\\n- `host_id`: the identifier of the host where the alert occurred.\\n- `alert_type`: the type of alert that triggered.\\n\\nUse the tools provided below to collect relevant telemetry data for the specified host:\\n\\nTools:\\n- `telemetry_metrics_host_heartbeat_check`: Use this to check the server's heartbeat and determine if the host is currently up and responsive.\\n- `telemetry_metrics_host_performance_check`: Use this to analyze CPU usage trends over the past 14 days and identify abnormal patterns.\\n\\nInstructions:\\n1. Run the appropriate tools based on the host and alert type.\\n2. Collect and include all relevant output from the tools in your response.\\n3. Analyze the data and provide reasoning to help determine whether the telemetry supports or explains the triggered alert.\\n\\nYour response should include:\\n- Raw data from each tool\\n- A concise summary of findings\\n- Any insights or hypotheses that explain the alert\"), 'maintenance_check': MaintenanceCheckToolConfig(description='Check if a host is under maintenance during the time of an alert to help determine if the alert can be deprioritized.', llm_name='agent_llm', prompt='User will provide you with a system alert represented in JSON format. You know for a fact that there is maintenance happening for the host. Maintenance start time for this host is : [{maintenance_start_str}]; end time is: [{maintenance_end_str}] (end time empty means that there is not yet a set end time for the maintenance on the host)\\nGenerate a markdown report in the following format:\\n\\n## Alert Summary\\n(summary of what happened in the alert JSON data)\\n\\n## Collected Metrics\\n(lay out the maintenance information)\\n\\n## Analysis\\n(Describe the maintenance status of this host)\\n\\n## Recommended Actions\\n(Bullet point list: write how the user may not need to worry about this alert given that the host is under maintenance, and they could check if the issue persists afterward)\\n\\n## Alert Status\\n(can deprioritize the investigation of the alert, host under maintenance)', static_data_path='/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/maintenance_static_dataset.csv', skip_maintenance_check=False), 'categorizer': CategorizerToolConfig(description='This is a categorization tool used at the end of the pipeline.', llm_name='agent_llm', prompt='You will be given a system-generated alert triage report. Your job is to read the report carefully and determine the most likely root cause of the issue. Then, categorize the root cause into one of the following predefined categories:\\n\\n**Valid Categories**\\n- `software`: The alert was triggered due to a malfunctioning or inactive monitoring service (e.g., Telegraf not running).\\n- `network_connectivity`: The host is not reachable via ping or curl, or there are signs of connection issues due to blocked ports, broken services, or firewall rules (e.g., telnet fails).\\n- `hardware`: The alert is caused by a hardware failure or degradation.\\n- `repetitive_behavior`: The alert is triggered by a recurring or periodic behavior pattern (e.g., regular CPU spikes or memory surges).\\n- `false_positive`: No clear signs of failure or degradation; system appears healthy and no suspicious pattern is found.\\n- `need_investigation`: The report contains conflicting, ambiguous, or insufficient information to determine a clear root cause.\\n\\n**Response Format**\\n- Line 1: Output only the category name (e.g., `hardware`)\\n- Line 2: Briefly explain your reasoning based on the contents of the report.\\n- Example response:\\nnetwork_connectivity\\nPing and curl to the host both failed, and telnet to the monitored port timed out, indicating a likely connectivity or firewall issue.\\n\\n**Important Guidelines**\\n- Base your categorization only on evidence presented in the report.\\n- If no category clearly fits, default to `need_investigation`.')} function_groups={} llms={'agent_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=2048), 'tool_reasoning_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.2, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=2048), 'nim_rag_eval_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/alert_triage_model_selection_output/optimizer'), eval_metrics={'classification_accuracy': OptimizerMetric(evaluator_name='classification_accuracy', direction='maximize', weight=1.0), 'llm_latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=1, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=False, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=AlertTriageAgentWorkflowConfig(tool_names=['hardware_check', 'host_performance_check', 'monitoring_process_check', 'network_connectivity_check', 'telemetry_metrics_analysis_agent'], llm_name='agent_llm', offline_mode=True, offline_data_path='/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/offline_data.csv', benign_fallback_data_path='/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/benign_fallback_offline_data.json', agent_prompt='**Role**\\nYou are a Triage Agent responsible for diagnosing and troubleshooting system alerts in real time. Your goal is to determine whether an alert indicates a true issue, identify the root cause, and provide a clear, structured triage report to assist system analysts.\\n\\n\\n**Instructions**\\n\\n1. **Analyze the Alert**\\n   Begin by interpreting the incoming alert. Identify its type (e.g., *InstanceDown*, *HighCPUUsage*) and note any relevant details.\\n\\n2. **Select and Use Diagnostic Tools**\\n   Based on the alert type, choose the most relevant tools to gather system metrics. Use each tool only once per alert.\\n\\n   - `hardware_check`: Retrieves server power status and hardware health via IPMI. Useful for diagnosing instance down alerts or suspected hardware failures.\\n   - `host_performance_check`: Collects system-level CPU and memory usage using commands like `top` and `ps`. Use this to identify host\\'s resource (CPR and memory) usage bottlenecks.\\n   - `monitoring_process_check`: Checks whether critical processes are running on the host. Useful for verifying system functionality during instance down or degraded performance.\\n   - `network_connectivity_check`: Tests host connectivity through ping, telnet, and HTTP health checks. Helps determine if the server is reachable from the network.\\n   - `telemetry_metrics_analysis_agent`: Pulls telemetry metrics to check host status and analyze usage trends. Effective for validating instance uptime and system load over time.\\n\\n   Once you\\'ve received outputs from all selected tools, **pause to analyze them before proceeding further**.\\n\\n3. **Correlate Data and Determine Root Cause**\\n   - Evaluate the retrieved metrics against the alert details.\\n   - Determine if the alert reflects a real problem or is a false positive.\\n   - If an issue is detected, identify likely causes—such as hardware failure, performance bottlenecks, or network issues.\\n\\n4. **Generate a Structured Triage Report (in Markdown format)**\\n   Organize your findings clearly under these sections:\\n\\n   - **Alert Summary**: Brief description of the alert received.\\n   - **Collected Metrics**: Outputs from the diagnostic tools used.\\n   - **Analysis**: Interpretation of the data and how it relates to the alert.\\n   - **Recommended Actions**: Suggested next steps to mitigate or resolve the issue.\\n   - **Alert Status**: Choose one — \"Valid\", \"Abnormal but benign\", or \"False alarm\".\\n\\n\\n**Important Rules**\\n- Do not call the same tool more than once per alert.\\n- Analyze tool outputs before taking any additional action.\\n- Stay concise, structured, and actionable.') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/alert_triage_model_selection_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/offline_data.json'), profiler=None), evaluators={'accuracy': RagasEvaluatorConfig(llm_name='nim_rag_eval_llm', metric='AnswerAccuracy', input_obj_field=None), 'groundedness': RagasEvaluatorConfig(llm_name='nim_rag_eval_llm', metric='ResponseGroundedness', input_obj_field=None), 'relevance': RagasEvaluatorConfig(llm_name='nim_rag_eval_llm', metric='ContextRelevance', input_obj_field=None), 'classification_accuracy': ClassificationEvaluatorConfig(), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8)})\n",
      "2025-10-25 15:51:06 - INFO     - nat_alert_triage_agent:104 - Preloaded test data from: /Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/offline_data.csv\n",
      "2025-10-25 15:51:06 - INFO     - nat_alert_triage_agent:108 - Preloaded benign fallback data from: /Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/benign_fallback_offline_data.json\n",
      "2025-10-25 15:51:06 - INFO     - nat_alert_triage_agent:80 - ================================================Running in offline mode=================================================\n",
      "Running workflow:   0%|                                   | 0/7 [00:00<?, ?it/s]2025-10-25 15:51:06 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-0.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:51:06 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-1.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:51:06 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-2.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:51:06 - INFO     - nat_alert_triage_agent:258 - Host: [test-instance-3.example.com] is under maintenance according to the maintenance database\n",
      "2025-10-25 15:51:09 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-4.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:51:09 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-5.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:51:09 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-6.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:51:09 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  14%|███▊                       | 1/7 [00:03<00:19,  3.19s/it]2025-10-25 15:51:54 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  29%|███████▋                   | 2/7 [00:48<02:20, 28.17s/it]2025-10-25 15:52:19 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  43%|███████████▌               | 3/7 [01:13<01:46, 26.62s/it]2025-10-25 15:52:37 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  57%|███████████████▍           | 4/7 [01:31<01:09, 23.02s/it]2025-10-25 15:53:10 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  71%|███████████████████▎       | 5/7 [02:04<00:53, 26.61s/it]2025-10-25 15:53:28 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  86%|███████████████████████▏   | 6/7 [02:22<00:23, 23.87s/it]2025-10-25 15:53:43 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow: 100%|███████████████████████████| 7/7 [02:37<00:00, 22.48s/it]\n",
      "Evaluating Ragas nv_accuracy:   0%|                       | 0/7 [00:00<?, ?it/s]\n",
      "Evaluating Ragas nv_response_groundedness:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Evaluating Ragas nv_context_relevance:   0%|              | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating classification accuracy:   0%|                 | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                         | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating classification accuracy: 100%|████████| 7/7 [00:00<00:00, 279.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg LLM Latency: 100%|████████████████| 7/7 [00:00<00:00, 279.16it/s]\n",
      "Evaluating Avg Tokens/LLM_END: 100%|█████████████| 7/7 [00:00<00:00, 270.10it/s]\n",
      "\n",
      "Evaluating Ragas nv_response_groundedness:  14%|▎ | 1/7 [00:00<00:02,  2.40it/s]\u001b[AAn error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "Evaluating Ragas nv_response_groundedness:  29%|▌ | 2/7 [00:00<00:01,  3.87it/s]\u001b[A\n",
      "\n",
      "Evaluating Ragas nv_context_relevance:  14%|▊     | 1/7 [00:00<00:02,  2.70it/s]\u001b[A\u001b[AAn error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:53:44 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  14%|██▏            | 1/7 [00:00<00:05,  1.03it/s]An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_context_relevance:  43%|██▌   | 3/7 [00:00<00:00,  7.07it/s]\u001b[A\u001b[A2025-10-25 15:53:44 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  29%|████▎          | 2/7 [00:01<00:03,  1.53it/s]An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:53:44 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_context_relevance:  71%|████▎ | 5/7 [00:00<00:00,  5.71it/s]\u001b[A\u001b[A2025-10-25 15:53:44 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  57%|████████▌      | 4/7 [00:01<00:00,  3.62it/s]2025-10-25 15:53:44 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:53:45 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:53:45 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:53:45 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  86%|████████████▊  | 6/7 [00:01<00:00,  4.73it/s]An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "Evaluating Ragas nv_response_groundedness:  57%|█▏| 4/7 [00:01<00:01,  2.80it/s]\u001b[AAn error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "Evaluating Ragas nv_response_groundedness:  86%|█▋| 6/7 [00:01<00:00,  4.68it/s]\u001b[A\n",
      "Evaluating Ragas nv_accuracy: 100%|███████████████| 7/7 [00:02<00:00,  3.30it/s]\u001b[A\n",
      "\n",
      "Evaluating Ragas nv_context_relevance: 100%|██████| 7/7 [00:02<00:00,  2.55it/s]\u001b[A\u001b[A\n",
      "Evaluating Ragas nv_response_groundedness: 100%|██| 7/7 [00:03<00:00,  2.23it/s]\n",
      "Evaluating Ragas nv_accuracy: 100%|███████████████| 7/7 [00:03<00:00,  1.88it/s]\n",
      "2025-10-25 15:53:47 - INFO     - nat_alert_triage_agent:164 - Cleaning up\n",
      "2025-10-25 15:53:47 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:53:47 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/alert_triage_model_selection_output/workflow_output.json\n",
      "2025-10-25 15:53:47 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/classification_accuracy_output.json\n",
      "2025-10-25 15:53:47 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/llm_latency_output.json\n",
      "2025-10-25 15:53:47 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/token_efficiency_output.json\n",
      "2025-10-25 15:53:47 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/relevance_output.json\n",
      "2025-10-25 15:53:47 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/groundedness_output.json\n",
      "2025-10-25 15:53:47 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/accuracy_output.json\n",
      "\u001b[32m[I 2025-10-25 15:53:47,262]\u001b[0m Trial 3 finished with values: [0.43, 8.3] and parameters: {'llms.agent_llm.temperature': 0.0, 'llms.agent_llm.model_name': 'meta/llama-3.1-8b-instruct'}.\u001b[0m\n",
      "2025-10-25 15:53:47 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:127 - Numeric optimization finished\n",
      "2025-10-25 15:53:47 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:173 - Generating Pareto front visualizations...\n",
      "2025-10-25 15:53:47 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:408 - Creating Pareto front visualizations...\n",
      "2025-10-25 15:53:47 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:409 - Total trials: 4\n",
      "2025-10-25 15:53:47 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:410 - Pareto optimal trials: 1\n",
      "2025-10-25 15:53:48 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:143 - 2D Pareto plot saved to: tmp_workflow/alert_triage_model_selection_output/optimizer/plots/pareto_front_2d.png\n",
      "2025-10-25 15:53:49 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:231 - Parallel coordinates plot saved to: tmp_workflow/alert_triage_model_selection_output/optimizer/plots/pareto_parallel_coordinates.png\n",
      "2025-10-25 15:53:49 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:325 - Pairwise matrix plot saved to: tmp_workflow/alert_triage_model_selection_output/optimizer/plots/pareto_pairwise_matrix.png\n",
      "2025-10-25 15:53:49 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:438 - Visualization complete!\n",
      "2025-10-25 15:53:49 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:440 - Plots saved to: tmp_workflow/alert_triage_model_selection_output/optimizer/plots\n",
      "2025-10-25 15:53:49 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:182 - Pareto visualizations saved to: tmp_workflow/alert_triage_model_selection_output/optimizer/plots\n",
      "2025-10-25 15:53:49 - INFO     - nat.profiler.parameter_optimization.optimizer_runtime:66 - All optimization phases complete.\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!nat optimize --config_file tmp_workflow/configs/alert_triage_config_model_selection.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bd59bb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search Optimization Results\n",
      "================================================================================\n",
      "\n",
      "Trials Summary:\n",
      " number  values_classification_accuracy  values_llm_latency             datetime_start          datetime_complete               duration params_llms.agent_llm.model_name  params_llms.agent_llm.temperature      rep_scores  system_attrs_grid_id                                                                                                              system_attrs_search_space    state  pareto_optimal\n",
      "      0                            0.29                8.67 2025-10-25 15:44:57.266748 2025-10-25 15:46:28.229276 0 days 00:01:30.962528      meta/llama-3.1-70b-instruct                                0.0  [[0.29, 8.67]]                     0 {'llms.agent_llm.model_name': ['meta/llama-3.1-8b-instruct', 'meta/llama-3.1-70b-instruct'], 'llms.agent_llm.temperature': [0.0, 0.5]} COMPLETE           False\n",
      "      1                            0.14               12.92 2025-10-25 15:46:28.229468 2025-10-25 15:48:28.625553 0 days 00:02:00.396085      meta/llama-3.1-70b-instruct                                0.5 [[0.14, 12.92]]                     1 {'llms.agent_llm.model_name': ['meta/llama-3.1-8b-instruct', 'meta/llama-3.1-70b-instruct'], 'llms.agent_llm.temperature': [0.0, 0.5]} COMPLETE           False\n",
      "      2                            0.43                7.88 2025-10-25 15:48:28.625670 2025-10-25 15:51:06.034491 0 days 00:02:37.408821       meta/llama-3.1-8b-instruct                                0.5  [[0.43, 7.88]]                     2 {'llms.agent_llm.model_name': ['meta/llama-3.1-8b-instruct', 'meta/llama-3.1-70b-instruct'], 'llms.agent_llm.temperature': [0.0, 0.5]} COMPLETE            True\n",
      "      3                            0.43                8.30 2025-10-25 15:51:06.034716 2025-10-25 15:53:47.262492 0 days 00:02:41.227776       meta/llama-3.1-8b-instruct                                0.0   [[0.43, 8.3]]                     3 {'llms.agent_llm.model_name': ['meta/llama-3.1-8b-instruct', 'meta/llama-3.1-70b-instruct'], 'llms.agent_llm.temperature': [0.0, 0.5]} COMPLETE           False\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the optimizer results\n",
    "trials_df_path = Path(\"tmp_workflow/alert_triage_model_selection_output/optimizer/trials_dataframe_params.csv\")\n",
    "\n",
    "if trials_df_path.exists():\n",
    "    trials_df = pd.read_csv(trials_df_path)\n",
    "\n",
    "    print(\"Grid Search Optimization Results\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nTrials Summary:\")\n",
    "    print(trials_df.to_string(index=False))\n",
    "    print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67550b0",
   "metadata": {},
   "source": [
    "<a id=\"eval-triage-agent2\"></a>\n",
    "## 2.6) Re-evaluate the optimized tool-calling agent\n",
    "\n",
    "After completing the `nat optimize` run above, a new file with the optimal parameters from the search have been serialized and saved to `'./tmp_workflow/alert_triage_model_selection_output/optimizer/optimized_config.yml`.\n",
    "\n",
    "<div style=\"color: red; font-style: italic;\">\n",
    "<strong>Note:</strong> Performance of the optimized model may vary due to size of prior search space and number of evaluation trials.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "506ef10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-25 15:54:01 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: tmp_workflow/alert_triage_model_selection_output/optimizer/optimized_config.yml\n",
      "2025-10-25 15:54:21 - INFO     - nat_alert_triage_agent:104 - Preloaded test data from: /Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/offline_data.csv\n",
      "2025-10-25 15:54:21 - INFO     - nat_alert_triage_agent:108 - Preloaded benign fallback data from: /Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/benign_fallback_offline_data.json\n",
      "2025-10-25 15:54:21 - INFO     - nat_alert_triage_agent:80 - ================================================Running in offline mode=================================================\n",
      "Running workflow:   0%|                                   | 0/7 [00:00<?, ?it/s]2025-10-25 15:54:22 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-0.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:54:22 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-1.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:54:22 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-2.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:54:22 - INFO     - nat_alert_triage_agent:258 - Host: [test-instance-3.example.com] is under maintenance according to the maintenance database\n",
      "2025-10-25 15:54:27 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-4.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:54:27 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-5.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:54:27 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-6.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 15:54:27 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  14%|███▊                       | 1/7 [00:04<00:26,  4.39s/it]2025-10-25 15:55:59 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  29%|███████▋                   | 2/7 [01:36<04:40, 56.15s/it]2025-10-25 15:56:32 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  43%|███████████▌               | 3/7 [02:09<03:01, 45.47s/it]2025-10-25 15:56:32 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "2025-10-25 15:56:37 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  71%|███████████████████▎       | 5/7 [02:14<00:42, 21.14s/it]2025-10-25 15:56:50 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  86%|███████████████████████▏   | 6/7 [02:27<00:18, 18.83s/it]2025-10-25 15:57:26 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow: 100%|███████████████████████████| 7/7 [03:03<00:00, 26.28s/it]\n",
      "Evaluating Ragas nv_accuracy:   0%|                       | 0/7 [00:00<?, ?it/s]\n",
      "Evaluating classification accuracy:   0%|                 | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Evaluating Ragas nv_response_groundedness:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating Avg LLM Latency:   0%|                         | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_context_relevance:   0%|              | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Avg Tokens/LLM_END:   0%|                      | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Evaluating classification accuracy:  14%|█▎       | 1/7 [00:00<00:02,  2.60it/s]\u001b[A\n",
      "\n",
      "\n",
      "Evaluating classification accuracy: 100%|█████████| 7/7 [00:00<00:00, 16.96it/s]\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg LLM Latency: 100%|█████████████████| 7/7 [00:00<00:00, 30.92it/s]\n",
      "Evaluating Avg Tokens/LLM_END: 100%|█████████████| 7/7 [00:00<00:00, 202.09it/s]\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_response_groundedness:  14%|▎ | 1/7 [00:00<00:02,  2.39it/s]\u001b[A\u001b[A2025-10-25 15:57:27 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  14%|██▏            | 1/7 [00:00<00:04,  1.27it/s]\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_context_relevance:  14%|▊     | 1/7 [00:00<00:02,  2.49it/s]\u001b[A\u001b[A\u001b[A\u001b[AAn error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_response_groundedness:  29%|▌ | 2/7 [00:00<00:01,  3.60it/s]\u001b[A\u001b[AAn error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_context_relevance:  43%|██▌   | 3/7 [00:00<00:00,  6.59it/s]\u001b[A\u001b[A\u001b[A\u001b[AAn error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:57:28 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  29%|████▎          | 2/7 [00:01<00:03,  1.53it/s]An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_context_relevance:  71%|████▎ | 5/7 [00:00<00:00,  5.38it/s]\u001b[A\u001b[A\u001b[A\u001b[AAn error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_response_groundedness:  43%|▊ | 3/7 [00:01<00:01,  2.45it/s]\u001b[A\u001b[A2025-10-25 15:57:28 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:57:28 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  43%|██████▍        | 3/7 [00:01<00:01,  2.47it/s]2025-10-25 15:57:28 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-25 15:57:28 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:57:28 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 15:57:28 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_response_groundedness:  71%|█▍| 5/7 [00:01<00:00,  4.59it/s]\u001b[A\u001b[A2025-10-25 15:57:28 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "Evaluating Ragas nv_accuracy:  86%|████████████▊  | 6/7 [00:01<00:00,  6.23it/s]An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_context_relevance:  86%|█████▏| 6/7 [00:01<00:00,  4.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Evaluating Ragas nv_response_groundedness:  86%|█▋| 6/7 [00:01<00:00,  4.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating Ragas nv_response_groundedness: 100%|██| 7/7 [00:02<00:00,  3.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_context_relevance: 100%|██████| 7/7 [00:02<00:00,  2.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Ragas nv_response_groundedness: 100%|██| 7/7 [00:02<00:00,  2.38it/s]\n",
      "Evaluating Ragas nv_accuracy: 100%|███████████████| 7/7 [00:03<00:00,  2.11it/s]\n",
      "2025-10-25 15:57:30 - INFO     - nat_alert_triage_agent:164 - Cleaning up\n",
      "2025-10-25 15:57:30 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 15:57:30 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/alert_triage_model_selection_output/workflow_output.json\n",
      "2025-10-25 15:57:30 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/classification_accuracy_output.json\n",
      "2025-10-25 15:57:30 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/llm_latency_output.json\n",
      "2025-10-25 15:57:30 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/token_efficiency_output.json\n",
      "2025-10-25 15:57:30 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/relevance_output.json\n",
      "2025-10-25 15:57:30 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/groundedness_output.json\n",
      "2025-10-25 15:57:30 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_model_selection_output/accuracy_output.json\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "# path-check-skip-next-line\n",
    "!nat eval --config_file ./tmp_workflow/alert_triage_model_selection_output/optimizer/optimized_config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b9a10743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Alerts Evaluated: 7\n",
      "Classification Accuracy Average Score: 29.00%\n",
      "LLM Latency Average Score: 9.55sec\n"
     ]
    }
   ],
   "source": [
    "# Load and display classification accuracy results\n",
    "# path-check-skip-next-line\n",
    "with open('./tmp_workflow/alert_triage_model_selection_output/classification_accuracy_output.json') as f:\n",
    "    classification_results = json.load(f)\n",
    "print(f\"Total Alerts Evaluated: {len(classification_results['eval_output_items'])}\")\n",
    "print(f\"Classification Accuracy Average Score: {classification_results['average_score']:.2%}\")\n",
    "\n",
    "# Load and display RAG accuracy results\n",
    "# path-check-skip-next-line\n",
    "with open('./tmp_workflow/alert_triage_model_selection_output/llm_latency_output.json') as f:\n",
    "    latency_results = json.load(f)\n",
    "\n",
    "print(f\"LLM Latency Average Score: {latency_results['average_score']}sec\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58efda71",
   "metadata": {},
   "source": [
    "Up to this point, we have shown how to add models and tunable LLM parameters to the `SearchSpace`. We have demonstrated this using `sampler: grid`, which uses Optuna's grid search methods to create a deterministic search space for all of the unique combinations for all `optimizable_params` in the configuration. If range of search parameters is large, and a grid search produces too many unique combinations, users may optionally specify `sampler: bayesian` in their configuration, and use Optuna's `TPESampler` (univariate) and genetic algorithm (multivariable) samplers to use non-deterministic search methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b6eb63",
   "metadata": {},
   "source": [
    "<a id=\"model-and-prompt-tuning\"></a>\n",
    "# 3.0) Concurrent Model Parameter and Prompt Tuning\n",
    "\n",
    "NAT uses a Genetic Algorithm (GA) to automatically optimize prompts through evolutionary search. This is a sophisticated approach that treats prompts as \"individuals\" in a population that evolves over multiple generations to find better-performing variations. The genetic algorithm is inspired by natural evolution and uses LLMs themselves to intelligently mutate and recombine prompts. Instead of random mutations like traditional GAs, NAT leverages the reasoning capabilities of LLMs to make informed changes to prompts.\n",
    "\n",
    "*Note: The genetic algorithm for prompt optimization is configured through several parameters:*\n",
    "- *`prompt.enabled`: Enable GA-based prompt optimization (default: `false`)*\n",
    "- *`prompt.ga_population_size`: Population size - larger populations increase diversity but cost more per generation (default: `10`)*\n",
    "- *`prompt.ga_generations`: Number of generations to evolve prompts (default: `5`)*\n",
    "- *`prompt.ga_offspring_size`: Number of offspring per generation - if `null`, defaults to `ga_population_size - ga_elitism`*\n",
    "- *`prompt.ga_crossover_rate`: Probability of recombination between two parents for each prompt parameter (default: `0.7`)*\n",
    "- *`prompt.ga_mutation_rate`: Probability of mutating a child's prompt parameter using the LLM optimizer (default: `0.1`)*\n",
    "- *`prompt.ga_elitism`: Number of elite individuals copied unchanged to the next generation (default: `1`)*\n",
    "- *`prompt.ga_selection_method`: Parent selection scheme - `tournament` (default) or `roulette`*\n",
    "- *`prompt.ga_tournament_size`: Tournament size when using tournament selection (default: `3`)*\n",
    "- *`prompt.ga_parallel_evaluations`: Maximum number of concurrent evaluations (default: `8`)*\n",
    "- *`prompt.ga_diversity_lambda`: Diversity penalty strength to discourage duplicate prompt sets - `0.0` disables it (default: `0.0`)- *`prompt.prompt_population_init_function`: Function name used to mutate base prompts to seed the initial population and perform tations. NAT includes a built-in `prompt_init` Function you can use.*\n",
    "- *`prompt.prompt_recombination_function`: Optional function name used to recombine two parent prompts into a child prompt. NAT cludes a built-in `prompt_recombiner` Function you can use.*\n",
    "\n",
    "** For more information see the [Optimizer documentation](../../docs/source/reference/optimizer.md) or go to your working branch on [Github - dev](https://github.com/NVIDIA/NeMo-Agent-Toolkit/blob/develop/docs/source/reference/optimizer.md).**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bbcac0",
   "metadata": {},
   "source": [
    "<a id=\"all-tuning-config\"></a>\n",
    "## 3.1) Optimizer configuration for all parameters (models, hyperparameters, and prompts)\n",
    "\n",
    "For this experiment we will create a new configuration at `tmp_workflow/configs/alert_triage_all_params_selection.yml`, for which we will configure an optimizer run to find the best model (backbone LLM only), hyperparameters (temperature only), and prompts. We can use our existing Alert Triage Agent here, with a modified config. Let's create a new config called `./tmp_workflow/configs/alert_triage_config_all_params_selection.yml` to manage this workflow for us.\n",
    "\n",
    "First we will copy the same base configuration as the last example - with updated output paths for this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cca1ad89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./tmp_workflow/configs/alert_triage_config_all_params_selection.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./tmp_workflow/configs/alert_triage_config_all_params_selection.yml\n",
    "# path-check-skip-begin\n",
    "functions:\n",
    "  hardware_check:\n",
    "    _type: hardware_check\n",
    "    llm_name: tool_reasoning_llm\n",
    "    offline_mode: true\n",
    "  host_performance_check:\n",
    "    _type: host_performance_check\n",
    "    llm_name: tool_reasoning_llm\n",
    "    offline_mode: true\n",
    "  monitoring_process_check:\n",
    "    _type: monitoring_process_check\n",
    "    llm_name: tool_reasoning_llm\n",
    "    offline_mode: true\n",
    "  network_connectivity_check:\n",
    "    _type: network_connectivity_check\n",
    "    llm_name: tool_reasoning_llm\n",
    "    offline_mode: true\n",
    "  telemetry_metrics_host_heartbeat_check:\n",
    "    _type: telemetry_metrics_host_heartbeat_check\n",
    "    llm_name: tool_reasoning_llm\n",
    "    offline_mode: true\n",
    "  telemetry_metrics_host_performance_check:\n",
    "    _type: telemetry_metrics_host_performance_check\n",
    "    llm_name: tool_reasoning_llm\n",
    "    offline_mode: true\n",
    "  telemetry_metrics_analysis_agent:\n",
    "    _type: telemetry_metrics_analysis_agent\n",
    "    tool_names:\n",
    "      - telemetry_metrics_host_heartbeat_check\n",
    "      - telemetry_metrics_host_performance_check\n",
    "    llm_name: agent_llm\n",
    "  maintenance_check:\n",
    "    _type: maintenance_check\n",
    "    llm_name: agent_llm\n",
    "    static_data_path: PLACEHOLDER_maintenance_static_dataset.csv\n",
    "  categorizer:\n",
    "    _type: categorizer\n",
    "    llm_name: agent_llm\n",
    "\n",
    "workflow:\n",
    "  _type: alert_triage_agent\n",
    "  tool_names:\n",
    "    - hardware_check\n",
    "    - host_performance_check\n",
    "    - monitoring_process_check\n",
    "    - network_connectivity_check\n",
    "    - telemetry_metrics_analysis_agent\n",
    "  llm_name: agent_llm\n",
    "  offline_mode: true\n",
    "  offline_data_path: PLACEHOLDER_offline_data.csv\n",
    "  benign_fallback_data_path: PLACEHOLDER_benign_fallback_offline_data.json\n",
    "  optimizable_params:\n",
    "    - agent_prompt\n",
    "  search_space:\n",
    "    agent_prompt:\n",
    "      is_prompt: true\n",
    "      prompt_purpose: \"Guide the agent to effectively diagnose system alerts, gather relevant metrics, and provide clear triage analysis with actionable recommendations.\"\n",
    "\n",
    "\n",
    "llms:\n",
    "  agent_llm:\n",
    "    _type: nim\n",
    "    model_name: meta/llama-3.1-8b-instruct\n",
    "    temperature: 0.0\n",
    "    max_tokens: 2048\n",
    "    optimizable_params:\n",
    "      - model_name\n",
    "      - temperature\n",
    "    search_space:\n",
    "      model_name:\n",
    "        values:\n",
    "          - meta/llama-3.1-8b-instruct\n",
    "          - meta/llama-3.1-70b-instruct\n",
    "          # - meta/llama-3.1-405b-instruct\n",
    "          # - meta/llama-3.3-3b-instruct\n",
    "          # - meta/llama-3.3-70b-instruct\n",
    "          # - meta/llama-4-scout-17b-16e-instruct\n",
    "          # - openai/gpt-oss-20b\n",
    "          # - openai/gpt-oss-120b\n",
    "          # - ibm/granite-3.3-8b-instruct\n",
    "          # - mistralai/mistral-small-3.1-24b-instruct-2503\n",
    "          # - mistralai/mistral-medium-3-instruct\n",
    "      temperature:\n",
    "        values:\n",
    "          - 0.0\n",
    "          - 0.5\n",
    "  tool_reasoning_llm:\n",
    "    _type: nim\n",
    "    model_name: meta/llama-3.1-70b-instruct\n",
    "    temperature: 0.2\n",
    "    max_tokens: 2048\n",
    "  nim_rag_eval_llm:\n",
    "    _type: nim\n",
    "    model_name: meta/llama-3.1-70b-instruct\n",
    "    max_tokens: 8\n",
    "\n",
    "eval:\n",
    "  general:\n",
    "    output_dir: ./tmp_workflow/alert_triage_all_params_selection_output/\n",
    "    dataset:\n",
    "      _type: json\n",
    "      file_path: PLACEHOLDER_offline_data.json\n",
    "  evaluators:\n",
    "    classification_accuracy:\n",
    "      _type: classification_accuracy\n",
    "    llm_latency:\n",
    "      _type: avg_llm_latency\n",
    "    token_efficiency:\n",
    "      _type: avg_tokens_per_llm_end\n",
    "    rag_accuracy:\n",
    "      _type: ragas\n",
    "      metric: AnswerAccuracy\n",
    "      llm_name: nim_rag_eval_llm\n",
    "  profiler:\n",
    "    token_uniqueness_forecast: true\n",
    "    workflow_runtime_forecast: true\n",
    "    compute_llm_metrics: true\n",
    "    csv_exclude_io_text: true\n",
    "    prompt_caching_prefixes:\n",
    "      enable: true\n",
    "      min_frequency: 0.1\n",
    "    bottleneck_analysis:\n",
    "      enable_nested_stack: true\n",
    "    concurrency_spike_analysis:\n",
    "      enable: true\n",
    "      spike_threshold: 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1eb804",
   "metadata": {},
   "source": [
    "Then we will add in updated optimizer configuration code that allows the system prompts to be optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0f4646a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./tmp_workflow/configs/alert_triage_config_all_params_selection.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./tmp_workflow/configs/alert_triage_config_all_params_selection.yml\n",
    "optimizer:\n",
    "  output_path: ./tmp_workflow/alert_triage_all_params_selection_output/optimizer/\n",
    "  reps_per_param_set: 1\n",
    "  eval_metrics:\n",
    "    classification_accuracy:\n",
    "      evaluator_name: classification_accuracy\n",
    "      direction: maximize\n",
    "    llm_latency:\n",
    "      evaluator_name: llm_latency\n",
    "      direction: minimize\n",
    "  numeric:\n",
    "    enabled: true\n",
    "    sampler: grid\n",
    "  prompt:\n",
    "    enabled: true\n",
    "# path-check-skip-end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1c14f8",
   "metadata": {},
   "source": [
    "Again, we will replace the placeholder paths for the output artifacts based on our earlier NAT source code pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bb3316dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Config written with data paths from: /Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data\n"
     ]
    }
   ],
   "source": [
    "# Replace placeholder paths with actual package data paths\n",
    "import importlib.resources\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the package data path\n",
    "package_data = importlib.resources.files('nat_alert_triage_agent').joinpath('data')\n",
    "\n",
    "# Read the YAML file\n",
    "config_path = Path('./tmp_workflow/configs/alert_triage_config_all_params_selection.yml')\n",
    "with open(config_path) as f:\n",
    "    config_content = f.read()\n",
    "\n",
    "# Replace placeholders with actual paths\n",
    "replacements = {\n",
    "    'PLACEHOLDER_maintenance_static_dataset.csv': str(package_data / 'maintenance_static_dataset.csv'),\n",
    "    'PLACEHOLDER_offline_data.csv': str(package_data / 'offline_data.csv'),\n",
    "    'PLACEHOLDER_benign_fallback_offline_data.json': str(package_data / 'benign_fallback_offline_data.json'),\n",
    "    'PLACEHOLDER_offline_data.json': str(package_data / 'offline_data.json')\n",
    "}\n",
    "\n",
    "for placeholder, actual_path in replacements.items():\n",
    "    config_content = config_content.replace(placeholder, actual_path)\n",
    "\n",
    "# Write back to file\n",
    "with open(config_path, 'w') as f:\n",
    "    f.write(config_content)\n",
    "\n",
    "print(f\"✓ Config written with data paths from: {package_data}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceceea3",
   "metadata": {},
   "source": [
    "<a id=\"all-tuning-initial-eval\"></a>\n",
    "## 3.2) Evaluate the agent\n",
    "\n",
    "As we've already tested this agent in Section 2.3, we will go right ahead to an initial evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cffc1569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-25 16:12:33 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: tmp_workflow/configs/alert_triage_config_all_params_selection.yml\n",
      "2025-10-25 16:12:57 - INFO     - nat_alert_triage_agent:104 - Preloaded test data from: /Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/offline_data.csv\n",
      "2025-10-25 16:12:57 - INFO     - nat_alert_triage_agent:108 - Preloaded benign fallback data from: /Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/benign_fallback_offline_data.json\n",
      "2025-10-25 16:12:57 - INFO     - nat_alert_triage_agent:80 - ================================================Running in offline mode=================================================\n",
      "Running workflow:   0%|                                   | 0/7 [00:00<?, ?it/s]2025-10-25 16:12:59 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-0.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 16:12:59 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-1.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 16:12:59 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-2.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 16:12:59 - INFO     - nat_alert_triage_agent:258 - Host: [test-instance-3.example.com] is under maintenance according to the maintenance database\n",
      "2025-10-25 16:13:03 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-4.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 16:13:03 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-5.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 16:13:03 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-6.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 16:13:03 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  14%|███▊                       | 1/7 [00:03<00:19,  3.27s/it]2025-10-25 16:13:57 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  29%|███████▋                   | 2/7 [00:57<02:47, 33.45s/it]2025-10-25 16:14:37 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  43%|███████████▌               | 3/7 [01:37<02:24, 36.22s/it]2025-10-25 16:14:57 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  57%|███████████████▍           | 4/7 [01:57<01:29, 29.93s/it]2025-10-25 16:15:09 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  71%|███████████████████▎       | 5/7 [02:09<00:47, 23.53s/it]2025-10-25 16:15:57 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  86%|███████████████████████▏   | 6/7 [02:57<00:31, 31.80s/it]2025-10-25 16:16:16 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow: 100%|███████████████████████████| 7/7 [03:16<00:00, 28.07s/it]\n",
      "Evaluating classification accuracy:   0%|                 | 0/7 [00:00<?, ?it/s]\n",
      "Evaluating Avg LLM Latency:   0%|                         | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Evaluating Avg Tokens/LLM_END:   0%|                      | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating classification accuracy:  14%|█▎       | 1/7 [00:00<00:02,  2.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg LLM Latency:  14%|██▍              | 1/7 [00:00<00:02,  2.30it/s]\u001b[A\n",
      "\n",
      "Evaluating classification accuracy: 100%|█████████| 7/7 [00:00<00:00, 15.48it/s]\u001b[A\u001b[A\n",
      "Evaluating Avg LLM Latency: 100%|█████████████████| 7/7 [00:00<00:00, 15.48it/s]\n",
      "Evaluating Avg Tokens/LLM_END: 100%|██████████████| 7/7 [00:00<00:00, 15.48it/s]\n",
      "2025-10-25 16:16:17 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  14%|██▏            | 1/7 [00:01<00:06,  1.12s/it]\u001b[A\u001b[A\u001b[A2025-10-25 16:16:17 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  43%|██████▍        | 3/7 [00:01<00:01,  2.61it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  71%|██████████▋    | 5/7 [00:01<00:00,  4.38it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  86%|████████████▊  | 6/7 [00:06<00:01,  1.51s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy: 100%|███████████████| 7/7 [00:07<00:00,  1.09s/it]\u001b[A\u001b[A\u001b[A\n",
      "2025-10-25 16:16:23 - INFO     - nat_alert_triage_agent:164 - Cleaning up\n",
      "2025-10-25 16:16:23 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 16:16:24 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/alert_triage_all_params_selection_output/workflow_output.json\n",
      "2025-10-25 16:16:24 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_all_params_selection_output/classification_accuracy_output.json\n",
      "2025-10-25 16:16:24 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_all_params_selection_output/llm_latency_output.json\n",
      "2025-10-25 16:16:24 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_all_params_selection_output/token_efficiency_output.json\n",
      "2025-10-25 16:16:24 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_all_params_selection_output/rag_accuracy_output.json\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!nat eval --config_file ./tmp_workflow/configs/alert_triage_config_all_params_selection.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa7489c",
   "metadata": {},
   "source": [
    "Then let's analyze the results of the untuned agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d1b58195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy Results:\n",
      "Average Score: 57.00%\n",
      "\n",
      "Per-Alert Results:\n",
      "  Alert 0: Score=1.00 - The prediction false_positive is correct. (label: false_positive)\n",
      "  Alert 1: Score=1.00 - The prediction hardware is correct. (label: hardware)\n",
      "  Alert 2: Score=0.00 - The prediction hardware is incorrect. (label: software)\n",
      "  Alert 3: Score=0.00 - The prediction ## alert summary is incorrect. (label: maintenance)\n",
      "  Alert 4: Score=0.00 - The prediction repetitive_behavior is incorrect. (label: software)\n",
      "  Alert 5: Score=1.00 - The prediction false_positive is correct. (label: false_positive)\n",
      "  Alert 6: Score=1.00 - The prediction repetitive_behavior is correct. (label: repetitive_behavior)\n",
      "\n",
      "\n",
      "RAG Accuracy Results:\n",
      "Average Score: 50.00%\n",
      "Total Alerts Evaluated: 7\n"
     ]
    }
   ],
   "source": [
    "# Load and display classification accuracy results\n",
    "# path-check-skip-next-line\n",
    "with open('./tmp_workflow/alert_triage_all_params_selection_output/classification_accuracy_output.json') as f:\n",
    "    classification_results = json.load(f)\n",
    "\n",
    "print(\"Classification Accuracy Results:\")\n",
    "print(f\"Average Score: {classification_results['average_score']:.2%}\")\n",
    "print(\"\\nPer-Alert Results:\")\n",
    "for item in classification_results['eval_output_items']:\n",
    "    print(f\"  Alert {item['id']}: Score={item['score']:.2f} - {item['reasoning']}\")\n",
    "\n",
    "# Load and display RAG accuracy results\n",
    "# path-check-skip-next-line\n",
    "with open('./tmp_workflow/alert_triage_all_params_selection_output/rag_accuracy_output.json') as f:\n",
    "    rag_results = json.load(f)\n",
    "\n",
    "print(\"\\n\\nRAG Accuracy Results:\")\n",
    "print(f\"Average Score: {rag_results['average_score']:.2%}\")\n",
    "print(f\"Total Alerts Evaluated: {len(rag_results['eval_output_items'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5813af73",
   "metadata": {},
   "source": [
    "<a id=\"all-tuning-optimize\"></a>\n",
    "## 3.3) Optimize the agent\n",
    "\n",
    "Now let's re-run the optmize, but this time we will have model, parameter, and prompt tuning all enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "754bd302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-26 08:21:38 - WARNING  - nat.experimental.decorators.experimental_warning_decorator:59 - The Optimizer feature is experimental and the API may change in future releases. Future versions may introduce breaking changes without notice. Function: nat.profiler.parameter_optimization.optimizer_runtime.optimize_config\n",
      "2025-10-26 08:22:10 - WARNING  - nat.experimental.decorators.experimental_warning_decorator:59 - The Optimizer feature is experimental and the API may change in future releases. Future versions may introduce breaking changes without notice. Function: nat.profiler.parameter_optimization.parameter_optimizer.optimize_parameters\n",
      "2025-10-26 08:22:10 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:70 - Using Grid sampler for numeric optimization\n",
      "\u001b[32m[I 2025-10-26 08:22:10,226]\u001b[0m A new study created in memory with name: no-name-5ebb3abf-fc67-482e-8cc6-d280df343490\u001b[0m\n",
      "2025-10-26 08:22:10 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:125 - Starting numeric / enum parameter optimization...\n",
      "2025-10-26 08:22:16 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={'hardware_check': HardwareCheckToolConfig(description='This tool checks hardware health status using IPMI monitoring to detect power state, hardware degradation, and anomalies that could explain alerts. Args: host_id: str', llm_name='tool_reasoning_llm', prompt=\"You are analyzing IPMI metrics to support host monitoring and alert triage. Use the provided IPMI output to assess overall system status. Your goals are to:\\n\\n1. Determine the system's current power state.\\n2. Identify any signs of hardware degradation or failure.\\n3. Flag any anomalies that could explain why a monitoring alert was triggered.\\n\\nReview the data carefully and summarize your assessment in a clear and structured format.\\n\\nIPMI Output:\\n{input_data}\\n\\nFormat your response as follows:\\n\\nPower Status: ON / OFF\\nHardware Health: Normal / Issues Detected\\nObserved Anomalies: [List any irregularities or warning signs]\\nPossible Cause of Alert: [e.g., hardware issue, thermal spike, power fluctuation, no clear issue]\\nNext Steps: [Recommended actions or checks for further triage]\", offline_mode=True), 'host_performance_check': HostPerformanceCheckToolConfig(description='This tool retrieves CPU usage, memory usage, and hardware I/O usage details for a given host. Args: host_id: str', llm_name='tool_reasoning_llm', parsing_prompt='You are given system performance data captured from a host. Your task is to extract and organize the information into a clean, structured JSON format. The input contains system details and performance metrics, such as CPU, memory, and disk I/O.\\n\\nFollow these instructions:\\n\\n1. Identify metric categories dynamically based on the line prefixes or column headers (e.g., \"Mem:\", \"Swap:\", \"CPU:\", \"Device:\").\\n2. For each category, extract the numerical values and map them to meaningful field names.\\n3. Group related fields under sections such as \"memory_usage\", \"swap_usage\", \"cpu_usage\", \"disk_io\", etc.\\n4. Use consistent, readable key names for all fields.\\n5. Return **only** the final JSON object — no explanations or extra text.\\n\\nHere is the input data:\\n{input_data}', analysis_prompt='You are analyzing system metrics to assess CPU and memory usage. Use the output below to determine whether CPU or memory usage is abnormally high, identify which processes are consuming the most resources, and assess whether the usage patterns could explain a recent alert.\\n\\nInstructions:\\n1. Evaluate overall CPU and memory usage levels.\\n2. List the top resource-consuming processes, including their name, PID, %CPU, and %MEM.\\n3. Identify any potential causes of high usage (e.g., memory leak, runaway process, legitimate high load).\\n4. Recommend possible next steps for investigation or mitigation.\\n\\nFormat your response as a structured summary:\\n\\nCPU Usage: Normal / High (X% usage)\\nMemory Usage: Normal / High (X% usage)\\nTop Resource-Consuming Processes: [Process name, PID, %CPU, %MEM]\\nPotential Cause of High Usage: [e.g., runaway process, heavy load, memory leak]\\nNext Steps: [Suggested mitigation actions]\\n\\nSystem Metrics Output:\\n{input_data}\\n', offline_mode=True), 'monitoring_process_check': MonitoringProcessCheckToolConfig(description='This tool checks the status of critical monitoring processes and services on a target host by executing system commands. Args: host_id: str', llm_name='tool_reasoning_llm', prompt='You are checking whether the telegraf service is running on the server. Use the monitoring output below to verify its status. If it’s not running, identify possible reasons and assess the impact.\\n\\nInstructions:\\n1. Check if the telegraf process is present and active.\\n2. Evaluate the potential impact of telegraf not running on system availability or monitoring.\\n3. Identify likely causes for the process not running.\\n\\nFormat your response as a structured summary:\\n* **Telegraf Running:** Yes / No\\n* **Potential Impact:** [e.g., host seems down to the monitoring system, delayed alerting]\\n* **Possible Cause:** [e.g., process crash, misconfiguration, resource constraints]\\n* **Next Steps:** [e.g., restart telegraf, check logs]\\n\\nMonitoring Output:\\n{input_data}', offline_mode=True), 'network_connectivity_check': NetworkConnectivityCheckToolConfig(description='This tool checks network connectivity of a host by running ping and socket connection tests. Args: host_id: str', llm_name='tool_reasoning_llm', prompt='You are assisting with alert triage by checking the network connectivity status of a host. Use the outputs from `ping` and `telnet` commands to determine whether the host is reachable. If connectivity issues are detected, analyze the possible root causes and provide a structured summary of your findings.\\n\\nInstructions:\\n1. Interpret the `ping` and `telnet` results to assess host reachability.\\n2. Determine whether there is a connectivity issue.\\n3. Identify potential causes, such as network failure, firewall restrictions, or service unavailability.\\n4. Recommend appropriate next steps for troubleshooting or escalation.\\n\\nFormat your response as a structured summary:\\n\\nPing Status: Successful / Failed\\nTelnet Status: Connected / Failed\\nPotential Cause of Connectivity Issue: [e.g., network failure, firewall rules, service outage, no issue]\\nNext Steps: [e.g., check network logs, restart network services, escalate issue, or no action needed]\\n\\nPing Output:\\n{ping_data}\\n\\nTelnet Output:\\n{telnet_data}', offline_mode=True), 'telemetry_metrics_host_heartbeat_check': TelemetryMetricsHostHeartbeatCheckToolConfig(description=\"This tool checks if a host's telemetry monitoring service is reporting heartbeat metrics. This tells us if the host is up and running. Args: host_id: str\", llm_name='tool_reasoning_llm', prompt=\"The following is the telemetry metrics fetched for the host to see if it's been up and running (if result is empty, then the monitoring service on the host is down):\\n{data}\\nBased on the data, summarize the fetched data and provide a conclusion of the host's running status.\", offline_mode=True, metrics_url=''), 'telemetry_metrics_host_performance_check': TelemetryMetricsHostPerformanceCheckToolConfig(description='This tool checks the performance of the host by analyzing the CPU usage timeseries. Args: host_id: str', llm_name='tool_reasoning_llm', prompt=\"You are an expert on analyzing CPU usage timeseries. Periodic usage peaks are expected benign system behavior.\\nUser will provide data in the format of a list of lists, where each sublist contains two elements: timestamp and CPU usage percentage. User will also provide statistics on the timeseries. Write a markdown report about what was observed in the timeseries.\\n\\nExample format:\\n# CPU Usage Analysis Report\\nThe data analysis is performed on 14 days of CPU usage percentage data.\\n\\n## Data Statistics\\ndata start and end time, data point interval, CPU usage statistics\\n\\n## Observations\\nany patterns observed? Should be one of the below cases:\\n- Are there any cyclic usage surges?\\n  - What is the cycle?\\n  - What is the high and low CPU usage of the pattern?\\n- Is there one anomalous peak?\\n  - When did it happen?\\n  - What is it like before and after?\\n- No obvious pattern? A mix of patterns? => it's normal flutuation of the system (max usage less than 60%)\\n  - What is the fluctuation range?\\n\\n## Conclusion\\nSummarize the observation.\\nCategories:\\n- peak in the data means the high CPU usage is an anomaly and requires attention\\n- periodic behvior means the high usage is benign\\n- overall moderate (max usage less than 60%) usage means no issue in the system\\n\\n## Pattern Label\\nAnomalous Peak/Periodic Surges/Normal Fluctuations\\n\", offline_mode=True, metrics_url=''), 'telemetry_metrics_analysis_agent': TelemetryMetricsAnalysisAgentConfig(description='This is a telemetry metrics tool used to monitor remotely collected telemetry data. It checks server heartbeat data to determine whether the server is up and running and analyzes CPU usage patterns over the past 14 days to identify potential CPU issues. Args: host_id: str, alert_type: str', tool_names=['telemetry_metrics_host_heartbeat_check', 'telemetry_metrics_host_performance_check'], llm_name='agent_llm', prompt=\"You arg a helpful alert triage assistant. Your task is to investigate an alert that was just triggered on a specific host. You will be given two inputs:\\n- `host_id`: the identifier of the host where the alert occurred.\\n- `alert_type`: the type of alert that triggered.\\n\\nUse the tools provided below to collect relevant telemetry data for the specified host:\\n\\nTools:\\n- `telemetry_metrics_host_heartbeat_check`: Use this to check the server's heartbeat and determine if the host is currently up and responsive.\\n- `telemetry_metrics_host_performance_check`: Use this to analyze CPU usage trends over the past 14 days and identify abnormal patterns.\\n\\nInstructions:\\n1. Run the appropriate tools based on the host and alert type.\\n2. Collect and include all relevant output from the tools in your response.\\n3. Analyze the data and provide reasoning to help determine whether the telemetry supports or explains the triggered alert.\\n\\nYour response should include:\\n- Raw data from each tool\\n- A concise summary of findings\\n- Any insights or hypotheses that explain the alert\"), 'maintenance_check': MaintenanceCheckToolConfig(description='Check if a host is under maintenance during the time of an alert to help determine if the alert can be deprioritized.', llm_name='agent_llm', prompt='User will provide you with a system alert represented in JSON format. You know for a fact that there is maintenance happening for the host. Maintenance start time for this host is : [{maintenance_start_str}]; end time is: [{maintenance_end_str}] (end time empty means that there is not yet a set end time for the maintenance on the host)\\nGenerate a markdown report in the following format:\\n\\n## Alert Summary\\n(summary of what happened in the alert JSON data)\\n\\n## Collected Metrics\\n(lay out the maintenance information)\\n\\n## Analysis\\n(Describe the maintenance status of this host)\\n\\n## Recommended Actions\\n(Bullet point list: write how the user may not need to worry about this alert given that the host is under maintenance, and they could check if the issue persists afterward)\\n\\n## Alert Status\\n(can deprioritize the investigation of the alert, host under maintenance)', static_data_path='/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/maintenance_static_dataset.csv', skip_maintenance_check=False), 'categorizer': CategorizerToolConfig(description='This is a categorization tool used at the end of the pipeline.', llm_name='agent_llm', prompt='You will be given a system-generated alert triage report. Your job is to read the report carefully and determine the most likely root cause of the issue. Then, categorize the root cause into one of the following predefined categories:\\n\\n**Valid Categories**\\n- `software`: The alert was triggered due to a malfunctioning or inactive monitoring service (e.g., Telegraf not running).\\n- `network_connectivity`: The host is not reachable via ping or curl, or there are signs of connection issues due to blocked ports, broken services, or firewall rules (e.g., telnet fails).\\n- `hardware`: The alert is caused by a hardware failure or degradation.\\n- `repetitive_behavior`: The alert is triggered by a recurring or periodic behavior pattern (e.g., regular CPU spikes or memory surges).\\n- `false_positive`: No clear signs of failure or degradation; system appears healthy and no suspicious pattern is found.\\n- `need_investigation`: The report contains conflicting, ambiguous, or insufficient information to determine a clear root cause.\\n\\n**Response Format**\\n- Line 1: Output only the category name (e.g., `hardware`)\\n- Line 2: Briefly explain your reasoning based on the contents of the report.\\n- Example response:\\nnetwork_connectivity\\nPing and curl to the host both failed, and telnet to the monitored port timed out, indicating a likely connectivity or firewall issue.\\n\\n**Important Guidelines**\\n- Base your categorization only on evidence presented in the report.\\n- If no category clearly fits, default to `need_investigation`.')} function_groups={} llms={'agent_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=2048), 'tool_reasoning_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.2, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=2048), 'nim_rag_eval_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/alert_triage_all_params_selection_output/optimizer'), eval_metrics={'classification_accuracy': OptimizerMetric(evaluator_name='classification_accuracy', direction='maximize', weight=1.0), 'llm_latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=1, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=True, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=AlertTriageAgentWorkflowConfig(tool_names=['hardware_check', 'host_performance_check', 'monitoring_process_check', 'network_connectivity_check', 'telemetry_metrics_analysis_agent'], llm_name='agent_llm', offline_mode=True, offline_data_path='/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/offline_data.csv', benign_fallback_data_path='/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/benign_fallback_offline_data.json', agent_prompt='**Role**\\nYou are a Triage Agent responsible for diagnosing and troubleshooting system alerts in real time. Your goal is to determine whether an alert indicates a true issue, identify the root cause, and provide a clear, structured triage report to assist system analysts.\\n\\n\\n**Instructions**\\n\\n1. **Analyze the Alert**\\n   Begin by interpreting the incoming alert. Identify its type (e.g., *InstanceDown*, *HighCPUUsage*) and note any relevant details.\\n\\n2. **Select and Use Diagnostic Tools**\\n   Based on the alert type, choose the most relevant tools to gather system metrics. Use each tool only once per alert.\\n\\n   - `hardware_check`: Retrieves server power status and hardware health via IPMI. Useful for diagnosing instance down alerts or suspected hardware failures.\\n   - `host_performance_check`: Collects system-level CPU and memory usage using commands like `top` and `ps`. Use this to identify host\\'s resource (CPR and memory) usage bottlenecks.\\n   - `monitoring_process_check`: Checks whether critical processes are running on the host. Useful for verifying system functionality during instance down or degraded performance.\\n   - `network_connectivity_check`: Tests host connectivity through ping, telnet, and HTTP health checks. Helps determine if the server is reachable from the network.\\n   - `telemetry_metrics_analysis_agent`: Pulls telemetry metrics to check host status and analyze usage trends. Effective for validating instance uptime and system load over time.\\n\\n   Once you\\'ve received outputs from all selected tools, **pause to analyze them before proceeding further**.\\n\\n3. **Correlate Data and Determine Root Cause**\\n   - Evaluate the retrieved metrics against the alert details.\\n   - Determine if the alert reflects a real problem or is a false positive.\\n   - If an issue is detected, identify likely causes—such as hardware failure, performance bottlenecks, or network issues.\\n\\n4. **Generate a Structured Triage Report (in Markdown format)**\\n   Organize your findings clearly under these sections:\\n\\n   - **Alert Summary**: Brief description of the alert received.\\n   - **Collected Metrics**: Outputs from the diagnostic tools used.\\n   - **Analysis**: Interpretation of the data and how it relates to the alert.\\n   - **Recommended Actions**: Suggested next steps to mitigate or resolve the issue.\\n   - **Alert Status**: Choose one — \"Valid\", \"Abnormal but benign\", or \"False alarm\".\\n\\n\\n**Important Rules**\\n- Do not call the same tool more than once per alert.\\n- Analyze tool outputs before taking any additional action.\\n- Stay concise, structured, and actionable.') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/alert_triage_all_params_selection_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/offline_data.json'), profiler=None), evaluators={'classification_accuracy': ClassificationEvaluatorConfig(), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8), 'rag_accuracy': RagasEvaluatorConfig(llm_name='nim_rag_eval_llm', metric='AnswerAccuracy', input_obj_field=None)})\n",
      "2025-10-26 08:22:16 - INFO     - nat_alert_triage_agent:104 - Preloaded test data from: /Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/offline_data.csv\n",
      "2025-10-26 08:22:16 - INFO     - nat_alert_triage_agent:108 - Preloaded benign fallback data from: /Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/benign_fallback_offline_data.json\n",
      "2025-10-26 08:22:16 - INFO     - nat_alert_triage_agent:80 - ================================================Running in offline mode=================================================\n",
      "Running workflow:   0%|                                   | 0/7 [00:00<?, ?it/s]2025-10-26 08:22:18 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-0.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-26 08:22:18 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-1.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-26 08:22:18 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-2.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-26 08:22:18 - INFO     - nat_alert_triage_agent:258 - Host: [test-instance-3.example.com] is under maintenance according to the maintenance database\n",
      "2025-10-26 08:22:30 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-4.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-26 08:22:30 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-5.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-26 08:22:30 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-6.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-26 08:22:30 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  14%|███▊                       | 1/7 [00:12<01:13, 12.19s/it]2025-10-26 08:23:06 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  29%|███████▋                   | 2/7 [00:47<02:09, 25.91s/it]2025-10-26 08:23:18 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  43%|███████████▌               | 3/7 [01:00<01:19, 19.76s/it]2025-10-26 08:23:44 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  57%|███████████████▍           | 4/7 [01:26<01:06, 22.29s/it]2025-10-26 08:24:05 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  71%|███████████████████▎       | 5/7 [01:47<00:43, 21.93s/it]2025-10-26 08:25:29 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  86%|███████████████████████▏   | 6/7 [03:10<00:42, 42.81s/it]2025-10-26 08:27:31 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow: 100%|███████████████████████████| 7/7 [05:12<00:00, 44.69s/it]\n",
      "Evaluating classification accuracy:   0%|                 | 0/7 [00:00<?, ?it/s]\n",
      "Evaluating Avg LLM Latency:   0%|                         | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Evaluating Avg Tokens/LLM_END:   0%|                      | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating classification accuracy:  14%|█▎       | 1/7 [00:00<00:02,  2.25it/s]\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg LLM Latency:  14%|██▍              | 1/7 [00:00<00:02,  2.25it/s]\u001b[A\n",
      "\n",
      "Evaluating classification accuracy: 100%|█████████| 7/7 [00:00<00:00, 15.35it/s]\u001b[A\u001b[A\n",
      "Evaluating Avg LLM Latency: 100%|█████████████████| 7/7 [00:00<00:00, 15.35it/s]\n",
      "Evaluating Avg Tokens/LLM_END: 100%|██████████████| 7/7 [00:00<00:00, 15.35it/s]\n",
      "2025-10-26 08:27:32 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  14%|██▏            | 1/7 [00:01<00:06,  1.09s/it]\u001b[A\u001b[A\u001b[A2025-10-26 08:27:32 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  29%|████▎          | 2/7 [00:01<00:02,  1.71it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  71%|██████████▋    | 5/7 [00:01<00:00,  5.21it/s]\u001b[A\u001b[A\u001b[A2025-10-26 08:27:32 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy: 100%|███████████████| 7/7 [00:02<00:00,  2.40it/s]\u001b[A\u001b[A\u001b[A\n",
      "2025-10-26 08:27:34 - INFO     - nat_alert_triage_agent:164 - Cleaning up\n",
      "2025-10-26 08:27:34 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-26 08:27:34 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/alert_triage_all_params_selection_output/workflow_output.json\n",
      "2025-10-26 08:27:34 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_all_params_selection_output/classification_accuracy_output.json\n",
      "2025-10-26 08:27:34 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_all_params_selection_output/llm_latency_output.json\n",
      "2025-10-26 08:27:34 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_all_params_selection_output/token_efficiency_output.json\n",
      "2025-10-26 08:27:34 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_all_params_selection_output/rag_accuracy_output.json\n",
      "\u001b[32m[I 2025-10-26 08:27:34,116]\u001b[0m Trial 0 finished with values: [0.29, 27.89] and parameters: {'llms.agent_llm.temperature': 0.0, 'llms.agent_llm.model_name': 'meta/llama-3.1-70b-instruct'}.\u001b[0m\n",
      "2025-10-26 08:27:34 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={'hardware_check': HardwareCheckToolConfig(description='This tool checks hardware health status using IPMI monitoring to detect power state, hardware degradation, and anomalies that could explain alerts. Args: host_id: str', llm_name='tool_reasoning_llm', prompt=\"You are analyzing IPMI metrics to support host monitoring and alert triage. Use the provided IPMI output to assess overall system status. Your goals are to:\\n\\n1. Determine the system's current power state.\\n2. Identify any signs of hardware degradation or failure.\\n3. Flag any anomalies that could explain why a monitoring alert was triggered.\\n\\nReview the data carefully and summarize your assessment in a clear and structured format.\\n\\nIPMI Output:\\n{input_data}\\n\\nFormat your response as follows:\\n\\nPower Status: ON / OFF\\nHardware Health: Normal / Issues Detected\\nObserved Anomalies: [List any irregularities or warning signs]\\nPossible Cause of Alert: [e.g., hardware issue, thermal spike, power fluctuation, no clear issue]\\nNext Steps: [Recommended actions or checks for further triage]\", offline_mode=True), 'host_performance_check': HostPerformanceCheckToolConfig(description='This tool retrieves CPU usage, memory usage, and hardware I/O usage details for a given host. Args: host_id: str', llm_name='tool_reasoning_llm', parsing_prompt='You are given system performance data captured from a host. Your task is to extract and organize the information into a clean, structured JSON format. The input contains system details and performance metrics, such as CPU, memory, and disk I/O.\\n\\nFollow these instructions:\\n\\n1. Identify metric categories dynamically based on the line prefixes or column headers (e.g., \"Mem:\", \"Swap:\", \"CPU:\", \"Device:\").\\n2. For each category, extract the numerical values and map them to meaningful field names.\\n3. Group related fields under sections such as \"memory_usage\", \"swap_usage\", \"cpu_usage\", \"disk_io\", etc.\\n4. Use consistent, readable key names for all fields.\\n5. Return **only** the final JSON object — no explanations or extra text.\\n\\nHere is the input data:\\n{input_data}', analysis_prompt='You are analyzing system metrics to assess CPU and memory usage. Use the output below to determine whether CPU or memory usage is abnormally high, identify which processes are consuming the most resources, and assess whether the usage patterns could explain a recent alert.\\n\\nInstructions:\\n1. Evaluate overall CPU and memory usage levels.\\n2. List the top resource-consuming processes, including their name, PID, %CPU, and %MEM.\\n3. Identify any potential causes of high usage (e.g., memory leak, runaway process, legitimate high load).\\n4. Recommend possible next steps for investigation or mitigation.\\n\\nFormat your response as a structured summary:\\n\\nCPU Usage: Normal / High (X% usage)\\nMemory Usage: Normal / High (X% usage)\\nTop Resource-Consuming Processes: [Process name, PID, %CPU, %MEM]\\nPotential Cause of High Usage: [e.g., runaway process, heavy load, memory leak]\\nNext Steps: [Suggested mitigation actions]\\n\\nSystem Metrics Output:\\n{input_data}\\n', offline_mode=True), 'monitoring_process_check': MonitoringProcessCheckToolConfig(description='This tool checks the status of critical monitoring processes and services on a target host by executing system commands. Args: host_id: str', llm_name='tool_reasoning_llm', prompt='You are checking whether the telegraf service is running on the server. Use the monitoring output below to verify its status. If it’s not running, identify possible reasons and assess the impact.\\n\\nInstructions:\\n1. Check if the telegraf process is present and active.\\n2. Evaluate the potential impact of telegraf not running on system availability or monitoring.\\n3. Identify likely causes for the process not running.\\n\\nFormat your response as a structured summary:\\n* **Telegraf Running:** Yes / No\\n* **Potential Impact:** [e.g., host seems down to the monitoring system, delayed alerting]\\n* **Possible Cause:** [e.g., process crash, misconfiguration, resource constraints]\\n* **Next Steps:** [e.g., restart telegraf, check logs]\\n\\nMonitoring Output:\\n{input_data}', offline_mode=True), 'network_connectivity_check': NetworkConnectivityCheckToolConfig(description='This tool checks network connectivity of a host by running ping and socket connection tests. Args: host_id: str', llm_name='tool_reasoning_llm', prompt='You are assisting with alert triage by checking the network connectivity status of a host. Use the outputs from `ping` and `telnet` commands to determine whether the host is reachable. If connectivity issues are detected, analyze the possible root causes and provide a structured summary of your findings.\\n\\nInstructions:\\n1. Interpret the `ping` and `telnet` results to assess host reachability.\\n2. Determine whether there is a connectivity issue.\\n3. Identify potential causes, such as network failure, firewall restrictions, or service unavailability.\\n4. Recommend appropriate next steps for troubleshooting or escalation.\\n\\nFormat your response as a structured summary:\\n\\nPing Status: Successful / Failed\\nTelnet Status: Connected / Failed\\nPotential Cause of Connectivity Issue: [e.g., network failure, firewall rules, service outage, no issue]\\nNext Steps: [e.g., check network logs, restart network services, escalate issue, or no action needed]\\n\\nPing Output:\\n{ping_data}\\n\\nTelnet Output:\\n{telnet_data}', offline_mode=True), 'telemetry_metrics_host_heartbeat_check': TelemetryMetricsHostHeartbeatCheckToolConfig(description=\"This tool checks if a host's telemetry monitoring service is reporting heartbeat metrics. This tells us if the host is up and running. Args: host_id: str\", llm_name='tool_reasoning_llm', prompt=\"The following is the telemetry metrics fetched for the host to see if it's been up and running (if result is empty, then the monitoring service on the host is down):\\n{data}\\nBased on the data, summarize the fetched data and provide a conclusion of the host's running status.\", offline_mode=True, metrics_url=''), 'telemetry_metrics_host_performance_check': TelemetryMetricsHostPerformanceCheckToolConfig(description='This tool checks the performance of the host by analyzing the CPU usage timeseries. Args: host_id: str', llm_name='tool_reasoning_llm', prompt=\"You are an expert on analyzing CPU usage timeseries. Periodic usage peaks are expected benign system behavior.\\nUser will provide data in the format of a list of lists, where each sublist contains two elements: timestamp and CPU usage percentage. User will also provide statistics on the timeseries. Write a markdown report about what was observed in the timeseries.\\n\\nExample format:\\n# CPU Usage Analysis Report\\nThe data analysis is performed on 14 days of CPU usage percentage data.\\n\\n## Data Statistics\\ndata start and end time, data point interval, CPU usage statistics\\n\\n## Observations\\nany patterns observed? Should be one of the below cases:\\n- Are there any cyclic usage surges?\\n  - What is the cycle?\\n  - What is the high and low CPU usage of the pattern?\\n- Is there one anomalous peak?\\n  - When did it happen?\\n  - What is it like before and after?\\n- No obvious pattern? A mix of patterns? => it's normal flutuation of the system (max usage less than 60%)\\n  - What is the fluctuation range?\\n\\n## Conclusion\\nSummarize the observation.\\nCategories:\\n- peak in the data means the high CPU usage is an anomaly and requires attention\\n- periodic behvior means the high usage is benign\\n- overall moderate (max usage less than 60%) usage means no issue in the system\\n\\n## Pattern Label\\nAnomalous Peak/Periodic Surges/Normal Fluctuations\\n\", offline_mode=True, metrics_url=''), 'telemetry_metrics_analysis_agent': TelemetryMetricsAnalysisAgentConfig(description='This is a telemetry metrics tool used to monitor remotely collected telemetry data. It checks server heartbeat data to determine whether the server is up and running and analyzes CPU usage patterns over the past 14 days to identify potential CPU issues. Args: host_id: str, alert_type: str', tool_names=['telemetry_metrics_host_heartbeat_check', 'telemetry_metrics_host_performance_check'], llm_name='agent_llm', prompt=\"You arg a helpful alert triage assistant. Your task is to investigate an alert that was just triggered on a specific host. You will be given two inputs:\\n- `host_id`: the identifier of the host where the alert occurred.\\n- `alert_type`: the type of alert that triggered.\\n\\nUse the tools provided below to collect relevant telemetry data for the specified host:\\n\\nTools:\\n- `telemetry_metrics_host_heartbeat_check`: Use this to check the server's heartbeat and determine if the host is currently up and responsive.\\n- `telemetry_metrics_host_performance_check`: Use this to analyze CPU usage trends over the past 14 days and identify abnormal patterns.\\n\\nInstructions:\\n1. Run the appropriate tools based on the host and alert type.\\n2. Collect and include all relevant output from the tools in your response.\\n3. Analyze the data and provide reasoning to help determine whether the telemetry supports or explains the triggered alert.\\n\\nYour response should include:\\n- Raw data from each tool\\n- A concise summary of findings\\n- Any insights or hypotheses that explain the alert\"), 'maintenance_check': MaintenanceCheckToolConfig(description='Check if a host is under maintenance during the time of an alert to help determine if the alert can be deprioritized.', llm_name='agent_llm', prompt='User will provide you with a system alert represented in JSON format. You know for a fact that there is maintenance happening for the host. Maintenance start time for this host is : [{maintenance_start_str}]; end time is: [{maintenance_end_str}] (end time empty means that there is not yet a set end time for the maintenance on the host)\\nGenerate a markdown report in the following format:\\n\\n## Alert Summary\\n(summary of what happened in the alert JSON data)\\n\\n## Collected Metrics\\n(lay out the maintenance information)\\n\\n## Analysis\\n(Describe the maintenance status of this host)\\n\\n## Recommended Actions\\n(Bullet point list: write how the user may not need to worry about this alert given that the host is under maintenance, and they could check if the issue persists afterward)\\n\\n## Alert Status\\n(can deprioritize the investigation of the alert, host under maintenance)', static_data_path='/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/maintenance_static_dataset.csv', skip_maintenance_check=False), 'categorizer': CategorizerToolConfig(description='This is a categorization tool used at the end of the pipeline.', llm_name='agent_llm', prompt='You will be given a system-generated alert triage report. Your job is to read the report carefully and determine the most likely root cause of the issue. Then, categorize the root cause into one of the following predefined categories:\\n\\n**Valid Categories**\\n- `software`: The alert was triggered due to a malfunctioning or inactive monitoring service (e.g., Telegraf not running).\\n- `network_connectivity`: The host is not reachable via ping or curl, or there are signs of connection issues due to blocked ports, broken services, or firewall rules (e.g., telnet fails).\\n- `hardware`: The alert is caused by a hardware failure or degradation.\\n- `repetitive_behavior`: The alert is triggered by a recurring or periodic behavior pattern (e.g., regular CPU spikes or memory surges).\\n- `false_positive`: No clear signs of failure or degradation; system appears healthy and no suspicious pattern is found.\\n- `need_investigation`: The report contains conflicting, ambiguous, or insufficient information to determine a clear root cause.\\n\\n**Response Format**\\n- Line 1: Output only the category name (e.g., `hardware`)\\n- Line 2: Briefly explain your reasoning based on the contents of the report.\\n- Example response:\\nnetwork_connectivity\\nPing and curl to the host both failed, and telnet to the monitored port timed out, indicating a likely connectivity or firewall issue.\\n\\n**Important Guidelines**\\n- Base your categorization only on evidence presented in the report.\\n- If no category clearly fits, default to `need_investigation`.')} function_groups={} llms={'agent_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.5, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=2048), 'tool_reasoning_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.2, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=2048), 'nim_rag_eval_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/alert_triage_all_params_selection_output/optimizer'), eval_metrics={'classification_accuracy': OptimizerMetric(evaluator_name='classification_accuracy', direction='maximize', weight=1.0), 'llm_latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=1, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=True, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=AlertTriageAgentWorkflowConfig(tool_names=['hardware_check', 'host_performance_check', 'monitoring_process_check', 'network_connectivity_check', 'telemetry_metrics_analysis_agent'], llm_name='agent_llm', offline_mode=True, offline_data_path='/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/offline_data.csv', benign_fallback_data_path='/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/benign_fallback_offline_data.json', agent_prompt='**Role**\\nYou are a Triage Agent responsible for diagnosing and troubleshooting system alerts in real time. Your goal is to determine whether an alert indicates a true issue, identify the root cause, and provide a clear, structured triage report to assist system analysts.\\n\\n\\n**Instructions**\\n\\n1. **Analyze the Alert**\\n   Begin by interpreting the incoming alert. Identify its type (e.g., *InstanceDown*, *HighCPUUsage*) and note any relevant details.\\n\\n2. **Select and Use Diagnostic Tools**\\n   Based on the alert type, choose the most relevant tools to gather system metrics. Use each tool only once per alert.\\n\\n   - `hardware_check`: Retrieves server power status and hardware health via IPMI. Useful for diagnosing instance down alerts or suspected hardware failures.\\n   - `host_performance_check`: Collects system-level CPU and memory usage using commands like `top` and `ps`. Use this to identify host\\'s resource (CPR and memory) usage bottlenecks.\\n   - `monitoring_process_check`: Checks whether critical processes are running on the host. Useful for verifying system functionality during instance down or degraded performance.\\n   - `network_connectivity_check`: Tests host connectivity through ping, telnet, and HTTP health checks. Helps determine if the server is reachable from the network.\\n   - `telemetry_metrics_analysis_agent`: Pulls telemetry metrics to check host status and analyze usage trends. Effective for validating instance uptime and system load over time.\\n\\n   Once you\\'ve received outputs from all selected tools, **pause to analyze them before proceeding further**.\\n\\n3. **Correlate Data and Determine Root Cause**\\n   - Evaluate the retrieved metrics against the alert details.\\n   - Determine if the alert reflects a real problem or is a false positive.\\n   - If an issue is detected, identify likely causes—such as hardware failure, performance bottlenecks, or network issues.\\n\\n4. **Generate a Structured Triage Report (in Markdown format)**\\n   Organize your findings clearly under these sections:\\n\\n   - **Alert Summary**: Brief description of the alert received.\\n   - **Collected Metrics**: Outputs from the diagnostic tools used.\\n   - **Analysis**: Interpretation of the data and how it relates to the alert.\\n   - **Recommended Actions**: Suggested next steps to mitigate or resolve the issue.\\n   - **Alert Status**: Choose one — \"Valid\", \"Abnormal but benign\", or \"False alarm\".\\n\\n\\n**Important Rules**\\n- Do not call the same tool more than once per alert.\\n- Analyze tool outputs before taking any additional action.\\n- Stay concise, structured, and actionable.') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/alert_triage_all_params_selection_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/offline_data.json'), profiler=None), evaluators={'classification_accuracy': ClassificationEvaluatorConfig(), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8), 'rag_accuracy': RagasEvaluatorConfig(llm_name='nim_rag_eval_llm', metric='AnswerAccuracy', input_obj_field=None)})\n",
      "2025-10-26 08:27:34 - INFO     - nat_alert_triage_agent:104 - Preloaded test data from: /Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/offline_data.csv\n",
      "2025-10-26 08:27:34 - INFO     - nat_alert_triage_agent:108 - Preloaded benign fallback data from: /Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/benign_fallback_offline_data.json\n",
      "2025-10-26 08:27:34 - INFO     - nat_alert_triage_agent:80 - ================================================Running in offline mode=================================================\n",
      "Running workflow:   0%|                                   | 0/7 [00:00<?, ?it/s]2025-10-26 08:27:34 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-0.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-26 08:27:34 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-1.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-26 08:27:34 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-2.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-26 08:27:34 - INFO     - nat_alert_triage_agent:258 - Host: [test-instance-3.example.com] is under maintenance according to the maintenance database\n",
      "2025-10-26 08:28:11 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-4.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-26 08:28:11 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-5.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-26 08:28:11 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-6.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-26 08:28:11 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  14%|███▊                       | 1/7 [00:36<03:40, 36.74s/it]2025-10-26 08:28:23 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  29%|███████▋                   | 2/7 [00:48<01:51, 22.21s/it]2025-10-26 08:28:40 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  43%|███████████▌               | 3/7 [01:05<01:19, 19.87s/it]2025-10-26 08:28:54 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  57%|███████████████▍           | 4/7 [01:19<00:52, 17.58s/it]2025-10-26 08:28:56 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  71%|███████████████████▎       | 5/7 [01:22<00:24, 12.14s/it]2025-10-26 08:29:07 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  86%|███████████████████████▏   | 6/7 [01:33<00:11, 11.76s/it]2025-10-26 08:30:15 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow: 100%|███████████████████████████| 7/7 [02:41<00:00, 23.06s/it]\n",
      "Evaluating classification accuracy:   0%|                 | 0/7 [00:00<?, ?it/s]\n",
      "Evaluating Avg LLM Latency:   0%|                         | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Evaluating Avg Tokens/LLM_END:   0%|                      | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating classification accuracy:  14%|█▎       | 1/7 [00:00<00:01,  5.19it/s]\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg LLM Latency:  14%|██▍              | 1/7 [00:00<00:01,  5.17it/s]\u001b[A\n",
      "\n",
      "Evaluating classification accuracy: 100%|█████████| 7/7 [00:00<00:00, 34.67it/s]\u001b[A\u001b[A\n",
      "Evaluating Avg LLM Latency: 100%|█████████████████| 7/7 [00:00<00:00, 34.68it/s]\n",
      "Evaluating Avg Tokens/LLM_END: 100%|██████████████| 7/7 [00:00<00:00, 34.69it/s]\n",
      "2025-10-26 08:30:16 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  14%|██▏            | 1/7 [00:00<00:05,  1.14it/s]\u001b[A\u001b[A\u001b[A2025-10-26 08:30:16 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  43%|██████▍        | 3/7 [00:00<00:01,  3.71it/s]\u001b[A\u001b[A\u001b[A2025-10-26 08:30:16 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  71%|██████████▋    | 5/7 [00:01<00:00,  4.04it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy: 100%|███████████████| 7/7 [00:02<00:00,  3.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "2025-10-26 08:30:18 - INFO     - nat_alert_triage_agent:164 - Cleaning up\n",
      "2025-10-26 08:30:18 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-26 08:30:18 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/alert_triage_all_params_selection_output/workflow_output.json\n",
      "2025-10-26 08:30:18 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_all_params_selection_output/classification_accuracy_output.json\n",
      "2025-10-26 08:30:18 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_all_params_selection_output/llm_latency_output.json\n",
      "2025-10-26 08:30:18 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_all_params_selection_output/token_efficiency_output.json\n",
      "2025-10-26 08:30:18 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_all_params_selection_output/rag_accuracy_output.json\n",
      "\u001b[32m[I 2025-10-26 08:30:18,119]\u001b[0m Trial 1 finished with values: [0.29, 16.36] and parameters: {'llms.agent_llm.temperature': 0.5, 'llms.agent_llm.model_name': 'meta/llama-3.1-70b-instruct'}.\u001b[0m\n",
      "2025-10-26 08:30:18 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={'hardware_check': HardwareCheckToolConfig(description='This tool checks hardware health status using IPMI monitoring to detect power state, hardware degradation, and anomalies that could explain alerts. Args: host_id: str', llm_name='tool_reasoning_llm', prompt=\"You are analyzing IPMI metrics to support host monitoring and alert triage. Use the provided IPMI output to assess overall system status. Your goals are to:\\n\\n1. Determine the system's current power state.\\n2. Identify any signs of hardware degradation or failure.\\n3. Flag any anomalies that could explain why a monitoring alert was triggered.\\n\\nReview the data carefully and summarize your assessment in a clear and structured format.\\n\\nIPMI Output:\\n{input_data}\\n\\nFormat your response as follows:\\n\\nPower Status: ON / OFF\\nHardware Health: Normal / Issues Detected\\nObserved Anomalies: [List any irregularities or warning signs]\\nPossible Cause of Alert: [e.g., hardware issue, thermal spike, power fluctuation, no clear issue]\\nNext Steps: [Recommended actions or checks for further triage]\", offline_mode=True), 'host_performance_check': HostPerformanceCheckToolConfig(description='This tool retrieves CPU usage, memory usage, and hardware I/O usage details for a given host. Args: host_id: str', llm_name='tool_reasoning_llm', parsing_prompt='You are given system performance data captured from a host. Your task is to extract and organize the information into a clean, structured JSON format. The input contains system details and performance metrics, such as CPU, memory, and disk I/O.\\n\\nFollow these instructions:\\n\\n1. Identify metric categories dynamically based on the line prefixes or column headers (e.g., \"Mem:\", \"Swap:\", \"CPU:\", \"Device:\").\\n2. For each category, extract the numerical values and map them to meaningful field names.\\n3. Group related fields under sections such as \"memory_usage\", \"swap_usage\", \"cpu_usage\", \"disk_io\", etc.\\n4. Use consistent, readable key names for all fields.\\n5. Return **only** the final JSON object — no explanations or extra text.\\n\\nHere is the input data:\\n{input_data}', analysis_prompt='You are analyzing system metrics to assess CPU and memory usage. Use the output below to determine whether CPU or memory usage is abnormally high, identify which processes are consuming the most resources, and assess whether the usage patterns could explain a recent alert.\\n\\nInstructions:\\n1. Evaluate overall CPU and memory usage levels.\\n2. List the top resource-consuming processes, including their name, PID, %CPU, and %MEM.\\n3. Identify any potential causes of high usage (e.g., memory leak, runaway process, legitimate high load).\\n4. Recommend possible next steps for investigation or mitigation.\\n\\nFormat your response as a structured summary:\\n\\nCPU Usage: Normal / High (X% usage)\\nMemory Usage: Normal / High (X% usage)\\nTop Resource-Consuming Processes: [Process name, PID, %CPU, %MEM]\\nPotential Cause of High Usage: [e.g., runaway process, heavy load, memory leak]\\nNext Steps: [Suggested mitigation actions]\\n\\nSystem Metrics Output:\\n{input_data}\\n', offline_mode=True), 'monitoring_process_check': MonitoringProcessCheckToolConfig(description='This tool checks the status of critical monitoring processes and services on a target host by executing system commands. Args: host_id: str', llm_name='tool_reasoning_llm', prompt='You are checking whether the telegraf service is running on the server. Use the monitoring output below to verify its status. If it’s not running, identify possible reasons and assess the impact.\\n\\nInstructions:\\n1. Check if the telegraf process is present and active.\\n2. Evaluate the potential impact of telegraf not running on system availability or monitoring.\\n3. Identify likely causes for the process not running.\\n\\nFormat your response as a structured summary:\\n* **Telegraf Running:** Yes / No\\n* **Potential Impact:** [e.g., host seems down to the monitoring system, delayed alerting]\\n* **Possible Cause:** [e.g., process crash, misconfiguration, resource constraints]\\n* **Next Steps:** [e.g., restart telegraf, check logs]\\n\\nMonitoring Output:\\n{input_data}', offline_mode=True), 'network_connectivity_check': NetworkConnectivityCheckToolConfig(description='This tool checks network connectivity of a host by running ping and socket connection tests. Args: host_id: str', llm_name='tool_reasoning_llm', prompt='You are assisting with alert triage by checking the network connectivity status of a host. Use the outputs from `ping` and `telnet` commands to determine whether the host is reachable. If connectivity issues are detected, analyze the possible root causes and provide a structured summary of your findings.\\n\\nInstructions:\\n1. Interpret the `ping` and `telnet` results to assess host reachability.\\n2. Determine whether there is a connectivity issue.\\n3. Identify potential causes, such as network failure, firewall restrictions, or service unavailability.\\n4. Recommend appropriate next steps for troubleshooting or escalation.\\n\\nFormat your response as a structured summary:\\n\\nPing Status: Successful / Failed\\nTelnet Status: Connected / Failed\\nPotential Cause of Connectivity Issue: [e.g., network failure, firewall rules, service outage, no issue]\\nNext Steps: [e.g., check network logs, restart network services, escalate issue, or no action needed]\\n\\nPing Output:\\n{ping_data}\\n\\nTelnet Output:\\n{telnet_data}', offline_mode=True), 'telemetry_metrics_host_heartbeat_check': TelemetryMetricsHostHeartbeatCheckToolConfig(description=\"This tool checks if a host's telemetry monitoring service is reporting heartbeat metrics. This tells us if the host is up and running. Args: host_id: str\", llm_name='tool_reasoning_llm', prompt=\"The following is the telemetry metrics fetched for the host to see if it's been up and running (if result is empty, then the monitoring service on the host is down):\\n{data}\\nBased on the data, summarize the fetched data and provide a conclusion of the host's running status.\", offline_mode=True, metrics_url=''), 'telemetry_metrics_host_performance_check': TelemetryMetricsHostPerformanceCheckToolConfig(description='This tool checks the performance of the host by analyzing the CPU usage timeseries. Args: host_id: str', llm_name='tool_reasoning_llm', prompt=\"You are an expert on analyzing CPU usage timeseries. Periodic usage peaks are expected benign system behavior.\\nUser will provide data in the format of a list of lists, where each sublist contains two elements: timestamp and CPU usage percentage. User will also provide statistics on the timeseries. Write a markdown report about what was observed in the timeseries.\\n\\nExample format:\\n# CPU Usage Analysis Report\\nThe data analysis is performed on 14 days of CPU usage percentage data.\\n\\n## Data Statistics\\ndata start and end time, data point interval, CPU usage statistics\\n\\n## Observations\\nany patterns observed? Should be one of the below cases:\\n- Are there any cyclic usage surges?\\n  - What is the cycle?\\n  - What is the high and low CPU usage of the pattern?\\n- Is there one anomalous peak?\\n  - When did it happen?\\n  - What is it like before and after?\\n- No obvious pattern? A mix of patterns? => it's normal flutuation of the system (max usage less than 60%)\\n  - What is the fluctuation range?\\n\\n## Conclusion\\nSummarize the observation.\\nCategories:\\n- peak in the data means the high CPU usage is an anomaly and requires attention\\n- periodic behvior means the high usage is benign\\n- overall moderate (max usage less than 60%) usage means no issue in the system\\n\\n## Pattern Label\\nAnomalous Peak/Periodic Surges/Normal Fluctuations\\n\", offline_mode=True, metrics_url=''), 'telemetry_metrics_analysis_agent': TelemetryMetricsAnalysisAgentConfig(description='This is a telemetry metrics tool used to monitor remotely collected telemetry data. It checks server heartbeat data to determine whether the server is up and running and analyzes CPU usage patterns over the past 14 days to identify potential CPU issues. Args: host_id: str, alert_type: str', tool_names=['telemetry_metrics_host_heartbeat_check', 'telemetry_metrics_host_performance_check'], llm_name='agent_llm', prompt=\"You arg a helpful alert triage assistant. Your task is to investigate an alert that was just triggered on a specific host. You will be given two inputs:\\n- `host_id`: the identifier of the host where the alert occurred.\\n- `alert_type`: the type of alert that triggered.\\n\\nUse the tools provided below to collect relevant telemetry data for the specified host:\\n\\nTools:\\n- `telemetry_metrics_host_heartbeat_check`: Use this to check the server's heartbeat and determine if the host is currently up and responsive.\\n- `telemetry_metrics_host_performance_check`: Use this to analyze CPU usage trends over the past 14 days and identify abnormal patterns.\\n\\nInstructions:\\n1. Run the appropriate tools based on the host and alert type.\\n2. Collect and include all relevant output from the tools in your response.\\n3. Analyze the data and provide reasoning to help determine whether the telemetry supports or explains the triggered alert.\\n\\nYour response should include:\\n- Raw data from each tool\\n- A concise summary of findings\\n- Any insights or hypotheses that explain the alert\"), 'maintenance_check': MaintenanceCheckToolConfig(description='Check if a host is under maintenance during the time of an alert to help determine if the alert can be deprioritized.', llm_name='agent_llm', prompt='User will provide you with a system alert represented in JSON format. You know for a fact that there is maintenance happening for the host. Maintenance start time for this host is : [{maintenance_start_str}]; end time is: [{maintenance_end_str}] (end time empty means that there is not yet a set end time for the maintenance on the host)\\nGenerate a markdown report in the following format:\\n\\n## Alert Summary\\n(summary of what happened in the alert JSON data)\\n\\n## Collected Metrics\\n(lay out the maintenance information)\\n\\n## Analysis\\n(Describe the maintenance status of this host)\\n\\n## Recommended Actions\\n(Bullet point list: write how the user may not need to worry about this alert given that the host is under maintenance, and they could check if the issue persists afterward)\\n\\n## Alert Status\\n(can deprioritize the investigation of the alert, host under maintenance)', static_data_path='/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/maintenance_static_dataset.csv', skip_maintenance_check=False), 'categorizer': CategorizerToolConfig(description='This is a categorization tool used at the end of the pipeline.', llm_name='agent_llm', prompt='You will be given a system-generated alert triage report. Your job is to read the report carefully and determine the most likely root cause of the issue. Then, categorize the root cause into one of the following predefined categories:\\n\\n**Valid Categories**\\n- `software`: The alert was triggered due to a malfunctioning or inactive monitoring service (e.g., Telegraf not running).\\n- `network_connectivity`: The host is not reachable via ping or curl, or there are signs of connection issues due to blocked ports, broken services, or firewall rules (e.g., telnet fails).\\n- `hardware`: The alert is caused by a hardware failure or degradation.\\n- `repetitive_behavior`: The alert is triggered by a recurring or periodic behavior pattern (e.g., regular CPU spikes or memory surges).\\n- `false_positive`: No clear signs of failure or degradation; system appears healthy and no suspicious pattern is found.\\n- `need_investigation`: The report contains conflicting, ambiguous, or insufficient information to determine a clear root cause.\\n\\n**Response Format**\\n- Line 1: Output only the category name (e.g., `hardware`)\\n- Line 2: Briefly explain your reasoning based on the contents of the report.\\n- Example response:\\nnetwork_connectivity\\nPing and curl to the host both failed, and telnet to the monitored port timed out, indicating a likely connectivity or firewall issue.\\n\\n**Important Guidelines**\\n- Base your categorization only on evidence presented in the report.\\n- If no category clearly fits, default to `need_investigation`.')} function_groups={} llms={'agent_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.5, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=2048), 'tool_reasoning_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.2, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=2048), 'nim_rag_eval_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/alert_triage_all_params_selection_output/optimizer'), eval_metrics={'classification_accuracy': OptimizerMetric(evaluator_name='classification_accuracy', direction='maximize', weight=1.0), 'llm_latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=1, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=True, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=AlertTriageAgentWorkflowConfig(tool_names=['hardware_check', 'host_performance_check', 'monitoring_process_check', 'network_connectivity_check', 'telemetry_metrics_analysis_agent'], llm_name='agent_llm', offline_mode=True, offline_data_path='/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/offline_data.csv', benign_fallback_data_path='/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/benign_fallback_offline_data.json', agent_prompt='**Role**\\nYou are a Triage Agent responsible for diagnosing and troubleshooting system alerts in real time. Your goal is to determine whether an alert indicates a true issue, identify the root cause, and provide a clear, structured triage report to assist system analysts.\\n\\n\\n**Instructions**\\n\\n1. **Analyze the Alert**\\n   Begin by interpreting the incoming alert. Identify its type (e.g., *InstanceDown*, *HighCPUUsage*) and note any relevant details.\\n\\n2. **Select and Use Diagnostic Tools**\\n   Based on the alert type, choose the most relevant tools to gather system metrics. Use each tool only once per alert.\\n\\n   - `hardware_check`: Retrieves server power status and hardware health via IPMI. Useful for diagnosing instance down alerts or suspected hardware failures.\\n   - `host_performance_check`: Collects system-level CPU and memory usage using commands like `top` and `ps`. Use this to identify host\\'s resource (CPR and memory) usage bottlenecks.\\n   - `monitoring_process_check`: Checks whether critical processes are running on the host. Useful for verifying system functionality during instance down or degraded performance.\\n   - `network_connectivity_check`: Tests host connectivity through ping, telnet, and HTTP health checks. Helps determine if the server is reachable from the network.\\n   - `telemetry_metrics_analysis_agent`: Pulls telemetry metrics to check host status and analyze usage trends. Effective for validating instance uptime and system load over time.\\n\\n   Once you\\'ve received outputs from all selected tools, **pause to analyze them before proceeding further**.\\n\\n3. **Correlate Data and Determine Root Cause**\\n   - Evaluate the retrieved metrics against the alert details.\\n   - Determine if the alert reflects a real problem or is a false positive.\\n   - If an issue is detected, identify likely causes—such as hardware failure, performance bottlenecks, or network issues.\\n\\n4. **Generate a Structured Triage Report (in Markdown format)**\\n   Organize your findings clearly under these sections:\\n\\n   - **Alert Summary**: Brief description of the alert received.\\n   - **Collected Metrics**: Outputs from the diagnostic tools used.\\n   - **Analysis**: Interpretation of the data and how it relates to the alert.\\n   - **Recommended Actions**: Suggested next steps to mitigate or resolve the issue.\\n   - **Alert Status**: Choose one — \"Valid\", \"Abnormal but benign\", or \"False alarm\".\\n\\n\\n**Important Rules**\\n- Do not call the same tool more than once per alert.\\n- Analyze tool outputs before taking any additional action.\\n- Stay concise, structured, and actionable.') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/alert_triage_all_params_selection_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/offline_data.json'), profiler=None), evaluators={'classification_accuracy': ClassificationEvaluatorConfig(), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8), 'rag_accuracy': RagasEvaluatorConfig(llm_name='nim_rag_eval_llm', metric='AnswerAccuracy', input_obj_field=None)})\n",
      "2025-10-26 08:30:18 - INFO     - nat_alert_triage_agent:104 - Preloaded test data from: /Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/offline_data.csv\n",
      "2025-10-26 08:30:18 - INFO     - nat_alert_triage_agent:108 - Preloaded benign fallback data from: /Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/benign_fallback_offline_data.json\n",
      "2025-10-26 08:30:18 - INFO     - nat_alert_triage_agent:80 - ================================================Running in offline mode=================================================\n",
      "Running workflow:   0%|                                   | 0/7 [00:00<?, ?it/s]2025-10-26 08:30:18 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-0.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-26 08:30:18 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-1.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-26 08:30:18 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-2.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-26 08:30:18 - INFO     - nat_alert_triage_agent:258 - Host: [test-instance-3.example.com] is under maintenance according to the maintenance database\n",
      "2025-10-26 08:30:20 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-4.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-26 08:30:20 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-5.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-26 08:30:20 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-6.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-26 08:30:20 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  14%|███▊                       | 1/7 [00:02<00:13,  2.24s/it]2025-10-26 08:30:20 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "2025-10-26 08:30:20 - ERROR    - nat.builder.function:162 - Error with ainvoke in function with input: {\"alert_id\":5,\"alert_name\":\"CPUUsageHighError\",\"host_id\":\"test-instance-5.example.com\",\"severity\":\"error\",\"description\":\"CPU usage on test-instance-5.example.com is critically high (current value: 100%). Please check the following: - CPU usage trends across all cores; - running processes that may be causing high load; - potential hardware issues (e.g., I/O bottlenecks).\",\"summary\":\"High CPU usage detected on test-instance-5.example.com\",\"timestamp\":\"2025-05-02T02:00:00.000000\"}. Error: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}\n",
      "2025-10-26 08:30:20 - ERROR    - nat.runtime.runner:200 - Error running workflow: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}\n",
      "2025-10-26 08:30:20 - ERROR    - nat.eval.evaluate:183 - Failed to run the workflow: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/src/nat/eval/evaluate.py\", line 176, in run_one\n",
      "    base_output = await runner_result\n",
      "                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/src/nat/runtime/runner.py\", line 176, in result\n",
      "    result = await self._entry_fn.ainvoke(self._input_message, to_type=to_type)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/src/nat/builder/function.py\", line 153, in ainvoke\n",
      "    result = await self._ainvoke(converted_input)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/src/nat/builder/function.py\", line 329, in _ainvoke\n",
      "    return await self._ainvoke_fn(value)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/register.py\", line 147, in _response_fn\n",
      "    result = await _process_alert(input_message)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/src/nat/profiler/decorators/function_tracking.py\", line 191, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/register.py\", line 137, in _process_alert\n",
      "    output = await agent_executor.ainvoke({\"messages\": [HumanMessage(content=input_message)]})\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langgraph/pregel/main.py\", line 3112, in ainvoke\n",
      "    async for chunk in self.astream(\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langgraph/pregel/main.py\", line 2939, in astream\n",
      "    async for _ in runner.atick(\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langgraph/pregel/_runner.py\", line 295, in atick\n",
      "    await arun_with_retry(\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langgraph/pregel/_retry.py\", line 137, in arun_with_retry\n",
      "    return await task.proc.ainvoke(task.input, config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langgraph/_internal/_runnable.py\", line 706, in ainvoke\n",
      "    input = await asyncio.create_task(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.11_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.11_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
      "    future.result()\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.11_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.11_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langgraph/_internal/_runnable.py\", line 474, in ainvoke\n",
      "    ret = await self.afunc(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/register.py\", line 101, in ata_assistant\n",
      "    return {\"messages\": [await llm_n_tools.ainvoke([sys_msg] + state[\"messages\"])]}\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
      "    return await self.bound.ainvoke(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/src/nat/utils/exception_handlers/automatic_retries.py\", line 252, in _call_with_retry_async\n",
      "    return await fn(*args, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
      "    llm_result = await self.agenerate_prompt(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/src/nat/utils/exception_handlers/automatic_retries.py\", line 252, in _call_with_retry_async\n",
      "    return await fn(*args, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/src/nat/utils/exception_handlers/automatic_retries.py\", line 252, in _call_with_retry_async\n",
      "    return await fn(*args, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.11_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py\", line 316, in __step_run_and_handle_result\n",
      "    result = coro.throw(exc)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 1222, in _agenerate\n",
      "    return await run_in_executor(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langchain_core/runnables/config.py\", line 611, in run_in_executor\n",
      "    return await asyncio.get_running_loop().run_in_executor(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.11_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.11_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
      "    future.result()\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.11_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.11_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/thread.py\", line 59, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langchain_core/runnables/config.py\", line 602, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langchain_nvidia_ai_endpoints/chat_models.py\", line 490, in _generate\n",
      "    response = self._client.get_req(payload=payload, extra_headers=extra_headers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langchain_nvidia_ai_endpoints/_common.py\", line 500, in get_req\n",
      "    response, session = self._post(\n",
      "                        ^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langchain_nvidia_ai_endpoints/_common.py\", line 396, in _post\n",
      "    self._try_raise(response)\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langchain_nvidia_ai_endpoints/_common.py\", line 489, in _try_raise\n",
      "    raise Exception(f\"{header}\\n{body}\") from None\n",
      "Exception: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}\n",
      "During task with name 'ata_assistant' and id 'c8a75f1f-e7d4-e700-4463-25467ac73390'\n",
      "Running workflow:  29%|███████▋                   | 2/7 [00:02<00:05,  1.06s/it]2025-10-26 08:30:20 - ERROR    - asyncio:1833 - Task exception was never retrieved\n",
      "future: <Task finished name='Task-931' coro=<Runner.result() done, defined at /Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/src/nat/runtime/runner.py:133> exception=RuntimeError('cannot reuse already awaited coroutine')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.11_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py\", line 316, in __step_run_and_handle_result\n",
      "    result = coro.throw(exc)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "RuntimeError: cannot reuse already awaited coroutine\n",
      "2025-10-26 08:31:04 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  43%|███████████▌               | 3/7 [00:46<01:22, 20.63s/it]2025-10-26 08:32:10 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  57%|███████████████▍           | 4/7 [01:52<01:55, 38.39s/it]2025-10-26 08:32:20 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  71%|███████████████████▎       | 5/7 [02:02<00:56, 28.28s/it]2025-10-26 08:32:43 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  86%|███████████████████████▏   | 6/7 [02:25<00:26, 26.46s/it]2025-10-26 08:33:09 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow: 100%|███████████████████████████| 7/7 [02:50<00:00, 24.41s/it]\n",
      "Evaluating classification accuracy:   0%|                 | 0/7 [00:00<?, ?it/s]\n",
      "Evaluating Avg LLM Latency:   0%|                         | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Evaluating Avg Tokens/LLM_END:   0%|                      | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating classification accuracy:  14%|█▎       | 1/7 [00:00<00:01,  5.08it/s]\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg LLM Latency:  14%|██▍              | 1/7 [00:00<00:01,  5.04it/s]\u001b[A\n",
      "\n",
      "Evaluating classification accuracy: 100%|█████████| 7/7 [00:00<00:00, 32.98it/s]\u001b[A\u001b[A\n",
      "Evaluating Avg LLM Latency: 100%|█████████████████| 7/7 [00:00<00:00, 32.98it/s]\n",
      "Evaluating Avg Tokens/LLM_END: 100%|██████████████| 7/7 [00:00<00:00, 32.99it/s]\n",
      "2025-10-26 08:33:09 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  14%|██▏            | 1/7 [00:00<00:05,  1.08it/s]\u001b[A\u001b[A\u001b[A2025-10-26 08:33:09 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  57%|████████▌      | 4/7 [00:01<00:00,  4.01it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy: 100%|███████████████| 7/7 [00:01<00:00,  3.68it/s]\u001b[A\u001b[A\u001b[A\n",
      "2025-10-26 08:33:10 - INFO     - nat_alert_triage_agent:164 - Cleaning up\n",
      "2025-10-26 08:33:10 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-26 08:33:10 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/alert_triage_all_params_selection_output/workflow_output.json\n",
      "2025-10-26 08:33:11 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_all_params_selection_output/classification_accuracy_output.json\n",
      "2025-10-26 08:33:11 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_all_params_selection_output/llm_latency_output.json\n",
      "2025-10-26 08:33:11 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_all_params_selection_output/token_efficiency_output.json\n",
      "2025-10-26 08:33:11 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_all_params_selection_output/rag_accuracy_output.json\n",
      "2025-10-26 08:33:11 - WARNING  - nat.eval.evaluate:360 - Workflow execution was interrupted due to an error. The results may be incomplete. You can re-execute evaluation for incomplete results by running `eval` with the --skip_completed_entries flag.\n",
      "\u001b[32m[I 2025-10-26 08:33:11,019]\u001b[0m Trial 2 finished with values: [0.43, 7.37] and parameters: {'llms.agent_llm.temperature': 0.5, 'llms.agent_llm.model_name': 'meta/llama-3.1-8b-instruct'}.\u001b[0m\n",
      "2025-10-26 08:33:11 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: general=GeneralConfig(use_uvloop=None, telemetry=TelemetryConfig(logging={}, tracing={}), front_end=FastApiFrontEndConfig(root_path='', host='localhost', port=8000, reload=False, workers=1, scheduler_address=None, db_url=None, max_running_async_jobs=10, dask_log_level='WARNING', step_adaptor=StepAdaptorConfig(mode=<StepAdaptorMode.DEFAULT: 'default'>, custom_event_types=[]), workflow=EndpointBase(method='POST', description='Executes the default NAT workflow from the loaded configuration ', path='/generate', websocket_path='/websocket', openai_api_path='/chat', openai_api_v1_path='/v1/chat/completions'), evaluate=EndpointBase(method='POST', description='Evaluates the performance and accuracy of the workflow on a dataset', path='/evaluate', websocket_path=None, openai_api_path=None, openai_api_v1_path=None), oauth2_callback_path='/auth/redirect', endpoints=[], cors=CrossOriginResourceSharing(allow_origins=None, allow_origin_regex=None, allow_methods=['GET'], allow_headers=[], allow_credentials=False, expose_headers=[], max_age=600), use_gunicorn=False, runner_class=None, object_store=None)) functions={'hardware_check': HardwareCheckToolConfig(description='This tool checks hardware health status using IPMI monitoring to detect power state, hardware degradation, and anomalies that could explain alerts. Args: host_id: str', llm_name='tool_reasoning_llm', prompt=\"You are analyzing IPMI metrics to support host monitoring and alert triage. Use the provided IPMI output to assess overall system status. Your goals are to:\\n\\n1. Determine the system's current power state.\\n2. Identify any signs of hardware degradation or failure.\\n3. Flag any anomalies that could explain why a monitoring alert was triggered.\\n\\nReview the data carefully and summarize your assessment in a clear and structured format.\\n\\nIPMI Output:\\n{input_data}\\n\\nFormat your response as follows:\\n\\nPower Status: ON / OFF\\nHardware Health: Normal / Issues Detected\\nObserved Anomalies: [List any irregularities or warning signs]\\nPossible Cause of Alert: [e.g., hardware issue, thermal spike, power fluctuation, no clear issue]\\nNext Steps: [Recommended actions or checks for further triage]\", offline_mode=True), 'host_performance_check': HostPerformanceCheckToolConfig(description='This tool retrieves CPU usage, memory usage, and hardware I/O usage details for a given host. Args: host_id: str', llm_name='tool_reasoning_llm', parsing_prompt='You are given system performance data captured from a host. Your task is to extract and organize the information into a clean, structured JSON format. The input contains system details and performance metrics, such as CPU, memory, and disk I/O.\\n\\nFollow these instructions:\\n\\n1. Identify metric categories dynamically based on the line prefixes or column headers (e.g., \"Mem:\", \"Swap:\", \"CPU:\", \"Device:\").\\n2. For each category, extract the numerical values and map them to meaningful field names.\\n3. Group related fields under sections such as \"memory_usage\", \"swap_usage\", \"cpu_usage\", \"disk_io\", etc.\\n4. Use consistent, readable key names for all fields.\\n5. Return **only** the final JSON object — no explanations or extra text.\\n\\nHere is the input data:\\n{input_data}', analysis_prompt='You are analyzing system metrics to assess CPU and memory usage. Use the output below to determine whether CPU or memory usage is abnormally high, identify which processes are consuming the most resources, and assess whether the usage patterns could explain a recent alert.\\n\\nInstructions:\\n1. Evaluate overall CPU and memory usage levels.\\n2. List the top resource-consuming processes, including their name, PID, %CPU, and %MEM.\\n3. Identify any potential causes of high usage (e.g., memory leak, runaway process, legitimate high load).\\n4. Recommend possible next steps for investigation or mitigation.\\n\\nFormat your response as a structured summary:\\n\\nCPU Usage: Normal / High (X% usage)\\nMemory Usage: Normal / High (X% usage)\\nTop Resource-Consuming Processes: [Process name, PID, %CPU, %MEM]\\nPotential Cause of High Usage: [e.g., runaway process, heavy load, memory leak]\\nNext Steps: [Suggested mitigation actions]\\n\\nSystem Metrics Output:\\n{input_data}\\n', offline_mode=True), 'monitoring_process_check': MonitoringProcessCheckToolConfig(description='This tool checks the status of critical monitoring processes and services on a target host by executing system commands. Args: host_id: str', llm_name='tool_reasoning_llm', prompt='You are checking whether the telegraf service is running on the server. Use the monitoring output below to verify its status. If it’s not running, identify possible reasons and assess the impact.\\n\\nInstructions:\\n1. Check if the telegraf process is present and active.\\n2. Evaluate the potential impact of telegraf not running on system availability or monitoring.\\n3. Identify likely causes for the process not running.\\n\\nFormat your response as a structured summary:\\n* **Telegraf Running:** Yes / No\\n* **Potential Impact:** [e.g., host seems down to the monitoring system, delayed alerting]\\n* **Possible Cause:** [e.g., process crash, misconfiguration, resource constraints]\\n* **Next Steps:** [e.g., restart telegraf, check logs]\\n\\nMonitoring Output:\\n{input_data}', offline_mode=True), 'network_connectivity_check': NetworkConnectivityCheckToolConfig(description='This tool checks network connectivity of a host by running ping and socket connection tests. Args: host_id: str', llm_name='tool_reasoning_llm', prompt='You are assisting with alert triage by checking the network connectivity status of a host. Use the outputs from `ping` and `telnet` commands to determine whether the host is reachable. If connectivity issues are detected, analyze the possible root causes and provide a structured summary of your findings.\\n\\nInstructions:\\n1. Interpret the `ping` and `telnet` results to assess host reachability.\\n2. Determine whether there is a connectivity issue.\\n3. Identify potential causes, such as network failure, firewall restrictions, or service unavailability.\\n4. Recommend appropriate next steps for troubleshooting or escalation.\\n\\nFormat your response as a structured summary:\\n\\nPing Status: Successful / Failed\\nTelnet Status: Connected / Failed\\nPotential Cause of Connectivity Issue: [e.g., network failure, firewall rules, service outage, no issue]\\nNext Steps: [e.g., check network logs, restart network services, escalate issue, or no action needed]\\n\\nPing Output:\\n{ping_data}\\n\\nTelnet Output:\\n{telnet_data}', offline_mode=True), 'telemetry_metrics_host_heartbeat_check': TelemetryMetricsHostHeartbeatCheckToolConfig(description=\"This tool checks if a host's telemetry monitoring service is reporting heartbeat metrics. This tells us if the host is up and running. Args: host_id: str\", llm_name='tool_reasoning_llm', prompt=\"The following is the telemetry metrics fetched for the host to see if it's been up and running (if result is empty, then the monitoring service on the host is down):\\n{data}\\nBased on the data, summarize the fetched data and provide a conclusion of the host's running status.\", offline_mode=True, metrics_url=''), 'telemetry_metrics_host_performance_check': TelemetryMetricsHostPerformanceCheckToolConfig(description='This tool checks the performance of the host by analyzing the CPU usage timeseries. Args: host_id: str', llm_name='tool_reasoning_llm', prompt=\"You are an expert on analyzing CPU usage timeseries. Periodic usage peaks are expected benign system behavior.\\nUser will provide data in the format of a list of lists, where each sublist contains two elements: timestamp and CPU usage percentage. User will also provide statistics on the timeseries. Write a markdown report about what was observed in the timeseries.\\n\\nExample format:\\n# CPU Usage Analysis Report\\nThe data analysis is performed on 14 days of CPU usage percentage data.\\n\\n## Data Statistics\\ndata start and end time, data point interval, CPU usage statistics\\n\\n## Observations\\nany patterns observed? Should be one of the below cases:\\n- Are there any cyclic usage surges?\\n  - What is the cycle?\\n  - What is the high and low CPU usage of the pattern?\\n- Is there one anomalous peak?\\n  - When did it happen?\\n  - What is it like before and after?\\n- No obvious pattern? A mix of patterns? => it's normal flutuation of the system (max usage less than 60%)\\n  - What is the fluctuation range?\\n\\n## Conclusion\\nSummarize the observation.\\nCategories:\\n- peak in the data means the high CPU usage is an anomaly and requires attention\\n- periodic behvior means the high usage is benign\\n- overall moderate (max usage less than 60%) usage means no issue in the system\\n\\n## Pattern Label\\nAnomalous Peak/Periodic Surges/Normal Fluctuations\\n\", offline_mode=True, metrics_url=''), 'telemetry_metrics_analysis_agent': TelemetryMetricsAnalysisAgentConfig(description='This is a telemetry metrics tool used to monitor remotely collected telemetry data. It checks server heartbeat data to determine whether the server is up and running and analyzes CPU usage patterns over the past 14 days to identify potential CPU issues. Args: host_id: str, alert_type: str', tool_names=['telemetry_metrics_host_heartbeat_check', 'telemetry_metrics_host_performance_check'], llm_name='agent_llm', prompt=\"You arg a helpful alert triage assistant. Your task is to investigate an alert that was just triggered on a specific host. You will be given two inputs:\\n- `host_id`: the identifier of the host where the alert occurred.\\n- `alert_type`: the type of alert that triggered.\\n\\nUse the tools provided below to collect relevant telemetry data for the specified host:\\n\\nTools:\\n- `telemetry_metrics_host_heartbeat_check`: Use this to check the server's heartbeat and determine if the host is currently up and responsive.\\n- `telemetry_metrics_host_performance_check`: Use this to analyze CPU usage trends over the past 14 days and identify abnormal patterns.\\n\\nInstructions:\\n1. Run the appropriate tools based on the host and alert type.\\n2. Collect and include all relevant output from the tools in your response.\\n3. Analyze the data and provide reasoning to help determine whether the telemetry supports or explains the triggered alert.\\n\\nYour response should include:\\n- Raw data from each tool\\n- A concise summary of findings\\n- Any insights or hypotheses that explain the alert\"), 'maintenance_check': MaintenanceCheckToolConfig(description='Check if a host is under maintenance during the time of an alert to help determine if the alert can be deprioritized.', llm_name='agent_llm', prompt='User will provide you with a system alert represented in JSON format. You know for a fact that there is maintenance happening for the host. Maintenance start time for this host is : [{maintenance_start_str}]; end time is: [{maintenance_end_str}] (end time empty means that there is not yet a set end time for the maintenance on the host)\\nGenerate a markdown report in the following format:\\n\\n## Alert Summary\\n(summary of what happened in the alert JSON data)\\n\\n## Collected Metrics\\n(lay out the maintenance information)\\n\\n## Analysis\\n(Describe the maintenance status of this host)\\n\\n## Recommended Actions\\n(Bullet point list: write how the user may not need to worry about this alert given that the host is under maintenance, and they could check if the issue persists afterward)\\n\\n## Alert Status\\n(can deprioritize the investigation of the alert, host under maintenance)', static_data_path='/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/maintenance_static_dataset.csv', skip_maintenance_check=False), 'categorizer': CategorizerToolConfig(description='This is a categorization tool used at the end of the pipeline.', llm_name='agent_llm', prompt='You will be given a system-generated alert triage report. Your job is to read the report carefully and determine the most likely root cause of the issue. Then, categorize the root cause into one of the following predefined categories:\\n\\n**Valid Categories**\\n- `software`: The alert was triggered due to a malfunctioning or inactive monitoring service (e.g., Telegraf not running).\\n- `network_connectivity`: The host is not reachable via ping or curl, or there are signs of connection issues due to blocked ports, broken services, or firewall rules (e.g., telnet fails).\\n- `hardware`: The alert is caused by a hardware failure or degradation.\\n- `repetitive_behavior`: The alert is triggered by a recurring or periodic behavior pattern (e.g., regular CPU spikes or memory surges).\\n- `false_positive`: No clear signs of failure or degradation; system appears healthy and no suspicious pattern is found.\\n- `need_investigation`: The report contains conflicting, ambiguous, or insufficient information to determine a clear root cause.\\n\\n**Response Format**\\n- Line 1: Output only the category name (e.g., `hardware`)\\n- Line 2: Briefly explain your reasoning based on the contents of the report.\\n- Example response:\\nnetwork_connectivity\\nPing and curl to the host both failed, and telnet to the monitored port timed out, indicating a likely connectivity or firewall issue.\\n\\n**Important Guidelines**\\n- Base your categorization only on evidence presented in the report.\\n- If no category clearly fits, default to `need_investigation`.')} function_groups={} llms={'agent_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-8b-instruct', max_tokens=2048), 'tool_reasoning_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.2, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=2048), 'nim_rag_eval_llm': NIMModelConfig(thinking=None, top_p=1.0, temperature=0.0, optimizable_params=[], search_space={}, do_auto_retry=True, num_retries=5, retry_on_status_codes=[429, 500, 502, 503, 504], retry_on_errors=['Too Many Requests'], api_type=<APITypeEnum.CHAT_COMPLETION: 'chat_completion'>, api_key=None, base_url=None, model_name='meta/llama-3.1-70b-instruct', max_tokens=8)} embedders={} memory={} object_stores={} optimizer=OptimizerConfig(output_path=PosixPath('tmp_workflow/alert_triage_all_params_selection_output/optimizer'), eval_metrics={'classification_accuracy': OptimizerMetric(evaluator_name='classification_accuracy', direction='maximize', weight=1.0), 'llm_latency': OptimizerMetric(evaluator_name='llm_latency', direction='minimize', weight=1.0)}, reps_per_param_set=1, target=None, multi_objective_combination_mode='harmonic', numeric=NumericOptimizationConfig(enabled=True, n_trials=20, sampler=<SamplerType.GRID: 'grid'>), prompt=PromptGAOptimizationConfig(enabled=True, prompt_population_init_function=None, prompt_recombination_function=None, ga_population_size=24, ga_generations=15, ga_offspring_size=None, ga_crossover_rate=0.8, ga_mutation_rate=0.3, ga_elitism=2, ga_selection_method='tournament', ga_tournament_size=3, ga_parallel_evaluations=8, ga_diversity_lambda=0.0)) retrievers={} ttc_strategies={} workflow=AlertTriageAgentWorkflowConfig(tool_names=['hardware_check', 'host_performance_check', 'monitoring_process_check', 'network_connectivity_check', 'telemetry_metrics_analysis_agent'], llm_name='agent_llm', offline_mode=True, offline_data_path='/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/offline_data.csv', benign_fallback_data_path='/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/benign_fallback_offline_data.json', agent_prompt='**Role**\\nYou are a Triage Agent responsible for diagnosing and troubleshooting system alerts in real time. Your goal is to determine whether an alert indicates a true issue, identify the root cause, and provide a clear, structured triage report to assist system analysts.\\n\\n\\n**Instructions**\\n\\n1. **Analyze the Alert**\\n   Begin by interpreting the incoming alert. Identify its type (e.g., *InstanceDown*, *HighCPUUsage*) and note any relevant details.\\n\\n2. **Select and Use Diagnostic Tools**\\n   Based on the alert type, choose the most relevant tools to gather system metrics. Use each tool only once per alert.\\n\\n   - `hardware_check`: Retrieves server power status and hardware health via IPMI. Useful for diagnosing instance down alerts or suspected hardware failures.\\n   - `host_performance_check`: Collects system-level CPU and memory usage using commands like `top` and `ps`. Use this to identify host\\'s resource (CPR and memory) usage bottlenecks.\\n   - `monitoring_process_check`: Checks whether critical processes are running on the host. Useful for verifying system functionality during instance down or degraded performance.\\n   - `network_connectivity_check`: Tests host connectivity through ping, telnet, and HTTP health checks. Helps determine if the server is reachable from the network.\\n   - `telemetry_metrics_analysis_agent`: Pulls telemetry metrics to check host status and analyze usage trends. Effective for validating instance uptime and system load over time.\\n\\n   Once you\\'ve received outputs from all selected tools, **pause to analyze them before proceeding further**.\\n\\n3. **Correlate Data and Determine Root Cause**\\n   - Evaluate the retrieved metrics against the alert details.\\n   - Determine if the alert reflects a real problem or is a false positive.\\n   - If an issue is detected, identify likely causes—such as hardware failure, performance bottlenecks, or network issues.\\n\\n4. **Generate a Structured Triage Report (in Markdown format)**\\n   Organize your findings clearly under these sections:\\n\\n   - **Alert Summary**: Brief description of the alert received.\\n   - **Collected Metrics**: Outputs from the diagnostic tools used.\\n   - **Analysis**: Interpretation of the data and how it relates to the alert.\\n   - **Recommended Actions**: Suggested next steps to mitigate or resolve the issue.\\n   - **Alert Status**: Choose one — \"Valid\", \"Abnormal but benign\", or \"False alarm\".\\n\\n\\n**Important Rules**\\n- Do not call the same tool more than once per alert.\\n- Analyze tool outputs before taking any additional action.\\n- Stay concise, structured, and actionable.') authentication={} eval=EvalConfig(general=EvalGeneralConfig(max_concurrency=8, workflow_alias=None, output_dir=PosixPath('tmp_workflow/alert_triage_all_params_selection_output'), output=None, dataset=EvalDatasetJsonConfig(id_key='id', structure=EvalDatasetStructureConfig(disable=False, question_key='question', answer_key='answer', generated_answer_key='generated_answer', trajectory_key='intermediate_steps', expected_trajectory_key='expected_intermediate_steps'), filter=EvalFilterConfig(allowlist=None, denylist=None), s3=None, remote_file_path=None, file_path='/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/offline_data.json'), profiler=None), evaluators={'classification_accuracy': ClassificationEvaluatorConfig(), 'llm_latency': AverageLLMLatencyConfig(max_concurrency=8), 'token_efficiency': AverageTokensPerLLMEndConfig(max_concurrency=8), 'rag_accuracy': RagasEvaluatorConfig(llm_name='nim_rag_eval_llm', metric='AnswerAccuracy', input_obj_field=None)})\n",
      "2025-10-26 08:33:11 - INFO     - nat_alert_triage_agent:104 - Preloaded test data from: /Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/offline_data.csv\n",
      "2025-10-26 08:33:11 - INFO     - nat_alert_triage_agent:108 - Preloaded benign fallback data from: /Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/benign_fallback_offline_data.json\n",
      "2025-10-26 08:33:11 - INFO     - nat_alert_triage_agent:80 - ================================================Running in offline mode=================================================\n",
      "Running workflow:   0%|                                   | 0/7 [00:00<?, ?it/s]2025-10-26 08:33:11 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-0.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-26 08:33:11 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-1.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-26 08:33:11 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-2.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-26 08:33:11 - INFO     - nat_alert_triage_agent:258 - Host: [test-instance-3.example.com] is under maintenance according to the maintenance database\n",
      "2025-10-26 08:33:14 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-4.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-26 08:33:14 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-5.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-26 08:33:14 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-6.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-26 08:33:14 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  14%|███▊                       | 1/7 [00:03<00:18,  3.14s/it]2025-10-26 08:33:14 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-26 08:33:14 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "2025-10-26 08:34:15 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  29%|███████▋                   | 2/7 [01:04<03:06, 37.40s/it]2025-10-26 08:34:38 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  43%|███████████▌               | 3/7 [01:27<02:03, 30.91s/it]2025-10-26 08:34:48 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  57%|███████████████▍           | 4/7 [01:37<01:07, 22.63s/it]2025-10-26 08:35:50 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  71%|███████████████████▎       | 5/7 [02:39<01:13, 36.64s/it]2025-10-26 08:36:22 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  86%|███████████████████████▏   | 6/7 [03:11<00:35, 35.06s/it]2025-10-26 08:37:37 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow: 100%|███████████████████████████| 7/7 [04:26<00:00, 38.04s/it]\n",
      "Evaluating classification accuracy:   0%|                 | 0/7 [00:00<?, ?it/s]\n",
      "Evaluating Avg LLM Latency:   0%|                         | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Evaluating Avg Tokens/LLM_END:   0%|                      | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating classification accuracy:  14%|█▎       | 1/7 [00:00<00:01,  3.95it/s]\u001b[A\u001b[A\u001b[A\n",
      "Evaluating Avg LLM Latency:  14%|██▍              | 1/7 [00:00<00:01,  3.91it/s]\u001b[A\n",
      "\n",
      "Evaluating classification accuracy: 100%|█████████| 7/7 [00:00<00:00, 25.68it/s]\u001b[A\u001b[A\n",
      "Evaluating Avg LLM Latency: 100%|█████████████████| 7/7 [00:00<00:00, 25.68it/s]\n",
      "Evaluating Avg Tokens/LLM_END: 100%|██████████████| 7/7 [00:00<00:00, 25.68it/s]\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  14%|██▏            | 1/7 [00:00<00:05,  1.05it/s]\u001b[A\u001b[A\u001b[A2025-10-26 08:37:38 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  29%|████▎          | 2/7 [00:01<00:02,  1.96it/s]\u001b[A\u001b[A\u001b[A2025-10-26 08:37:38 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-26 08:37:38 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-26 08:37:38 - INFO     - nat.utils.exception_handlers.automatic_retries:159 - Retrying on exception [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'} with matched message [429] too many requests\n",
      "{'status': 429, 'title': 'too many requests'}\n",
      "\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  71%|██████████▋    | 5/7 [00:01<00:00,  5.01it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  86%|████████████▊  | 6/7 [00:01<00:00,  5.67it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy: 100%|███████████████| 7/7 [00:02<00:00,  3.15it/s]\u001b[A\u001b[A\u001b[A\n",
      "2025-10-26 08:37:39 - INFO     - nat_alert_triage_agent:164 - Cleaning up\n",
      "2025-10-26 08:37:39 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-26 08:37:39 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/alert_triage_all_params_selection_output/workflow_output.json\n",
      "2025-10-26 08:37:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_all_params_selection_output/classification_accuracy_output.json\n",
      "2025-10-26 08:37:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_all_params_selection_output/llm_latency_output.json\n",
      "2025-10-26 08:37:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_all_params_selection_output/token_efficiency_output.json\n",
      "2025-10-26 08:37:39 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_all_params_selection_output/rag_accuracy_output.json\n",
      "\u001b[32m[I 2025-10-26 08:37:39,640]\u001b[0m Trial 3 finished with values: [0.57, 9.76] and parameters: {'llms.agent_llm.temperature': 0.0, 'llms.agent_llm.model_name': 'meta/llama-3.1-8b-instruct'}.\u001b[0m\n",
      "2025-10-26 08:37:39 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:127 - Numeric optimization finished\n",
      "2025-10-26 08:37:39 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:173 - Generating Pareto front visualizations...\n",
      "2025-10-26 08:37:39 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:408 - Creating Pareto front visualizations...\n",
      "2025-10-26 08:37:39 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:409 - Total trials: 4\n",
      "2025-10-26 08:37:39 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:410 - Pareto optimal trials: 2\n",
      "2025-10-26 08:37:41 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:143 - 2D Pareto plot saved to: tmp_workflow/alert_triage_all_params_selection_output/optimizer/plots/pareto_front_2d.png\n",
      "2025-10-26 08:37:41 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:231 - Parallel coordinates plot saved to: tmp_workflow/alert_triage_all_params_selection_output/optimizer/plots/pareto_parallel_coordinates.png\n",
      "2025-10-26 08:37:42 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:325 - Pairwise matrix plot saved to: tmp_workflow/alert_triage_all_params_selection_output/optimizer/plots/pareto_pairwise_matrix.png\n",
      "2025-10-26 08:37:42 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:438 - Visualization complete!\n",
      "2025-10-26 08:37:42 - INFO     - nat.profiler.parameter_optimization.pareto_visualizer:440 - Plots saved to: tmp_workflow/alert_triage_all_params_selection_output/optimizer/plots\n",
      "2025-10-26 08:37:42 - INFO     - nat.profiler.parameter_optimization.parameter_optimizer:182 - Pareto visualizations saved to: tmp_workflow/alert_triage_all_params_selection_output/optimizer/plots\n",
      "2025-10-26 08:37:42 - WARNING  - nat.experimental.decorators.experimental_warning_decorator:59 - The Optimizer feature is experimental and the API may change in future releases. Future versions may introduce breaking changes without notice. Function: nat.profiler.parameter_optimization.prompt_optimizer.optimize_prompts\n",
      "2025-10-26 08:37:42 - INFO     - nat.profiler.parameter_optimization.prompt_optimizer:133 - No prompts to optimize – skipping.\n",
      "2025-10-26 08:37:42 - INFO     - nat.profiler.parameter_optimization.optimizer_runtime:66 - All optimization phases complete.\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!nat optimize --config_file tmp_workflow/configs/alert_triage_config_all_params_selection.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b3b51cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search Optimization Results\n",
      "================================================================================\n",
      "\n",
      "Trials Summary:\n",
      " number  values_classification_accuracy  values_llm_latency             datetime_start          datetime_complete               duration params_llms.agent_llm.model_name  params_llms.agent_llm.temperature      rep_scores  system_attrs_grid_id                                                                                                              system_attrs_search_space    state  pareto_optimal\n",
      "      0                            0.14               20.93 2025-10-25 16:27:18.487450 2025-10-25 16:36:40.685437 0 days 00:09:22.197987      meta/llama-3.1-70b-instruct                                0.0 [[0.14, 20.93]]                     0 {'llms.agent_llm.model_name': ['meta/llama-3.1-8b-instruct', 'meta/llama-3.1-70b-instruct'], 'llms.agent_llm.temperature': [0.0, 0.5]} COMPLETE           False\n",
      "      1                            0.29               10.99 2025-10-25 16:36:40.685690 2025-10-25 16:39:55.659208 0 days 00:03:14.973518      meta/llama-3.1-70b-instruct                                0.5 [[0.29, 10.99]]                     1 {'llms.agent_llm.model_name': ['meta/llama-3.1-8b-instruct', 'meta/llama-3.1-70b-instruct'], 'llms.agent_llm.temperature': [0.0, 0.5]} COMPLETE           False\n",
      "      2                            0.43               14.22 2025-10-25 16:39:55.659348 2025-10-25 16:47:48.087579 0 days 00:07:52.428231       meta/llama-3.1-8b-instruct                                0.5 [[0.43, 14.22]]                     2 {'llms.agent_llm.model_name': ['meta/llama-3.1-8b-instruct', 'meta/llama-3.1-70b-instruct'], 'llms.agent_llm.temperature': [0.0, 0.5]} COMPLETE           False\n",
      "      3                            0.43               10.10 2025-10-25 16:47:48.087698 2025-10-25 16:51:09.674472 0 days 00:03:21.586774       meta/llama-3.1-8b-instruct                                0.0  [[0.43, 10.1]]                     3 {'llms.agent_llm.model_name': ['meta/llama-3.1-8b-instruct', 'meta/llama-3.1-70b-instruct'], 'llms.agent_llm.temperature': [0.0, 0.5]} COMPLETE            True\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the optimizer results\n",
    "trials_df_path = Path(\"tmp_workflow/alert_triage_all_params_selection_output/optimizer/trials_dataframe_params.csv\")\n",
    "\n",
    "if trials_df_path.exists():\n",
    "    trials_df = pd.read_csv(trials_df_path)\n",
    "\n",
    "    print(\"Grid Search Optimization Results\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nTrials Summary:\")\n",
    "    print(trials_df.to_string(index=False))\n",
    "    print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69001d0",
   "metadata": {},
   "source": [
    "<a id=\"eval-triage-agent2\"></a>\n",
    "## 3.4) Re-evaluate the optimized tool-calling agent\n",
    "\n",
    "After completing the `nat optimize` run above, a new file with the optimal parameters from the search have been serialized and saved to `'./tmp_workflow/alert_triage_all_params_selection_output/optimizer/optimized_config.yml`. Let's re-run those optimized parameters back through `nat eval` and compare the performance.\n",
    "\n",
    "<div style=\"color: red; font-style: italic;\">\n",
    "<strong>Note:</strong> Performance of the optimized model may vary due to size of prior search space and number of evaluation trials.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "21a94fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-25 16:51:27 - INFO     - nat.eval.evaluate:448 - Starting evaluation run with config file: tmp_workflow/alert_triage_all_params_selection_output/optimizer/optimized_config.yml\n",
      "2025-10-25 16:51:51 - INFO     - nat_alert_triage_agent:104 - Preloaded test data from: /Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/offline_data.csv\n",
      "2025-10-25 16:51:51 - INFO     - nat_alert_triage_agent:108 - Preloaded benign fallback data from: /Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/data/benign_fallback_offline_data.json\n",
      "2025-10-25 16:51:51 - INFO     - nat_alert_triage_agent:80 - ================================================Running in offline mode=================================================\n",
      "Running workflow:   0%|                                   | 0/7 [00:00<?, ?it/s]2025-10-25 16:51:53 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-0.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 16:51:53 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-1.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 16:51:53 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-2.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 16:51:53 - INFO     - nat_alert_triage_agent:258 - Host: [test-instance-3.example.com] is under maintenance according to the maintenance database\n",
      "2025-10-25 16:52:00 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-4.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 16:52:00 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-5.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 16:52:00 - INFO     - nat_alert_triage_agent:246 - Host: [test-instance-6.example.com] is NOT under maintenance according to the maintenance database\n",
      "2025-10-25 16:52:00 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  14%|███▊                       | 1/7 [00:06<00:41,  6.92s/it]2025-10-25 16:52:00 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "2025-10-25 16:52:00 - ERROR    - nat.builder.function:162 - Error with ainvoke in function with input: { \"alert_id\": 2, \"alert_name\": \"InstanceDown\", \"host_id\": \"test-instance-2.example.com\", \"severity\": \"critical\", \"description\": \"Instance test-instance-2.example.com is not available for scrapping for the last 5m. Please check: - instance is up and running; - monitoring service is in place and running; - network connectivity is ok\", \"summary\": \"Instance test-instance-2.example.com is down\", \"timestamp\": \"2025-04-28T05:00:00.000000\" }. Error: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}\n",
      "2025-10-25 16:52:00 - ERROR    - nat.runtime.runner:200 - Error running workflow: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}\n",
      "2025-10-25 16:52:00 - ERROR    - nat.eval.evaluate:183 - Failed to run the workflow: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/src/nat/eval/evaluate.py\", line 176, in run_one\n",
      "    base_output = await runner_result\n",
      "                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/src/nat/runtime/runner.py\", line 176, in result\n",
      "    result = await self._entry_fn.ainvoke(self._input_message, to_type=to_type)  # type: ignore\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/src/nat/builder/function.py\", line 153, in ainvoke\n",
      "    result = await self._ainvoke(converted_input)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/src/nat/builder/function.py\", line 329, in _ainvoke\n",
      "    return await self._ainvoke_fn(value)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/register.py\", line 147, in _response_fn\n",
      "    result = await _process_alert(input_message)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/src/nat/profiler/decorators/function_tracking.py\", line 191, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/register.py\", line 137, in _process_alert\n",
      "    output = await agent_executor.ainvoke({\"messages\": [HumanMessage(content=input_message)]})\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langgraph/pregel/main.py\", line 3112, in ainvoke\n",
      "    async for chunk in self.astream(\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langgraph/pregel/main.py\", line 2939, in astream\n",
      "    async for _ in runner.atick(\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langgraph/pregel/_runner.py\", line 295, in atick\n",
      "    await arun_with_retry(\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langgraph/pregel/_retry.py\", line 137, in arun_with_retry\n",
      "    return await task.proc.ainvoke(task.input, config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langgraph/_internal/_runnable.py\", line 706, in ainvoke\n",
      "    input = await asyncio.create_task(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.11_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.11_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
      "    future.result()\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.11_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.11_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langgraph/_internal/_runnable.py\", line 474, in ainvoke\n",
      "    ret = await self.afunc(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/examples/advanced_agents/alert_triage_agent/src/nat_alert_triage_agent/register.py\", line 101, in ata_assistant\n",
      "    return {\"messages\": [await llm_n_tools.ainvoke([sys_msg] + state[\"messages\"])]}\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 5723, in ainvoke\n",
      "    return await self.bound.ainvoke(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/src/nat/utils/exception_handlers/automatic_retries.py\", line 252, in _call_with_retry_async\n",
      "    return await fn(*args, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 417, in ainvoke\n",
      "    llm_result = await self.agenerate_prompt(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/src/nat/utils/exception_handlers/automatic_retries.py\", line 252, in _call_with_retry_async\n",
      "    return await fn(*args, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 1034, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/src/nat/utils/exception_handlers/automatic_retries.py\", line 252, in _call_with_retry_async\n",
      "    return await fn(*args, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 992, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.11_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py\", line 316, in __step_run_and_handle_result\n",
      "    result = coro.throw(exc)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 1162, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 1222, in _agenerate\n",
      "    return await run_in_executor(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langchain_core/runnables/config.py\", line 611, in run_in_executor\n",
      "    return await asyncio.get_running_loop().run_in_executor(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.11_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.11_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
      "    future.result()\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.11_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.11_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/thread.py\", line 59, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langchain_core/runnables/config.py\", line 602, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langchain_nvidia_ai_endpoints/chat_models.py\", line 490, in _generate\n",
      "    response = self._client.get_req(payload=payload, extra_headers=extra_headers)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langchain_nvidia_ai_endpoints/_common.py\", line 500, in get_req\n",
      "    response, session = self._post(\n",
      "                        ^^^^^^^^^^^\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langchain_nvidia_ai_endpoints/_common.py\", line 396, in _post\n",
      "    self._try_raise(response)\n",
      "  File \"/Users/bbednarski/.venvs/unew_312/lib/python3.12/site-packages/langchain_nvidia_ai_endpoints/_common.py\", line 489, in _try_raise\n",
      "    raise Exception(f\"{header}\\n{body}\") from None\n",
      "Exception: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}\n",
      "During task with name 'ata_assistant' and id 'c87e5a37-7616-53de-db33-2f27a0a0f948'\n",
      "Running workflow:  29%|███████▋                   | 2/7 [00:07<00:15,  3.02s/it]2025-10-25 16:52:00 - ERROR    - asyncio:1833 - Task exception was never retrieved\n",
      "future: <Task finished name='Task-64' coro=<Runner.result() done, defined at /Users/bbednarski/Projects/nat-getting-started-fork/NeMo-Agent-Toolkit/src/nat/runtime/runner.py:133> exception=RuntimeError('cannot reuse already awaited coroutine')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.11_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py\", line 316, in __step_run_and_handle_result\n",
      "    result = coro.throw(exc)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "RuntimeError: cannot reuse already awaited coroutine\n",
      "2025-10-25 16:52:46 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  43%|███████████▌               | 3/7 [00:53<01:30, 22.68s/it]2025-10-25 16:53:06 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  57%|███████████████▍           | 4/7 [01:13<01:05, 21.83s/it]2025-10-25 16:53:30 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  71%|███████████████████▎       | 5/7 [01:37<00:45, 22.60s/it]2025-10-25 16:53:50 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow:  86%|███████████████████████▏   | 6/7 [01:57<00:21, 21.63s/it]2025-10-25 16:54:09 - INFO     - nat_alert_triage_agent:150 - Finished agent execution\n",
      "Running workflow: 100%|███████████████████████████| 7/7 [02:16<00:00, 19.43s/it]\n",
      "Evaluating classification accuracy:   0%|                 | 0/7 [00:00<?, ?it/s]\n",
      "Evaluating Avg LLM Latency:   0%|                         | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Evaluating Ragas nv_accuracy:   0%|                       | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Evaluating classification accuracy:  14%|█▎       | 1/7 [00:00<00:01,  4.03it/s]\u001b[A\u001b[A\u001b[A\n",
      "Evaluating classification accuracy: 100%|█████████| 7/7 [00:00<00:00, 26.51it/s]\u001b[A\n",
      "Evaluating Avg LLM Latency: 100%|█████████████████| 7/7 [00:00<00:00, 26.52it/s]\n",
      "Evaluating Avg Tokens/LLM_END: 100%|█████████████| 7/7 [00:00<00:00, 366.09it/s]\n",
      "2025-10-25 16:54:09 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  14%|██▏            | 1/7 [00:00<00:04,  1.26it/s]\u001b[A\u001b[A2025-10-25 16:54:09 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "2025-10-25 16:54:10 - WARNING  - ragas.metrics._nv_metrics:157 - An error occurred: [429] Too Many Requests\n",
      "{'status': 429, 'title': 'Too Many Requests'}. Skipping a sample by assigning it nan score.\n",
      "\n",
      "\n",
      "Evaluating Ragas nv_accuracy:  57%|████████▌      | 4/7 [00:01<00:00,  4.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "Evaluating Ragas nv_accuracy: 100%|███████████████| 7/7 [00:01<00:00,  4.33it/s]\u001b[A\u001b[A\n",
      "2025-10-25 16:54:10 - INFO     - nat_alert_triage_agent:164 - Cleaning up\n",
      "2025-10-25 16:54:10 - INFO     - nat.eval.evaluate:252 - Profiler is not enabled. Skipping profiling.\n",
      "2025-10-25 16:54:10 - INFO     - nat.eval.evaluate:337 - Workflow output written to tmp_workflow/alert_triage_all_params_selection_output/workflow_output.json\n",
      "2025-10-25 16:54:10 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_all_params_selection_output/classification_accuracy_output.json\n",
      "2025-10-25 16:54:10 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_all_params_selection_output/llm_latency_output.json\n",
      "2025-10-25 16:54:10 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_all_params_selection_output/token_efficiency_output.json\n",
      "2025-10-25 16:54:10 - INFO     - nat.eval.evaluate:348 - Evaluation results written to tmp_workflow/alert_triage_all_params_selection_output/rag_accuracy_output.json\n",
      "2025-10-25 16:54:10 - WARNING  - nat.eval.evaluate:360 - Workflow execution was interrupted due to an error. The results may be incomplete. You can re-execute evaluation for incomplete results by running `eval` with the --skip_completed_entries flag.\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "# path-check-skip-next-line\n",
    "!nat eval --config_file ./tmp_workflow/alert_triage_all_params_selection_output/optimizer/optimized_config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "152cdd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy Results:\n",
      "Average Score: 43.00%\n",
      "\n",
      "Per-Alert Results:\n",
      "  Alert 0: Score=0.00 - The prediction network_connectivity is incorrect. (label: false_positive)\n",
      "  Alert 1: Score=1.00 - The prediction hardware is correct. (label: hardware)\n",
      "  Alert 2: Score=0.00 - The prediction  is incorrect. (label: software)\n",
      "  Alert 3: Score=0.00 - The prediction ## alert summary is incorrect. (label: maintenance)\n",
      "  Alert 4: Score=0.00 - The prediction hardware is incorrect. (label: software)\n",
      "  Alert 5: Score=1.00 - The prediction false_positive is correct. (label: false_positive)\n",
      "  Alert 6: Score=1.00 - The prediction repetitive_behavior is correct. (label: repetitive_behavior)\n",
      "\n",
      "\n",
      "RAG Accuracy Results:\n",
      "Average Score: 14.29%\n",
      "Total Alerts Evaluated: 7\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load and display classification accuracy results\n",
    "# path-check-skip-next-line\n",
    "with open('./tmp_workflow/alert_triage_all_params_selection_output/classification_accuracy_output.json') as f:\n",
    "    classification_results = json.load(f)\n",
    "\n",
    "print(\"Classification Accuracy Results:\")\n",
    "print(f\"Average Score: {classification_results['average_score']:.2%}\")\n",
    "print(\"\\nPer-Alert Results:\")\n",
    "for item in classification_results['eval_output_items']:\n",
    "    print(f\"  Alert {item['id']}: Score={item['score']:.2f} - {item['reasoning']}\")\n",
    "\n",
    "# Load and display RAG accuracy results\n",
    "# path-check-skip-next-line\n",
    "with open('./tmp_workflow/alert_triage_all_params_selection_output/rag_accuracy_output.json') as f:\n",
    "    rag_results = json.load(f)\n",
    "\n",
    "print(\"\\n\\nRAG Accuracy Results:\")\n",
    "print(f\"Average Score: {rag_results['average_score']:.2%}\")\n",
    "print(f\"Total Alerts Evaluated: {len(rag_results['eval_output_items'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c411a80a",
   "metadata": {},
   "source": [
    "Explain the results of prompt optimization..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4682debd",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20f6d61",
   "metadata": {},
   "source": [
    "<a id=\"next-steps\"></a>\n",
    "# 4.0) Next steps\n",
    "\n",
    "Continue learning how to fully utilize the NVIDIA NeMo Agent toolkit by exploring the other documentation and advanced agents in the `examples` directory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unew_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
