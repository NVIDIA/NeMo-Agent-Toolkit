{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "257c1153",
   "metadata": {},
   "source": [
    "## Model Selection and Parameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbf6a12",
   "metadata": {},
   "source": [
    "In this notebook, we will demonstrate how the NVIDIA NeMo Agent toolkit (NAT) optimizer can be used to create a robust model evaluation, comparison, and selection pipeline for custom datasets.\n",
    "\n",
    "**Goal**: exemplify the minimal configuration and demonstrate a practical example using the NAT optimizer module to evaluate and compare the performance of various LLMs. Help NAT users establish a working understanding of this feature so that they can create similar workflows in their own institutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328ee544",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    " \n",
    "- [0.0) Setup](#setup)\n",
    "  - [0.1) Prerequisites](#prereqs)\n",
    "  - [0.2) API Keys](#api-keys)\n",
    "  - [0.3) Installing NeMo Agent Toolkit](#install-nat)\n",
    "  - [0.4) Additional dependencies](#deps)\n",
    "- [1.0) LLM-as-a-judge with NAT](#llm-judge-h1)\n",
    "  - [1.1) Create a new workflow](#new-workflow)\n",
    "  - [1.2) Head-to-head comparison of multiple LLMs using eval](#nat-eval)\n",
    "    - [1.2.1) LLM-as-a-judge workflow config](#config)\n",
    "    - [1.2.2) Create an eval dataset](#dataset)\n",
    "    - [1.2.3) Run the optimizer](#optimize-first)\n",
    "    - [1.2.4) Interpret first optimizer run](#interpret-optimizer-first)\n",
    "- [2.0) Optimized model selection for tool-calling agents](#optimize-tool-calling-agents)\n",
    "  - [2.1) Create a tool-calling agent](#create-triage-agent)\n",
    "  - [2.2) Configure the tool-calling agent](#configure-triage-agent)\n",
    "  - [2.3) Test the tool-calling agent](#test-triage-agent)\n",
    "  - [2.4) Evaluate the tool-calling agent](#eval-triage-agent1)\n",
    "  - [2.5) Optimize the tool-calling agent's LLM](#optimize-triage-agent)\n",
    "  - [2.6) Re-evaluate the optimized tool-calling agent](#eval-triage-agent2)\n",
    "- [3.0) Next steps](#next-steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46297fd",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "# 0.0) Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8421ac5",
   "metadata": {},
   "source": [
    "<a id=\"prereqs\"></a>\n",
    "## 0.1) Prerequisites\n",
    "\n",
    "We strongly recommend that users begin this notebook with a working understanding of NAT workflows. Please refer to earlier iterations of this notebook series prior to beginning this notebook.\n",
    "\n",
    "- **Platform:** Linux, macOS, or Windows\n",
    "- **Python:** version 3.11, 3.12, or 3.13\n",
    "- **Python Packages:** `pip`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248a4ad7",
   "metadata": {},
   "source": [
    "<a id=\"api-keys\"></a>\n",
    "## 0.2) API Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b790034",
   "metadata": {},
   "source": [
    "For this notebook, you will need the following API keys to run all examples end-to-end:\n",
    "\n",
    "- **NVIDIA Build:** You can obtain an NVIDIA Build API Key by creating an [NVIDIA Build](https://build.nvidia.com) account and generating a key at https://build.nvidia.com/settings/api-keys\n",
    "\n",
    "Then you can run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ff151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"NVIDIA_API_KEY\" not in os.environ:\n",
    "    nvidia_api_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvidia_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d98208",
   "metadata": {},
   "source": [
    "<a id=\"install-nat\"></a>\n",
    "## 0.3) Installing NeMo Agent Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b6b321",
   "metadata": {},
   "source": [
    "The recommended way to install NAT is through `pip` or `uv pip`.\n",
    "\n",
    "First, we will install `uv` which offers parallel downloads and faster dependency resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8855c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install uv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1109b11",
   "metadata": {},
   "source": [
    "NeMo Agent toolkit can be installed through the PyPI `nvidia-nat` package.\n",
    "\n",
    "There are several optional subpackages available for NAT. For this example, we will rely on three subpackages:\n",
    "* The `nvidia-nat[langchain]` subpackage contains components for integrating with [LangChain](https://python.langchain.com/docs/introduction/).\n",
    "* The `nvidia-nat[profiling]` subpackage contains components for profiling and performance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc18e8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install \"nvidia-nat[langchain,profiling]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a34464",
   "metadata": {},
   "source": [
    "<a id=\"deps\"></a>\n",
    "## 0.4) Additional dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73082df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed for the alert triage agent used later\n",
    "!uv pip install ansible-runner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a71e5cc",
   "metadata": {},
   "source": [
    "<div style=\"color: red; font-style: italic;\">\n",
    "<strong>Note:</strong> Uncomment and run this cell to install git-lfs if using Google Colab.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95af680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt-get update\n",
    "# !apt-get install git git-lfs -y\n",
    "# !git lfs install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba3615a",
   "metadata": {},
   "source": [
    "<a id=\"llm-judge-h1\"></a>\n",
    "# 1.0) LLM-as-a-judge with NAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98db2f3a",
   "metadata": {},
   "source": [
    "The `nat eval` and `nat optimize` utilities enable developers to easily integrate LLM-as-a-judge capabilities with their workflows. `nat eval` allows for simple evaluations of a NAT workflow against an eval dataset. `nat optimize` extends this functionality by integrating with the **Optuna** library to perform grid and stochastic parameter sweeps and evaluations to identify optimal configurations for a task.\n",
    "\n",
    "**Note:** _In this notebook, we will primarily demonstrate how to use `nat optimize` to identify a potentially optimal set of parameters for a NAT workflow. It is assumed that users will already have a strong understanding of ML model evaluations before building this concept into their workflows - as we will not be covering cross validation and train, validation, and test splitting of datasets. Please refer to python's [SciKit-Learn](https://scikit-learn.org/stable/) package as a strong reference for these concepts._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5114d358",
   "metadata": {},
   "source": [
    "<a id=\"new-workflow\"></a>\n",
    "## 1.1) Create a new workflow\n",
    "\n",
    "Create a basic chat completions workflow (using LangChain chat completions on backend)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94f46ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nat workflow create tmp_workflow --description \"A simple chat completion workflow to compare model performance\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c757d63",
   "metadata": {},
   "source": [
    "Let's look at the default configuration of this agent and confirm the agent type, LLMs, tool calls, and functions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53d5365",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./tmp_workflow/configs/config_a.yml\n",
    "llms:\n",
    "  nim_llm:\n",
    "    _type: nim\n",
    "    model_name: meta/llama-3.1-8b-instruct\n",
    "    temperature: 0.7\n",
    "    max_tokens: 1024\n",
    "\n",
    "workflow:\n",
    "  _type: chat_completion  # Use the type directly\n",
    "  system_prompt: |\n",
    "    You are a helpful AI assistant. Provide clear, accurate, and helpful \n",
    "    responses to user queries. Be concise and informative.\n",
    "  llm_name: nim_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1a0afc",
   "metadata": {},
   "source": [
    "Now let's run this workflow for a simple Q&A example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6510270",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nat run --config_file tmp_workflow/configs/config_a.yml --input \"Suggest a single name for my new dog\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a740010",
   "metadata": {},
   "source": [
    "<a id=\"nat-eval\"></a>\n",
    "## 1.2) Head-to-head comparison of multiple LLMs using eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba364eb",
   "metadata": {},
   "source": [
    "Now that we've made a new workflow and shown that it works for a cursory `nat run` example, we will begin to build out an LLM-as-a-judge evaluation with trace profiling enabled for additional observability. In this next section, we are going to update the workflow configuration for evaluation and profiling.\n",
    "\n",
    "Step-by-step instructions can be found in [4_observability_evaluation_and_profiling.ipynb](./4_observability_evaluation_and_profiling.ipynb). An end-to-end example of using the Optimizer can be viewed in the [Email Phishing Analyzer](https://github.com/NVIDIA/NeMo-Agent-Toolkit/blob/develop/examples/evaluation_and_profiling/email_phishing_analyzer/src/nat_email_phishing_analyzer/configs/config_optimizer.yml).\n",
    "\n",
    "The profiler instruments and measures your workflow's performance, while evaluators judge the quality of the outputs. They're separate concepts, so they belong in different sections of the config!\n",
    "\n",
    "In this next step we will combine the eval and profile configuration into a single config for brevity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f222012b",
   "metadata": {},
   "source": [
    "<a id=\"config\"></a>\n",
    "### 1.2.1) LLM-as-a-judge workflow config\n",
    "\n",
    "In the cell below we edit our initial workflow configuration to include `eval` and `optimizer` configurations.\n",
    "\n",
    "Key components of this configuration:\n",
    "\n",
    "**LLM Configuration:**\n",
    "- `chat_completion_llm`: The backbone LLM that powers the workflow\n",
    "- `optimizable_params`: Specifies which parameters the optimizer can tune (model name, temperature)\n",
    "- `search_space`: Defines the values the optimizer will explore during optimization\n",
    "\n",
    "**Judge LLM:**\n",
    "- `nim_judge_llm`: A separate, more capable LLM (meta/llama-3.1-405b-instruct) used by the evaluator to assess the quality of the workflow's outputs\n",
    "  - This LLM acts as an \"LLM-as-a-judge\" to score responses\n",
    "\n",
    "**Evaluation Components:**\n",
    "- `evaluators`: Define metrics to measure workflow quality (for example, accuracy, relevance)\n",
    "- `profiler`: Instruments the workflow to collect performance metrics (latency, token usage, costs)\n",
    "\n",
    "**Optimizer Components:**\n",
    "- `reps_per_param_set`: Number of times to evaluate each parameter combination for statistical reliability\n",
    "- `grid_search`: Strategy for exploring the search space (tests all combinations)\n",
    "- `eval_metrics`: Metrics used to guide optimization decisions (for example, maximize accuracy while minimizing cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f354066",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tmp_workflow/configs/config_b.yml\n",
    "llms:\n",
    "  chat_completion_llm:\n",
    "    _type: nim\n",
    "    model_name: meta/llama-3.1-8b-instruct\n",
    "    temperature: 0.0\n",
    "    max_tokens: 1024\n",
    "    optimizable_params:\n",
    "      - model_name\n",
    "      - temperature\n",
    "    search_space:\n",
    "      model_name:\n",
    "        values:\n",
    "          - meta/llama-3.1-8b-instruct\n",
    "          - meta/llama-3.1-70b-instruct\n",
    "      temperature:\n",
    "        values:\n",
    "          - 0.0\n",
    "          - 0.7\n",
    "\n",
    "  # Judge LLM for accuracy evaluation\n",
    "  nim_judge_llm:\n",
    "    _type: nim\n",
    "    model_name: meta/llama-3.1-405b-instruct\n",
    "    temperature: 0.0\n",
    "    max_tokens: 8  # RAGAS accuracy only needs a score (0-1)\n",
    "\n",
    "workflow:\n",
    "  _type: chat_completion\n",
    "  system_prompt: |\n",
    "    You are a helpful AI assistant. Provide clear, accurate, and helpful \n",
    "    responses to user queries. Be concise and informative.\n",
    "  llm_name: chat_completion_llm\n",
    "\n",
    "general:\n",
    "  telemetry:\n",
    "    logging:\n",
    "      console:\n",
    "        _type: console\n",
    "        level: INFO\n",
    "\n",
    "eval:\n",
    "  general:\n",
    "    output_dir: ./tmp_workflow/eval_output\n",
    "    verbose: true\n",
    "    dataset:\n",
    "        _type: json\n",
    "        file_path: ./tmp_workflow/data/eval_data.json\n",
    "\n",
    "  evaluators:\n",
    "    answer_accuracy:\n",
    "      _type: ragas\n",
    "      metric: AnswerAccuracy\n",
    "      llm_name: nim_judge_llm\n",
    "    llm_latency:\n",
    "      _type: avg_llm_latency\n",
    "    token_efficiency:\n",
    "      _type: avg_tokens_per_llm_end\n",
    "\n",
    "  profiler:\n",
    "      token_uniqueness_forecast: true\n",
    "      workflow_runtime_forecast: true\n",
    "      compute_llm_metrics: true\n",
    "      csv_exclude_io_text: true\n",
    "      prompt_caching_prefixes:\n",
    "        enable: true\n",
    "        min_frequency: 0.1\n",
    "      bottleneck_analysis:\n",
    "        enable_nested_stack: true\n",
    "      concurrency_spike_analysis:\n",
    "        enable: true\n",
    "        spike_threshold: 7\n",
    "\n",
    "optimizer:\n",
    "  output_path: ./tmp_workflow/eval_output/optimizer/\n",
    "  reps_per_param_set: 10 # Number of times to evaluate EACH config (for statistical significance)\n",
    "  eval_metrics:\n",
    "    accuracy:\n",
    "      evaluator_name: answer_accuracy  # References the evaluator above\n",
    "      direction: maximize\n",
    "    token_efficiency:\n",
    "      evaluator_name: token_efficiency\n",
    "      direction: minimize\n",
    "    latency:\n",
    "      evaluator_name: llm_latency\n",
    "      direction: minimize\n",
    "\n",
    "  numeric:\n",
    "    enabled: true\n",
    "    sampler: grid # determines the number of trials to run for each parameter set\n",
    "\n",
    "  prompt:\n",
    "    enabled: false  # Disable for pure model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692dfb0b",
   "metadata": {},
   "source": [
    "<a id=\"dataset\"></a>\n",
    "### 1.2.2) Create an eval dataset\n",
    "\n",
    "The dataset below is intended to be difficult for simple LLM chat completions, because:\n",
    "- Math calculations (questions 1, 2, 5, 7, 9) require precise arithmetic that LLMs often struggle with\n",
    "- Real-time data queries (questions 3, 8) need current information beyond the model's training cutoff\n",
    "- Factual knowledge (questions 4, 6) may be outdated or incorrect without access to recent data\n",
    "- Multi-step reasoning (questions 2, 7) requires combining multiple operations accurately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c388ff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tmp_workflow/data/eval_data.json\n",
    "[\n",
    "    {\n",
    "        \"id\": \"1\",\n",
    "        \"question\": \"What is 15% of 847?\",\n",
    "        \"answer\": \"The answer is 127.05\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"2\", \n",
    "        \"question\": \"If I invest $10,000 at 5% annual interest compounded monthly for 3 years, how much will I have?\",\n",
    "        \"answer\": \"Approximately $11,614.72\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"3\",\n",
    "        \"question\": \"What is the current weather in Tokyo?\",\n",
    "        \"answer\": \"This requires real-time weather data for Tokyo, Japan.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"4\",\n",
    "        \"question\": \"Who won the FIFA World Cup in 2022 and where was it held?\",\n",
    "        \"answer\": \"Argentina won the 2022 FIFA World Cup, which was held in Qatar.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"5\",\n",
    "        \"question\": \"Calculate the average of these numbers: 23, 45, 67, 89, 12, 34\",\n",
    "        \"answer\": \"The average is 45\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"6\",\n",
    "        \"question\": \"What is the capital of Australia and what is its approximate population?\",\n",
    "        \"answer\": \"Canberra is the capital of Australia with a population of approximately 460,000 people.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"7\",\n",
    "        \"question\": \"If a train travels 120 miles in 2 hours, then 180 miles in 3 hours, what is its average speed over the entire journey?\",\n",
    "        \"answer\": \"The average speed is 60 miles per hour (300 miles / 5 hours).\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"8\",\n",
    "        \"question\": \"Search for information about the latest NASA Mars mission and summarize the key findings.\",\n",
    "        \"answer\": \"Requires web search for current NASA Mars mission information and synthesis of findings.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"9\",\n",
    "        \"question\": \"What is 2 to the power of 10?\",\n",
    "        \"answer\": \"1024\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"10\",\n",
    "        \"question\": \"Who is the current CEO of Microsoft and when did they take the position?\",\n",
    "        \"answer\": \"Satya Nadella has been CEO of Microsoft since February 2014.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"11\",\n",
    "        \"question\": \"Convert 100 degrees Fahrenheit to Celsius and then to Kelvin.\",\n",
    "        \"answer\": \"100°F = 37.78°C = 310.93K\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"12\",\n",
    "        \"question\": \"Find the top 3 most popular programming languages in 2024 according to recent developer surveys.\",\n",
    "        \"answer\": \"Requires web search for recent programming language popularity surveys from 2024.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"13\",\n",
    "        \"question\": \"What is the square root of 289?\",\n",
    "        \"answer\": \"17\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"14\",\n",
    "        \"question\": \"If I start with $1000 and lose 20%, then gain 20% on the new amount, how much do I have?\",\n",
    "        \"answer\": \"$960 (First: $1000 - 20% = $800, Then: $800 + 20% = $960)\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"15\",\n",
    "        \"question\": \"What are the main differences between Python 3.11 and Python 3.12? Search for official documentation.\",\n",
    "        \"answer\": \"Requires web search for Python 3.12 release notes and feature comparison.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"16\",\n",
    "        \"question\": \"Calculate: (15 + 25) × 3 - 48 ÷ 6\",\n",
    "        \"answer\": \"112\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"17\",\n",
    "        \"question\": \"What is the chemical formula for water and what are its key properties?\",\n",
    "        \"answer\": \"H2O. Key properties include: polar molecule, high specific heat capacity, excellent solvent, density maximum at 4°C.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"18\",\n",
    "        \"question\": \"How many days are there between January 15, 2024 and March 30, 2024?\",\n",
    "        \"answer\": \"75 days\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"19\",\n",
    "        \"question\": \"Search for the latest NVIDIA GPU announcement and tell me the model name and key specifications.\",\n",
    "        \"answer\": \"Requires web search for recent NVIDIA GPU announcements.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"20\",\n",
    "        \"question\": \"If a rectangle has a length of 12 cm and a width of 8 cm, what is its area and perimeter?\",\n",
    "        \"answer\": \"Area: 96 cm², Perimeter: 40 cm\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b71b0b",
   "metadata": {},
   "source": [
    "<a id=\"optimize-first\"></a>\n",
    "### 1.2.3) Run the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71420933",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nat optimize --config_file tmp_workflow/configs/config_b.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f79ce00",
   "metadata": {},
   "source": [
    "<a id=\"interpret-optimizer-first\"></a>\n",
    "### 1.2.4) Interpret first optimizer run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6029eeeb",
   "metadata": {},
   "source": [
    "**Understanding Evaluation Outputs**\n",
    "\n",
    "This evaluation will have generated two artifacts for analysis at the `output_dir` specified in `config_b.yml`:\n",
    " - **`answer_accuracy_output.json`**\n",
    " - **`workflow_output.json`**\n",
    " - **`llm_latency_output.json`**\n",
    " - **`token_efficiency_output.json`**\n",
    "\n",
    "**Interpreting `trajectory_accuracy_output.json`**\n",
    "\n",
    "The `trajectory_accuracy_output.json` file contains the results of agent trajectory evaluation.\n",
    "\n",
    "**Top-level fields:**\n",
    "- **`average_score`** - Mean trajectory accuracy score across all evaluated examples (0.0 to 1.0)\n",
    "- **`eval_output_items`** - Array of individual evaluation results for each test case\n",
    "\n",
    "**Per-item fields:**\n",
    "- **`id`** - Unique identifier for the test case\n",
    "- **`score`** - Trajectory accuracy score for this specific example (0.0 to 1.0)\n",
    "- **`reasoning`** - Evaluation reasoning, either:\n",
    "  - String containing error message if evaluation failed\n",
    "  - Object with:\n",
    "    - **`reasoning`** - LLM judge's explanation of the score\n",
    "    - **`trajectory`** - Array of [AgentAction, Output] pairs showing the agent's execution path\n",
    "\n",
    "The trajectory accuracy evaluator assesses whether the agent used appropriate tools, followed a logical sequence of steps, and efficiently reached the correct answer.\n",
    "\n",
    "**Interpreting `workflow_output.json`**\n",
    "\n",
    "The `workflow_output.json` file contains the raw execution results from running the workflow on each test case.\n",
    "\n",
    "**Top-level fields:**\n",
    "- **`output_items`** - Array of workflow execution results for each test case in the dataset\n",
    "\n",
    "**Per-item fields:**\n",
    "- **`id`** - Unique identifier matching the test case ID\n",
    "- **`input_obj`** - The input question or prompt sent to the workflow\n",
    "- **`output_obj`** - The final answer generated by the workflow\n",
    "- **`trajectory`** - Detailed execution trace containing:\n",
    "  - **`event_type`** - Type of event (e.g., `LLM_START`, `LLM_END`, `TOOL_START`, `TOOL_END`, `SPAN_START`, `SPAN_END`)\n",
    "  - **`event_timestamp`** - Unix timestamp of when the event occurred\n",
    "  - **`metadata`** - Event-specific data including:\n",
    "    - Tool names and inputs\n",
    "    - LLM prompts and responses\n",
    "    - Token counts (`prompt_tokens`, `completion_tokens`)\n",
    "    - Model names\n",
    "    - Function names\n",
    "    - Error information\n",
    "\n",
    "The workflow output provides complete observability into each execution, enabling detailed analysis of agent behavior, performance profiling, and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab620774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import ast\n",
    "\n",
    "# Load the optimizer results\n",
    "trials_df_path = Path(\"tmp_workflow/eval_output/optimizer/trials_dataframe_params.csv\")\n",
    "\n",
    "if trials_df_path.exists():\n",
    "    trials_df = pd.read_csv(trials_df_path)\n",
    "    \n",
    "    print(\"Grid Search Optimization Results\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nTrials Summary:\")\n",
    "    print(trials_df.to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"\\nModel Performance Statistics (Mean across repetitions):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Group by model name to calculate statistics across repetitions\n",
    "    for model_name in trials_df['params_llms.chat_completion_llm.model_name'].unique():\n",
    "        model_trials = trials_df[trials_df['params_llms.chat_completion_llm.model_name'] == model_name]\n",
    "        \n",
    "        print(f\"\\n{model_name}:\")\n",
    "        \n",
    "        # Parse rep_scores to extract individual repetition metrics\n",
    "        if 'rep_scores' in model_trials.columns:\n",
    "            all_accuracies = []\n",
    "            all_token_efficiencies = []\n",
    "            all_latencies = []\n",
    "            \n",
    "            for rep_scores_str in model_trials['rep_scores']:\n",
    "                try:\n",
    "                    rep_scores = ast.literal_eval(str(rep_scores_str))\n",
    "                except (ValueError, SyntaxError, TypeError):\n",
    "                    continue\n",
    "                for score_set in rep_scores:\n",
    "                    # score_set format: [accuracy, token_efficiency, latency]\n",
    "                    all_accuracies.append(score_set[0])\n",
    "                    all_token_efficiencies.append(score_set[1])\n",
    "                    all_latencies.append(score_set[2])\n",
    "            \n",
    "            # Calculate mean and standard deviation\n",
    "            def calculate_stats(values):\n",
    "                mean = np.mean(values)\n",
    "                std = np.std(values)\n",
    "                ci_lower = np.percentile(values, 2.5)\n",
    "                ci_upper = np.percentile(values, 97.5)\n",
    "                return mean, std, ci_lower, ci_upper\n",
    "            \n",
    "            acc_mean, acc_std, acc_ci_lower, acc_ci_upper = calculate_stats(all_accuracies)\n",
    "            tok_mean, tok_std, tok_ci_lower, tok_ci_upper = calculate_stats(all_token_efficiencies)\n",
    "            lat_mean, lat_std, lat_ci_lower, lat_ci_upper = calculate_stats(all_latencies)\n",
    "            \n",
    "            print(f\"  Accuracy:\")\n",
    "            print(f\"    Mean: {acc_mean:.3f} (±{acc_std:.3f})\")\n",
    "            print(f\"    95% CI: [{acc_ci_lower:.3f}, {acc_ci_upper:.3f}]\")\n",
    "            \n",
    "            print(f\"  Token Efficiency:\")\n",
    "            print(f\"    Mean: {tok_mean:.3f} (±{tok_std:.3f})\")\n",
    "            print(f\"    95% CI: [{tok_ci_lower:.3f}, {tok_ci_upper:.3f}]\")\n",
    "            \n",
    "            print(f\"  Latency:\")\n",
    "            print(f\"    Mean: {lat_mean:.3f} (±{lat_std:.3f})\")\n",
    "            print(f\"    95% CI: [{lat_ci_lower:.3f}, {lat_ci_upper:.3f}]\")\n",
    "        else:\n",
    "            # Fallback to aggregated values if rep_scores not available\n",
    "            # values_0 = accuracy, values_1 = token_efficiency, values_2 = latency\n",
    "            acc_mean = np.mean(model_trials['values_0'])\n",
    "            tok_mean = np.mean(model_trials['values_1'])\n",
    "            lat_mean = np.mean(model_trials['values_2'])\n",
    "            \n",
    "            print(f\"  Accuracy (mean): {acc_mean:.3f}\")\n",
    "            print(f\"  Token Efficiency (mean): {tok_mean:.3f}\")\n",
    "            print(f\"  Latency (mean): {lat_mean:.3f}\")\n",
    "            print(\"  Note: 95% CI not available without rep_scores data\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"\\nBest Configuration (by aggregated accuracy across all repetitions):\")\n",
    "    # Find the trial with best aggregated accuracy\n",
    "    best_trial = trials_df.loc[trials_df['values_0'].idxmax()]\n",
    "    print(f\"Model: {best_trial['params_llms.chat_completion_llm.model_name']}\")\n",
    "    print(f\"Temperature: {best_trial['params_llms.chat_completion_llm.temperature']}\")\n",
    "    print(f\"Aggregated Accuracy Score: {best_trial['values_0']}\")\n",
    "    print(f\"Aggregated Token Efficiency: {best_trial['values_1']}\")\n",
    "    print(f\"Aggregated Latency: {best_trial['values_2']}\")\n",
    "else:\n",
    "    print(f\"Optimizer results not found at {trials_df_path}\")\n",
    "    print(\"Please run the optimizer first (cell 40)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3836ec19",
   "metadata": {},
   "source": [
    "The results above show:\n",
    " \n",
    "**Grid Search Optimization Summary:**\n",
    "- The optimizer evaluated all combinations of models and temperatures defined in the search space\n",
    "- Each configuration was tested multiple times (repetitions) to account for variability\n",
    "- Three key metrics were tracked: accuracy, token efficiency (tokens used), and latency (response time)\n",
    " \n",
    " **Understanding the Statistics:**\n",
    "- **Mean**: Average performance across all repetitions for each model\n",
    "- **Standard Deviation (±)**: Measure of variability in performance\n",
    "- **95% Confidence Interval**: Range where we expect 95% of results to fall\n",
    "\n",
    "**Key Insights:**\n",
    " - Different models show different trade-offs between accuracy, efficiency, and speed\n",
    "- Temperature settings affect response variability and quality\n",
    "- The \"Best Configuration\" represents the optimal balance based on the weighted combination of all metrics\n",
    " \n",
    "**Interpreting Your Results:**\n",
    "When you run this optimization, look for:\n",
    "- Which model/temperature combination achieves the highest aggregated accuracy\n",
    "- How token efficiency varies between models (lower is more efficient)\n",
    "- Latency differences (lower is faster)\n",
    "- The confidence intervals to understand result stability\n",
    "\n",
    "The optimizer automatically selects the best configuration and saves it to `optimized_config.yml` for use in production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59876571",
   "metadata": {},
   "source": [
    "<a id=\"optimize-tool-calling-agents\"></a>\n",
    "# 2.0) Optimized model selection for tool-calling agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7223a3b2",
   "metadata": {},
   "source": [
    "<a id=\"create-triage-agent\"></a>\n",
    "# 2.1) Create a tool-calling agent\n",
    "As we explained above, in many real-world applications straightforward chat completions requests may not be adequate without agentic tool-calling integration. Therefore, for the next exercise we are going to build a similar optimize pipeline for an advanced tool calling agent: the [Alert Triage Agent](https://github.com/NVIDIA/NeMo-Agent-Toolkit/tree/develop/examples/advanced_agents/alert_triage_agent). This agent uses tool calling to automate the triage of server-monitoring alerts. It demonstrates how to build an intelligent troubleshooting workflow using NeMo Agent toolkit and LangGraph.\n",
    "\n",
    "The Alert Triage Agent is an advanced example that demonstrates:\n",
    "- **Multi-tool orchestration** - Dynamically selects and uses diagnostic tools\n",
    "- **Structured report generation** - Creates comprehensive analysis reports\n",
    "- **Root cause categorization** - Classifies alerts into predefined categories\n",
    "- **Offline evaluation mode** - Test with synthetic data before live deployment\n",
    "\n",
    "We aim to demonstrate the power of model evaluation and optimization on agentic AI platforms. There are many foundational models to choose as your agent's backbone and academic benchmarks are not always representative of potential performance on your institutional data (refer to training data leakage and data domain shift research for more motivation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a101122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple input prompt for branch selection\n",
    "print(\"=\" * 60)\n",
    "print(\"Alert Triage Agent Installation\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nOptions:\")\n",
    "print(\"  - Enter 'local' for editable install from local repository\")\n",
    "print(\"  - Enter a branch name (e.g., 'develop', 'main') for git install\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "branch_name = input(\"\\nEnter your choice: \").strip()\n",
    "\n",
    "if branch_name.lower() == 'local':\n",
    "    # Local editable install\n",
    "    print(\"\\nInstalling alert triage agent in editable mode from local repository...\")\n",
    "    \n",
    "    # Try to find the local path relative to current directory\n",
    "    from pathlib import Path\n",
    "    # path-check-skip-next-line\n",
    "    local_path = Path('../../examples/advanced_agents/alert_triage_agent')\n",
    "    \n",
    "    if local_path.exists():\n",
    "        get_ipython().system(f'pip install -e {local_path}')\n",
    "        print(f\"✓ Installed from local path: {local_path.absolute()}\")\n",
    "    else:\n",
    "        print(f\"✗ Error: Local path not found: {local_path.absolute()}\")\n",
    "        print(\"Make sure you're running this from the correct directory\")\n",
    "else:\n",
    "    # Git install from specified branch\n",
    "    print(f\"\\nInstalling alert triage agent from branch: {branch_name}\")\n",
    "    get_ipython().system(f'pip install --no-deps \"git+https://github.com/NVIDIA/NeMo-Agent-Toolkit.git@{branch_name}#subdirectory=examples/advanced_agents/alert_triage_agent\"')\n",
    "    print(f\"✓ Installed from git branch: {branch_name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fc34f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.resources\n",
    "from pathlib import Path\n",
    "\n",
    "# Find the installed package data directory\n",
    "package_data = importlib.resources.files('nat_alert_triage_agent').joinpath('data')\n",
    "\n",
    "maintenance_csv = str(package_data / 'maintenance_static_dataset.csv')\n",
    "offline_csv = str(package_data / 'offline_data.csv')\n",
    "benign_json = str(package_data / 'benign_fallback_offline_data.json')\n",
    "offline_json = str(package_data / 'offline_data.json')\n",
    "\n",
    "print(f\"Package data directory: {package_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc40fd05",
   "metadata": {},
   "source": [
    "<a id=\"configure-triage-agent\"></a>\n",
    "## 2.2) Configure the tool-calling agent\n",
    "\n",
    "**Configuring the Alert Triage Agent**\n",
    "\n",
    "The Alert Triage Agent requires several components:\n",
    "\n",
    "1. **Diagnostic Tools** - Hardware checks, network connectivity, performance monitoring, telemetry analysis\n",
    "2. **Sub-agents** - Telemetry metrics analysis agent that coordinates multiple telemetry tools\n",
    "3. **Categorizer** - Classifies root causes into predefined categories\n",
    "4. **Maintenance Check** - Filters out alerts during maintenance windows\n",
    "\n",
    "We'll create a **local configuration file** and run in **offline mode** using synthetic data.\n",
    "\n",
    "In the configuration file, you can see the list of LLMs that we have predefined to be compared when the optimizer runs. These 11 models will each be used as the agents backbone LLM for reasoning steps. The `tool_reasoning_llm` and `nim_rag_eval_llm` remain fixed to `meta/llama-3.1-70b-instruct`, but in a modified evaluation these models could be evaluated as well. \n",
    "```\n",
    "- Meta: llama-3.1-8b-instruct\n",
    "- Meta: llama-3.1-70b-instruct\n",
    "- Meta: llama-3.1-405b-instruct\n",
    "- Meta: llama-3.3-3b-instruct\n",
    "- Meta: llama-3.3-70b-instruct\n",
    "- Meta: llama-4-scout-17b-16e-instruct\n",
    "- OpenAI: gpt-oss-20b\n",
    "- OpenAI: gpt-oss-120b\n",
    "- IBM: granite-3.3-8b-instruct\n",
    "- MistralAI: mistral-small-3.1-24b-instruct-2503\n",
    "- MistralAI: mistral-medium-3-instruct\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f1940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.resources\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "package_data = importlib.resources.files('nat_alert_triage_agent').joinpath('data')\n",
    "\n",
    "# Create config dictionary\n",
    "config = {\n",
    "    'functions': {\n",
    "        'hardware_check': {\n",
    "            '_type': 'hardware_check',\n",
    "            'llm_name': 'tool_reasoning_llm',\n",
    "            'offline_mode': True\n",
    "        },\n",
    "        'host_performance_check': {\n",
    "            '_type': 'host_performance_check',\n",
    "            'llm_name': 'tool_reasoning_llm',\n",
    "            'offline_mode': True\n",
    "        },\n",
    "        'monitoring_process_check': {\n",
    "            '_type': 'monitoring_process_check',\n",
    "            'llm_name': 'tool_reasoning_llm',\n",
    "            'offline_mode': True\n",
    "        },\n",
    "        'network_connectivity_check': {\n",
    "            '_type': 'network_connectivity_check',\n",
    "            'llm_name': 'tool_reasoning_llm',\n",
    "            'offline_mode': True\n",
    "        },\n",
    "        'telemetry_metrics_host_heartbeat_check': {\n",
    "            '_type': 'telemetry_metrics_host_heartbeat_check',\n",
    "            'llm_name': 'tool_reasoning_llm',\n",
    "            'offline_mode': True\n",
    "        },\n",
    "        'telemetry_metrics_host_performance_check': {\n",
    "            '_type': 'telemetry_metrics_host_performance_check',\n",
    "            'llm_name': 'tool_reasoning_llm',\n",
    "            'offline_mode': True\n",
    "        },\n",
    "        'telemetry_metrics_analysis_agent': {\n",
    "            '_type': 'telemetry_metrics_analysis_agent',\n",
    "            'tool_names': [\n",
    "                'telemetry_metrics_host_heartbeat_check',\n",
    "                'telemetry_metrics_host_performance_check'\n",
    "            ],\n",
    "            'llm_name': 'agent_llm'\n",
    "        },\n",
    "        'maintenance_check': {\n",
    "            '_type': 'maintenance_check',\n",
    "            'llm_name': 'agent_llm',\n",
    "            'static_data_path': str(package_data / 'maintenance_static_dataset.csv')\n",
    "        },\n",
    "        'categorizer': {\n",
    "            '_type': 'categorizer',\n",
    "            'llm_name': 'agent_llm'\n",
    "        }\n",
    "    },\n",
    "    'workflow': {\n",
    "        '_type': 'alert_triage_agent',\n",
    "        'tool_names': [\n",
    "            'hardware_check',\n",
    "            'host_performance_check',\n",
    "            'monitoring_process_check',\n",
    "            'network_connectivity_check',\n",
    "            'telemetry_metrics_analysis_agent'\n",
    "        ],\n",
    "        'llm_name': 'agent_llm',\n",
    "        'offline_mode': True,\n",
    "        'offline_data_path': str(package_data / 'offline_data.csv'),\n",
    "        'benign_fallback_data_path': str(package_data / 'benign_fallback_offline_data.json')\n",
    "    },\n",
    "    'llms': {\n",
    "        'agent_llm': {\n",
    "            '_type': 'nim',\n",
    "            'model_name': 'meta/llama-3.1-8b-instruct',\n",
    "            'temperature': 0.0,\n",
    "            'max_tokens': 2048,\n",
    "            'optimizable_params': ['model_name'],\n",
    "            'search_space': {\n",
    "                'model_name': {\n",
    "                    'values': [\n",
    "                        # path-check-skip-next-line\n",
    "                        'meta/llama-3.1-8b-instruct',\n",
    "                        # path-check-skip-next-line\n",
    "                        'meta/llama-3.1-70b-instruct',\n",
    "                        # path-check-skip-next-line\n",
    "                        'meta/llama-3.1-405b-instruct',\n",
    "                        # path-check-skip-next-line\n",
    "                        'meta/llama-3.3-3b-instruct',\n",
    "                        # path-check-skip-next-line\n",
    "                        'meta/llama-3.3-70b-instruct',\n",
    "                        # path-check-skip-next-line\n",
    "                        'meta/llama-4-scout-17b-16e-instruct',\n",
    "                        # path-check-skip-next-line\n",
    "                        'openai/gpt-oss-20b',\n",
    "                        # path-check-skip-next-line\n",
    "                        'openai/gpt-oss-120b',\n",
    "                        # path-check-skip-next-line\n",
    "                        'ibm/granite-3.3-8b-instruct',\n",
    "                        # path-check-skip-next-line\n",
    "                        'mistralai/mistral-small-3.1-24b-instruct-2503',\n",
    "                        # path-check-skip-next-line\n",
    "                        'mistralai/mistral-medium-3-instruct'\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        'tool_reasoning_llm': {\n",
    "            '_type': 'nim',\n",
    "            'model_name': 'meta/llama-3.1-70b-instruct',\n",
    "            'temperature': 0.2,\n",
    "            'max_tokens': 2048\n",
    "        },\n",
    "        'nim_rag_eval_llm': {\n",
    "            '_type': 'nim',\n",
    "            'model_name': 'meta/llama-3.1-70b-instruct',\n",
    "            'max_tokens': 8\n",
    "        }\n",
    "    },\n",
    "    'eval': {\n",
    "        'general': {\n",
    "            # path-check-skip-next-line\n",
    "            'output_dir': './tmp_workflow/alert_triage_output/',\n",
    "            'dataset': {\n",
    "                '_type': 'json',\n",
    "                'file_path': str(package_data / 'offline_data.json')\n",
    "            }\n",
    "        },\n",
    "        'evaluators': {\n",
    "            'classification_accuracy': {\n",
    "                '_type': 'classification_accuracy'\n",
    "            },\n",
    "            'rag_accuracy': {\n",
    "                '_type': 'ragas',\n",
    "                'metric': 'AnswerAccuracy',\n",
    "                'llm_name': 'nim_rag_eval_llm'\n",
    "            }\n",
    "        },\n",
    "        'profiler': {\n",
    "            'token_uniqueness_forecast': True,\n",
    "            'workflow_runtime_forecast': True,\n",
    "            'compute_llm_metrics': True,\n",
    "            'csv_exclude_io_text': True,\n",
    "            'prompt_caching_prefixes': {\n",
    "                'enable': True,\n",
    "                'min_frequency': 0.1\n",
    "            },\n",
    "            'bottleneck_analysis': {\n",
    "                'enable_nested_stack': True\n",
    "            },\n",
    "            'concurrency_spike_analysis': {\n",
    "                'enable': True,\n",
    "                'spike_threshold': 7\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'optimizer': {\n",
    "        # path-check-skip-next-line\n",
    "        'output_path': './tmp_workflow/alert_triage_output/optimizer/',\n",
    "        'reps_per_param_set': 1,\n",
    "        'eval_metrics': {\n",
    "            'classification_accuracy': {\n",
    "                'evaluator_name': 'classification_accuracy',\n",
    "                'direction': 'maximize'\n",
    "            },\n",
    "            'rag_accuracy': {\n",
    "                'evaluator_name': 'rag_accuracy',\n",
    "                'direction': 'maximize'\n",
    "            }\n",
    "        },\n",
    "        'numeric': {\n",
    "            'enabled': True,\n",
    "            'sampler': 'grid'\n",
    "        },\n",
    "        'prompt': {\n",
    "            'enabled': False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Write to file\n",
    "Path('./tmp_workflow/configs').mkdir(parents=True, exist_ok=True)\n",
    "with open('./tmp_workflow/configs/alert_triage_config.yml', 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"Config written with data paths from: {package_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299df6c9",
   "metadata": {},
   "source": [
    "<a id=\"test-triage-agent\"></a>\n",
    "## 2.3) Test the tool-calling agent\n",
    "\n",
    "Let's test the Alert Triage Agent with a single alert. This alert is an \"InstanceDown\" alert that, according to the offline dataset, is actually a false positive (the system is healthy).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b468a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nat run --config_file tmp_workflow/configs/alert_triage_config.yml \\\n",
    "  --input '{\"alert_id\": 0, \\\n",
    "           \"alert_name\": \"InstanceDown\", \\\n",
    "           \"host_id\": \"test-instance-0.example.com\", \\\n",
    "           \"severity\": \"critical\", \\\n",
    "           \"description\": \"Instance test-instance-0.example.com is not available for scraping for the last 5m. \" \\\n",
    "                         \"Please check: - instance is up and running; - monitoring service is in place and running; - network connectivity is ok\", \\\n",
    "           \"summary\": \"Instance test-instance-0.example.com is down\", \\\n",
    "           \"timestamp\": \"2025-04-28T05:00:00.000000\"}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ac49a9",
   "metadata": {},
   "source": [
    "After running the cell above, we have confirmed that the tool calling agent is properly configured and ready for a naive evaluation. This evaluation will be our performance baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730ef191",
   "metadata": {},
   "source": [
    "<a id=\"eval-triage-agent1\"></a>\n",
    "## 2.4) Evaluate the tool-calling agent (naive parameters)\n",
    "\n",
    "*using `nat eval`...*\n",
    "\n",
    "Now let's run a full evaluation on the Alert Triage Agent using the complete offline dataset. This dataset contains seven alerts with different root causes:\n",
    "\n",
    "- **False positives** - System appears healthy despite alert\n",
    "- **Hardware issues** - Hardware failures or degradation  \n",
    "- **Software issues** - Malfunctioning monitoring services\n",
    "- **Maintenance** - Scheduled maintenance windows\n",
    "- **Repetitive behavior** - Benign recurring patterns\n",
    "\n",
    "The evaluation will measure:\n",
    "1. **Classification Accuracy** - How well the agent categorizes root causes\n",
    "2. **Answer Accuracy** - How well the generated reports match expected outcomes (using RAGAS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c4fcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nat eval --config_file ./tmp_workflow/configs/alert_triage_config.yml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe817e6",
   "metadata": {},
   "source": [
    "**Understanding Alert Triage Evaluation Results**\n",
    "\n",
    "The evaluation generates several output files in the `alert_triage_output` directory:\n",
    "\n",
    "1. **classification_accuracy_output.json** - Root cause classification metrics\n",
    "   - Shows accuracy, precision, recall, and F1 scores for each category\n",
    "   - Contains confusion matrix for detailed analysis\n",
    "   \n",
    "2. **rag_accuracy_output.json** - Answer quality metrics\n",
    "   - Measures how well generated reports match expected outcomes\n",
    "   - Uses LLM-as-a-judge to evaluate report quality\n",
    "\n",
    "3. **workflow_output.json** - Complete execution traces\n",
    "   - Contains full agent trajectories with tool calls\n",
    "   - Includes generated reports for each alert\n",
    "   - Shows token usage and performance metrics\n",
    "\n",
    "Let's examine the classification accuracy results:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754384df",
   "metadata": {},
   "source": [
    "We see that the classification accuracy results are around 43% based on RAG accuracy results of 46%.\n",
    "\n",
    "Next we will run the optimizer over a variety of models and some reasonable hyperparameters, then use that optimal configuration and run the evaluation again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbbe01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load and display classification accuracy results\n",
    "# path-check-skip-next-line\n",
    "with open('./tmp_workflow/alert_triage_output/classification_accuracy_output.json', 'r') as f:\n",
    "    classification_results = json.load(f)\n",
    "\n",
    "print(\"Classification Accuracy Results:\")\n",
    "print(f\"Average Score: {classification_results['average_score']:.2%}\")\n",
    "print(\"\\nPer-Alert Results:\")\n",
    "for item in classification_results['eval_output_items']:\n",
    "    print(f\"  Alert {item['id']}: Score={item['score']:.2f} - {item['reasoning']}\")\n",
    "\n",
    "# Load and display RAG accuracy results\n",
    "# path-check-skip-next-line\n",
    "with open('./tmp_workflow/alert_triage_output/rag_accuracy_output.json', 'r') as f:\n",
    "    rag_results = json.load(f)\n",
    "\n",
    "print(\"\\n\\nRAG Accuracy Results:\")\n",
    "print(f\"Average Score: {rag_results['average_score']:.2%}\")\n",
    "print(f\"Total Alerts Evaluated: {len(rag_results['eval_output_items'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2c2cf1",
   "metadata": {},
   "source": [
    "<a id=\"optimize-triage-agent\"></a>\n",
    "## 2.5) Optimize the tool-calling agent's LLM\n",
    "*using `nat optimize`...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddd831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nat optimize --config_file tmp_workflow/configs/alert_triage_config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd59bb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import ast\n",
    "\n",
    "# Load the optimizer results\n",
    "trials_df_path = Path(\"tmp_workflow/alert_triage_output/optimizer/trials_dataframe_params.csv\")\n",
    "\n",
    "if trials_df_path.exists():\n",
    "    trials_df = pd.read_csv(trials_df_path)\n",
    "    \n",
    "    print(\"Grid Search Optimization Results\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nTrials Summary:\")\n",
    "    print(trials_df.to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"\\nModel Performance Statistics (Mean across repetitions):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Group by model name to calculate statistics across repetitions\n",
    "    for model_name in trials_df['params_llms.agent_llm.model_name'].unique():\n",
    "        model_trials = trials_df[trials_df['params_llms.agent_llm.model_name'] == model_name]\n",
    "        \n",
    "        print(f\"\\n{model_name}:\")\n",
    "        \n",
    "        # Parse rep_scores to extract individual repetition metrics\n",
    "        if 'rep_scores' in model_trials.columns:\n",
    "            all_classification_accuracies = []\n",
    "            all_rag_accuracies = []\n",
    "            \n",
    "            for rep_scores_str in model_trials['rep_scores']:\n",
    "                rep_scores = ast.literal_eval(rep_scores_str)\n",
    "                for score_set in rep_scores:\n",
    "                    # score_set format: [classification_accuracy, rag_accuracy]\n",
    "                    all_classification_accuracies.append(score_set[0])\n",
    "                    all_rag_accuracies.append(score_set[1])\n",
    "            \n",
    "            # Calculate mean and standard deviation\n",
    "            def calculate_stats(values):\n",
    "                mean = np.mean(values)\n",
    "                std = np.std(values)\n",
    "                ci_lower = np.percentile(values, 2.5)\n",
    "                ci_upper = np.percentile(values, 97.5)\n",
    "                return mean, std, ci_lower, ci_upper\n",
    "            \n",
    "            class_acc_mean, class_acc_std, class_acc_ci_lower, class_acc_ci_upper = \\\n",
    "                calculate_stats(all_classification_accuracies)\n",
    "            rag_acc_mean, rag_acc_std, rag_acc_ci_lower, rag_acc_ci_upper = calculate_stats(all_rag_accuracies)\n",
    "            \n",
    "            print(\"  Classification Accuracy:\")\n",
    "            print(f\"    Mean: {class_acc_mean:.3f} (±{class_acc_std:.3f})\")\n",
    "            print(f\"    95% CI: [{class_acc_ci_lower:.3f}, {class_acc_ci_upper:.3f}]\")\n",
    "            \n",
    "            print(\"  RAG Accuracy:\")\n",
    "            print(f\"    Mean: {rag_acc_mean:.3f} (±{rag_acc_std:.3f})\")\n",
    "            print(f\"    95% CI: [{rag_acc_ci_lower:.3f}, {rag_acc_ci_upper:.3f}]\")\n",
    "        else:\n",
    "            # Fallback to aggregated values if rep_scores not available\n",
    "            # values_0 = classification_accuracy, values_1 = rag_accuracy\n",
    "            class_acc_mean = np.mean(model_trials['values_0'])\n",
    "            rag_acc_mean = np.mean(model_trials['values_1'])\n",
    "            \n",
    "            print(f\"  Classification Accuracy (mean): {class_acc_mean:.3f}\")\n",
    "            print(f\"  RAG Accuracy (mean): {rag_acc_mean:.3f}\")\n",
    "            print(\"  Note: 95% CI not available without rep_scores data\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"\\nBest Configuration (by aggregated classification accuracy across all repetitions):\")\n",
    "    # Find the trial with best aggregated classification accuracy\n",
    "    best_trial = trials_df.loc[trials_df['values_0'].idxmax()]\n",
    "    print(f\"Model: {best_trial['params_llms.agent_llm.model_name']}\")\n",
    "    print(f\"Aggregated Classification Accuracy Score: {best_trial['values_0']}\")\n",
    "    print(f\"Aggregated RAG Accuracy: {best_trial['values_1']}\")\n",
    "else:\n",
    "    print(f\"Optimizer results not found at {trials_df_path}\")\n",
    "    print(\"Please run the optimizer first (cell 55)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67550b0",
   "metadata": {},
   "source": [
    "<a id=\"eval-triage-agent2\"></a>\n",
    "## 2.6) Re-evaluate the optimized tool-calling agent\n",
    "\n",
    "Now apply the optimal parameters in **optimized_config.yml** to re-evaluate the original eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506ef10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path-check-skip-next-line\n",
    "!nat eval --config_file ./tmp_workflow/alert_triage_output/optimizer/optimized_config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a10743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load and display classification accuracy results\n",
    "# path-check-skip-next-line\n",
    "with open('./tmp_workflow/alert_triage_output/classification_accuracy_output.json', 'r') as f:\n",
    "    classification_results = json.load(f)\n",
    "\n",
    "print(\"Classification Accuracy Results:\")\n",
    "print(f\"Average Score: {classification_results['average_score']:.2%}\")\n",
    "print(\"\\nPer-Alert Results:\")\n",
    "for item in classification_results['eval_output_items']:\n",
    "    print(f\"  Alert {item['id']}: Score={item['score']:.2f} - {item['reasoning']}\")\n",
    "\n",
    "# Load and display RAG accuracy results\n",
    "# path-check-skip-next-line\n",
    "with open('./tmp_workflow/alert_triage_output/rag_accuracy_output.json', 'r') as f:\n",
    "    rag_results = json.load(f)\n",
    "\n",
    "print(\"\\n\\nRAG Accuracy Results:\")\n",
    "print(f\"Average Score: {rag_results['average_score']:.2%}\")\n",
    "print(f\"Total Alerts Evaluated: {len(rag_results['eval_output_items'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20f6d61",
   "metadata": {},
   "source": [
    "<a id=\"next-steps\"></a>\n",
    "# 3.0) Next steps\n",
    "\n",
    "Continue learning how to fully utilize the NVIDIA NeMo Agent toolkit by exploring the other documentation and advanced agents in the `examples` directory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unew_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
