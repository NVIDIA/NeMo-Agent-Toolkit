{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Size a GPU Cluster With NVIDIA NeMo Agent Toolkit\n",
    "\n",
    "This notebook demonstrates how to use the NVIDIA NeMo Agent toolkit's sizing calculator to estimate the GPU cluster size required to accommodate a target number of users with a target response time. The estimation is based on the performance of the workflow at different concurrency levels.\n",
    "\n",
    "The sizing calculator uses the [evaluation](https://docs.nvidia.com/aiqtoolkit/latest/workflows/evaluate.html) and [profiling](https://docs.nvidia.com/aiqtoolkit/latest/workflows/profiler.html) systems in the NeMo Agent toolkit.\n",
    "\n",
    "**Note:** This guide assumes that you have an LLM hosted by an isolated GPU cluster, for which you want to perform the sizing calculations. Although you can run the sizing calculator against a publicly hosted LLM, the results may not be accurate due to the variability in the performance of public LLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPDX-FileCopyrightText: Copyright (c) 2025, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "import sys\n",
    "import site\n",
    "import getpass\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install ``uv`` and Git LFS\n",
    "\n",
    "This environment will be managed by ``uv``. First, we can go ahead an install it on this system and add it to ``$PATH``. We can also install Git LFS to ensure submodules are properly populated upon cloning the NeMo Agent Toolkit repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install uv\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "os.environ[\"PATH\"]=f\"/home/ubuntu/.local/bin:{os.environ['PATH']}\"\n",
    "\n",
    "# Install Git LFS\n",
    "!sudo apt-get install git-lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Clone the NeMo Agent Toolkit Repository\n",
    "\n",
    "First, let's clone the NVIDIA NeMo Agent Toolkit repository to get access to all the necessary files and examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the repository already exists\n",
    "repo_name = \"NeMo-Agent-Toolkit\"\n",
    "if not os.path.exists(repo_name):\n",
    "    print(\"Cloning NVIDIA NeMo Agent Toolkit repository...\")\n",
    "    subprocess.run([\"git\", \"clone\", \"https://github.com/NVIDIA/NeMo-Agent-Toolkit.git\"], check=True)\n",
    "    print(\"Repository cloned successfully!\")\n",
    "else:\n",
    "    print(f\"Repository {repo_name} already exists.\")\n",
    "\n",
    "# Change to the repository directory\n",
    "os.chdir(repo_name)\n",
    "print(f\"Changed to directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Git Submodules and LFS\n",
    "\n",
    "The NeMo Agent Toolkit uses Git submodules and Large File Storage (LFS) for some components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize, fetch, and update submodules\n",
    "print(\"Initializing Git submodules...\")\n",
    "subprocess.run([\"git\", \"submodule\", \"update\", \"--init\", \"--recursive\"], check=True)\n",
    "\n",
    "# Fetch LFS files\n",
    "print(\"Setting up Git LFS...\")\n",
    "subprocess.run([\"git\", \"lfs\", \"install\"], check=True)\n",
    "subprocess.run([\"git\", \"lfs\", \"fetch\"], check=True)\n",
    "subprocess.run([\"git\", \"lfs\", \"pull\"], check=True)\n",
    "\n",
    "print(\"Git setup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "We'll install the NeMo Agent Toolkit and its dependencies using `uv` (or `pip` as a fallback).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if uv is available, otherwise use pip\n",
    "if shutil.which(\"uv\"):\n",
    "    print(\"Installing NeMo Agent Toolkit using uv...\")\n",
    "    \n",
    "    # Create virtual environment with uv (specifying Python 3.12 as recommended)\n",
    "    subprocess.run([\"uv\", \"venv\", \"--python\", \"3.12\", \"--seed\", \".venv\"], check=True)\n",
    "    print(\"Virtual environment created successfully!\")\n",
    "    \n",
    "    # Install all dependencies including profiling tools and plugins\n",
    "    subprocess.run([\"uv\", \"sync\", \"--all-groups\", \"--all-extras\"], check=True)\n",
    "    print(\"Installation completed with uv!\")\n",
    "    \n",
    "    # Activate the virtual environment for this session\n",
    "    venv_path = os.path.join(os.getcwd(), \".venv\")\n",
    "    if os.name == 'nt':  # Windows\n",
    "        scripts_dir = os.path.join(venv_path, \"Scripts\")\n",
    "        python_exe = os.path.join(scripts_dir, \"python.exe\")\n",
    "    else:  # Unix/Linux/macOS\n",
    "        scripts_dir = os.path.join(venv_path, \"bin\")\n",
    "        python_exe = os.path.join(scripts_dir, \"python\")\n",
    "    \n",
    "    # Add virtual environment to Python path\n",
    "    site.addsitedir(os.path.join(venv_path, \"Lib\", \"site-packages\" if os.name == 'nt' else \"lib/python{}.{}/site-packages\".format(*sys.version_info[:2])))\n",
    "    \n",
    "    # Update PATH to include the virtual environment's Scripts/bin directory\n",
    "    current_path = os.environ.get('PATH', '')\n",
    "    os.environ['PATH'] = scripts_dir + os.pathsep + current_path\n",
    "    \n",
    "    print(f\"Virtual environment activated! Scripts directory: {scripts_dir}\")\n",
    "    \n",
    "else:\n",
    "    print(\"uv not found, using pip instead...\")\n",
    "    print(\"Note: uv is recommended for proper installation. Please install uv for best results.\")\n",
    "    # Install using pip\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", \".[profiling]\"], check=True)\n",
    "    print(\"Installation completed with pip!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Installation\n",
    "\n",
    "Let's verify that the NeMo Agent Toolkit is properly installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installation\n",
    "print(\"Checking installation...\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"PATH: {os.environ.get('PATH', '')[:200]}...\")  # Show first 200 chars of PATH\n",
    "\n",
    "# Check if aiq command is available\n",
    "aiq_path = shutil.which(\"aiq\")\n",
    "if aiq_path:\n",
    "    print(f\"aiq command found at: {aiq_path}\")\n",
    "else:\n",
    "    print(\"aiq command not found in PATH\")\n",
    "    \n",
    "    # Check if it exists in the virtual environment\n",
    "    venv_path = os.path.join(os.getcwd(), \".venv\")\n",
    "    if os.name == 'nt':  # Windows\n",
    "        potential_aiq = os.path.join(venv_path, \"Scripts\", \"aiq.exe\")\n",
    "    else:  # Unix/Linux/macOS\n",
    "        potential_aiq = os.path.join(venv_path, \"bin\", \"aiq\")\n",
    "    \n",
    "    if os.path.exists(potential_aiq):\n",
    "        print(f\"aiq found in virtual environment at: {potential_aiq}\")\n",
    "        print(\"The virtual environment may not be properly activated.\")\n",
    "    else:\n",
    "        print(f\"aiq not found at expected location: {potential_aiq}\")\n",
    "\n",
    "# Try to run aiq command\n",
    "try:\n",
    "    result = subprocess.run([\"aiq\", \"--version\"], capture_output=True, text=True, check=True)\n",
    "    print(f\"✅ NeMo Agent Toolkit version: {result.stdout.strip()}\")\n",
    "    print(\"✅ Installation verified successfully!\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"❌ Error running aiq --version: {e}\")\n",
    "    print(f\"Return code: {e.returncode}\")\n",
    "    if e.stderr:\n",
    "        print(f\"Error output: {e.stderr}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ aiq command not found: {e}\")\n",
    "    print(\"\\nTroubleshooting steps:\")\n",
    "    print(\"1. Make sure uv is installed: pip install uv\")\n",
    "    print(\"2. Re-run the installation cell above\")\n",
    "    print(\"3. Check that the virtual environment was created successfully\")\n",
    "    print(\"4. Manually activate the virtual environment if needed:\")\n",
    "    if os.name == 'nt':\n",
    "        print(\"   .venv\\\\Scripts\\\\activate\")\n",
    "    else:\n",
    "        print(\"   source .venv/bin/activate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run aiq commands with proper environment\n",
    "def run_aiq_command(args, **kwargs):\n",
    "    \"\"\"\n",
    "    Run aiq command with proper virtual environment activation.\n",
    "    This ensures the command works even if the notebook environment isn't fully activated.\n",
    "    \"\"\"\n",
    "    venv_path = os.path.join(os.getcwd(), \".venv\")\n",
    "    \n",
    "    if os.name == 'nt':  # Windows\n",
    "        aiq_executable = os.path.join(venv_path, \"Scripts\", \"aiq.exe\")\n",
    "        python_executable = os.path.join(venv_path, \"Scripts\", \"python.exe\")\n",
    "    else:  # Unix/Linux/macOS\n",
    "        aiq_executable = os.path.join(venv_path, \"bin\", \"aiq\")\n",
    "        python_executable = os.path.join(venv_path, \"bin\", \"python\")\n",
    "    \n",
    "    # Try direct aiq executable first\n",
    "    if os.path.exists(aiq_executable):\n",
    "        cmd = [aiq_executable] + args\n",
    "        print(f\"Running: {' '.join(cmd)}\")\n",
    "        return subprocess.run(cmd, **kwargs)\n",
    "    \n",
    "    # Fallback to python -m aiq\n",
    "    elif os.path.exists(python_executable):\n",
    "        cmd = [python_executable, \"-m\", \"aiq\"] + args\n",
    "        print(f\"Running: {' '.join(cmd)}\")\n",
    "        return subprocess.run(cmd, **kwargs)\n",
    "    \n",
    "    # Last resort: try system aiq\n",
    "    else:\n",
    "        cmd = [\"aiq\"] + args\n",
    "        print(f\"Running: {' '.join(cmd)}\")\n",
    "        return subprocess.run(cmd, **kwargs)\n",
    "\n",
    "# Test the helper function\n",
    "print(\"Testing aiq command with helper function...\")\n",
    "try:\n",
    "    result = run_aiq_command([\"--version\"], capture_output=True, text=True, check=True)\n",
    "    print(f\"✅ Success! Version: {result.stdout.strip()}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Still having issues: {e}\")\n",
    "    print(\"The installation may need to be re-run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Variables\n",
    "\n",
    "Set up the required environment variables for API access (if using NVIDIA NIMs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set API key in notebook environment\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    nvapi_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key\n",
    "    print(\"✅ API key has been set in notebook environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy an LLM\n",
    "\n",
    "Uncomment the following and run the following command to spin up a sample model on ``http://localhost:8000/v1``. \n",
    "\n",
    "Alternatively, you can use a model running elsewhere, just remember to input that model location as the ``base_url`` in the below model configurations section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !echo \"${NVIDIA_API_KEY}\" | docker login nvcr.io -u '$oauthtoken' --password-stdin\n",
    "# !docker run -it -d --rm \\\n",
    "#   --name local-nim \\\n",
    "#   --runtime=nvidia \\\n",
    "#   --gpus '\"device=0,1\"' \\\n",
    "#   -p 8000:8000 \\\n",
    "#   -v \"/tmp:/opt/nim/.cache\" \\\n",
    "#   -e NGC_API_KEY=\"${NVIDIA_API_KEY}\" \\\n",
    "#   --shm-size=20g \\\n",
    "#   nvcr.io/nim/meta/llama-3.3-70b-instruct:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View container status - look for 'uvicorn running on http://0.0.0.0:8000'\n",
    "# !docker logs local-nim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Setup\n",
    "\n",
    "To begin, set the configuration file and output directory. For this example we will start with the simple calculator evaluation configuration file, however in a real-world scenario you would use the configuration file of your own workflow you want to size.\n",
    "\n",
    "### Update the Tools\n",
    "\n",
    "First, let's update the configuration file to use the right simple calculator tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_config = \"examples/evaluation_and_profiling/simple_calculator_eval/configs/config-sizing-calc.yml\"\n",
    "\n",
    "with open(source_config, 'r') as f:\n",
    "    try:\n",
    "        config_data = yaml.safe_load(f)\n",
    "    \n",
    "        # Adjust tool names \n",
    "        if 'functions' not in config_data:\n",
    "            config_data['functions'] = {}\n",
    "        if 'calculator_divide' not in config_data['functions']:\n",
    "            config_data['functions']['calculator_divide'] = {}\n",
    "        if '_type' not in config_data['functions']['calculator_divide']:\n",
    "            config_data['functions']['calculator_divide']['_type'] = {}\n",
    "    \n",
    "        if 'calculator_inequality' not in config_data['functions']:\n",
    "            config_data['functions']['calculator_inequality'] = {}\n",
    "        if '_type' not in config_data['functions']['calculator_inequality']:\n",
    "            config_data['functions']['calculator_inequality']['_type'] = {}\n",
    "        \n",
    "        if 'calculator_multiply' not in config_data['functions']:\n",
    "            config_data['functions']['calculator_multiply'] = {}\n",
    "        if '_type' not in config_data['functions']['calculator_multiply']:\n",
    "            config_data['functions']['calculator_multiply']['_type'] = {}\n",
    "            \n",
    "        if 'calculator_subtract' not in config_data['functions']:\n",
    "            config_data['functions']['calculator_subtract'] = {}\n",
    "        if '_type' not in config_data['functions']['calculator_subtract']:\n",
    "            config_data['functions']['calculator_subtract']['_type'] = {}\n",
    "            \n",
    "        if 'current_datetime' not in config_data['functions']:\n",
    "            config_data['functions']['current_datetime'] = {}\n",
    "        if '_type' not in config_data['functions']['current_datetime']:\n",
    "            config_data['functions']['current_datetime']['_type'] = {}\n",
    "        \n",
    "        config_data['functions']['calculator_divide']['_type'] = 'nat_simple_calculator/calculator_divide'\n",
    "        config_data['functions']['calculator_inequality']['_type'] = 'nat_simple_calculator/calculator_inequality'\n",
    "        config_data['functions']['calculator_multiply']['_type'] = 'nat_simple_calculator/calculator_multiply'\n",
    "        config_data['functions']['calculator_subtract']['_type'] = 'nat_simple_calculator/calculator_subtract'\n",
    "        config_data['functions']['current_datetime']['_type'] = 'nat.tool/current_datetime'\n",
    "        \n",
    "        with open(source_config, 'w') as file:\n",
    "            yaml.dump(config_data, file, default_flow_style=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing YAML: {e}\")\n",
    "        print(\"Please edit the file manually.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Considerations\n",
    "\n",
    "When using the sizing calculator, you need a representative dataset of inputs. The size of the dataset can be as small as one input. However, if your workflow's behavior varies significantly depending on the input, we recommend including representative dataset entries for each trajectory.\n",
    "\n",
    "The dataset is provided in the eval section of the workflow configuration file.\n",
    "`examples/evaluation_and_profiling/simple_calculator_eval/configs/config-sizing-calc.yml`:\n",
    "```yaml\n",
    "eval:\n",
    "  general:\n",
    "    output_dir: .tmp/aiq/examples/simple_calculator/eval\n",
    "    dataset:\n",
    "      _type: json\n",
    "      file_path: examples/getting_started/simple_calculator/data/simple_calculator.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the dataset, you need to specify the `eval.general.output_dir` parameter for storing the evaluation results. Other parameters in the eval section are not used by the calculator. For more information, refer to the [Evaluate](NeMo-Agent-Toolkit/docs/source/reference/evaluate.md) documentation.\n",
    "\n",
    "The dataset used by the sizing calculator does not need to include ground truth answers. Only the inputs are needed.\n",
    "For example, the following dataset is valid:\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"question\": \"What is the product of 3 and 7, and is it greater than the current hour?\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"question\": \"What is the product of 4 and 5, and is it greater than the current hour?\",\n",
    "    }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust the Model Configuration\n",
    "\n",
    "Finally, you will need to edit the configuration file to point to your LLM cluster. Make sure your LLM(s) are up and running and reachable! \n",
    "\n",
    "The configuration should include a `base_url` parameter for your cluster. You can edit the file manually yourself, or use the below interactive configuration editor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the current configuration file (if it exists)\n",
    "if os.path.exists(source_config):\n",
    "    print(\"Current configuration file content:\")\n",
    "    print(\"=\" * 50)\n",
    "    with open(source_config, 'r') as f:\n",
    "        config_content = f.read()\n",
    "        print(config_content)\n",
    "    print(\"=\" * 50)\n",
    "    print(\"\\nPlease edit this file to add your LLM cluster details.\")\n",
    "    print(\"Example for a locally hosted NIM:\")\n",
    "    print(\"\"\"\n",
    "llms:\n",
    "  nim_llm:\n",
    "    _type: nim\n",
    "    base_url: \"http://localhost:8000/v1\"\n",
    "    model_name: meta/llama-3.3-70b-instruct\n",
    "    \"\"\")\n",
    "    \n",
    "    # Interactive configuration editor\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"INTERACTIVE CONFIGURATION EDITOR\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    modify_config = input(\"Would you like to modify the configuration interactively? (y/n): \").lower().strip()\n",
    "    \n",
    "    if modify_config == 'y':\n",
    "        base_url = input(\"Enter your LLM base URL (e.g., http://localhost:8000/v1): \").strip()\n",
    "        model_name = input(\"Enter your model name (e.g., meta/llama-3.3-70b-instruct): \").strip()\n",
    "        \n",
    "        if base_url and model_name:\n",
    "            # Simple configuration update\n",
    "            try:\n",
    "                config_data = yaml.safe_load(config_content)\n",
    "                if 'llms' not in config_data:\n",
    "                    config_data['llms'] = {}\n",
    "                if 'nim_llm' not in config_data['llms']:\n",
    "                    config_data['llms']['nim_llm'] = {}\n",
    "                \n",
    "                config_data['llms']['nim_llm']['_type'] = 'nim'\n",
    "                config_data['llms']['nim_llm']['base_url'] = base_url\n",
    "                config_data['llms']['nim_llm']['model_name'] = model_name\n",
    "                \n",
    "                print(f\"\\nUpdated configuration:\")\n",
    "                print(yaml.dump(config_data, default_flow_style=False))\n",
    "                \n",
    "                save_config = input(\"Save this configuration? (y/n): \").lower().strip()\n",
    "                if save_config == 'y':\n",
    "                    with open(source_config, 'w') as f:\n",
    "                        yaml.dump(config_data, f, default_flow_style=False)\n",
    "                    print(\"✅ Configuration saved!\")\n",
    "                else:\n",
    "                    print(\"Configuration not saved.\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing YAML: {e}\")\n",
    "                print(\"Please edit the file manually.\")\n",
    "        else:\n",
    "            print(\"Base URL and model name are required. Please edit manually.\")\n",
    "    else:\n",
    "        print(\"Please edit the configuration file manually before proceeding.\")\n",
    "        \n",
    "else:\n",
    "    print(\"Configuration file not found. Please create one manually.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment variables and directories\n",
    "calc_output_dir = \".tmp/sizing_calc/\"\n",
    "config_file = os.path.join(calc_output_dir, \"config-sizing-calc.yml\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(calc_output_dir, exist_ok=True)\n",
    "\n",
    "# Copy the example configuration file\n",
    "if os.path.exists(source_config):\n",
    "    shutil.copy(source_config, config_file)\n",
    "    print(f\"Configuration file copied to: {config_file}\")\n",
    "else:\n",
    "    print(f\"Warning: Source configuration file not found at {source_config}\")\n",
    "    print(\"You may need to create a custom configuration file.\")\n",
    "\n",
    "print(f\"Output directory: {calc_output_dir}\")\n",
    "print(f\"Config file: {config_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Gather Metrics\n",
    "\n",
    "Collect performance data at different concurrency levels. This step runs the workflow at specified concurrency levels to gather performance metrics.\n",
    "\n",
    "To use the calculator, gather metrics from the workflow and then separately size the cluster in `offline_mode` using the previously gathered metrics.\n",
    "\n",
    "The following is a sample command for gathering metrics:\n",
    "\n",
    "```\n",
    "aiq sizing calc --config_file $CONFIG_FILE --calc_output_dir $CALC_OUTPUT_DIR --concurrencies 1,2,4,8,16,32 --num_passes 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying the Concurrency Range\n",
    "A slope based mechanism is used to estimate the GPU count required for the workflow. To create a robust linear fit, we recommend using a wide range of concurrency values. A minimum of ten concurrency values is recommended, though the calculator can work with fewer values (accuracy may decrease). The concurrency range is specified as a comma separated list with the `--concurrencies` command line parameter.\n",
    "\n",
    "In addition to the concurrency range, you can specify the number of passes made with each concurrency with the `--num_passes` command line parameter. By default the number of passes is one or a multiple of the concurrency if the dataset is larger than the concurrency value.\n",
    "\n",
    "If the size of the dataset is smaller than the concurrency range specified, the dataset is repeated to match the concurrency range.\n",
    "\n",
    "**Note:** Depending on the number of concurrencies, the number of passes, and the size of the cluster being tested, this could take several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define parameters for metric gathering\n",
    "concurrencies = \"1,2,4,8,16,32\"\n",
    "num_passes = 2\n",
    "\n",
    "# Build the command\n",
    "gather_metrics_cmd = [\n",
    "    \"aiq\", \"sizing\", \"calc\",\n",
    "    \"--config_file\", config_file,\n",
    "    \"--calc_output_dir\", calc_output_dir,\n",
    "    \"--concurrencies\", concurrencies,\n",
    "    \"--num_passes\", str(num_passes)\n",
    "]\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(gather_metrics_cmd, check=True, capture_output=True, text=True)\n",
    "    print(\"Metrics gathering completed successfully!\")\n",
    "    print(result.stdout)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error running sizing calculator: {e}\\n\")\n",
    "    print(f\"Error output: {e.stderr}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Estimate GPU Cluster Size\n",
    "\n",
    "Once the metrics are gathered, you can estimate the GPU cluster size using the `aiq sizing calc` command in `offline_mode`.\n",
    "Sample command:\n",
    "```\n",
    "aiq sizing calc --offline_mode --calc_output_dir $CALC_OUTPUT_DIR --test_gpu_count 8 --target_workflow_runtime 10 --target_users 100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target and Test Parameters\n",
    "**Target Parameters**\n",
    "\n",
    "To estimate the GPU cluster size, you need to specify the target number of users and the target workflow runtime, that is the maximum acceptable response time for the workflow.\n",
    "\n",
    "Optionally, you can specify the target p95 LLM latency if the LLM latency is a defining factor for the workflow and if it is possible to measure the maximum acceptable LLM latency.\n",
    "- `target_users`: Target number of users to support.\n",
    "- `target_workflow_runtime`: Target p95 workflow runtime (seconds). Can be set to 0 to ignore.\n",
    "- `target_llm_latency`: Target p95 LLM latency (seconds). Can be set to 0 to ignore.\n",
    "\n",
    "**Test Parameters**\n",
    "\n",
    "You need to specify the number of GPUs used for running the workflow via the `--test_gpu_count` command line parameter. This is the number of GPUs used during the profiling run, not the target cluster size. This information is used to extrapolate the GPU count required for the target users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target parameters\n",
    "test_gpu_count = 8  # Number of GPUs used during testing\n",
    "target_workflow_runtime = 10  # Target response time in seconds\n",
    "target_users = 100  # Target number of concurrent users\n",
    "\n",
    "# Build the estimation command\n",
    "estimate_cmd = [\n",
    "    \"aiq\", \"sizing\", \"calc\",\n",
    "    \"--offline_mode\",\n",
    "    \"--calc_output_dir\", calc_output_dir,\n",
    "    \"--test_gpu_count\", str(test_gpu_count),\n",
    "    \"--target_workflow_runtime\", str(target_workflow_runtime),\n",
    "    \"--target_users\", str(target_users)\n",
    "]\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(estimate_cmd, check=True, capture_output=True, text=True)\n",
    "    print(\"GPU cluster sizing completed successfully!\")\n",
    "    print(result.stdout)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error running GPU estimation: {e}\\n\")\n",
    "    print(f\"Error output: {e.stderr}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Approach (Alternative)\n",
    "\n",
    "You can also combine both steps into a single command by adding the target and test parameters to the first command.\n",
    "\n",
    "**Note:** Depending on the number of concurrencies, the number of passes, and the size of the cluster being tested, this could take several minutes to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run combined command that gathers metrics and estimates GPU count\n",
    "\n",
    "# combined_cmd = [\n",
    "#     \"aiq\", \"sizing\", \"calc\",\n",
    "#     \"--config_file\", config_file,\n",
    "#     \"--calc_output_dir\", calc_output_dir,\n",
    "#     \"--concurrencies\", concurrencies,\n",
    "#     \"--num_passes\", str(num_passes),\n",
    "#     \"--test_gpu_count\", str(test_gpu_count),\n",
    "#     \"--target_workflow_runtime\", str(target_workflow_runtime),\n",
    "#     \"--target_users\", str(target_users)\n",
    "# ]\n",
    "\n",
    "# try:\n",
    "#     result = subprocess.run(combined_cmd, check=True, capture_output=True, text=True)\n",
    "#     print(\"Combined sizing operation completed successfully!\")\n",
    "#     print(result.stdout)\n",
    "# except subprocess.CalledProcessError as e:\n",
    "#     print(f\"Error running combined sizing: {e}\\n\")\n",
    "#     print(f\"Error output: {e.stderr}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Outputs\n",
    "\n",
    "The per-concurrency metrics are stored in the `calc_output_dir` specified in the command line. We recommend using a separate output directory for the calculator than the one used for the evaluation (specified through `eval.general.output_dir` in the workflow configuration file). This avoids accidental deletion of the calculator metrics when the evaluation jobs cleans up.\n",
    "\n",
    "By default, the metrics of the latest calculator run overwrite the previous runs. You can use the `--append_calc_outputs` command line parameter to store each run in a separate subdirectory.\n",
    "\n",
    "The results of each run are available in the following formats:\n",
    "- A summary table\n",
    "- Analysis plots\n",
    "- A JSON file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Table\n",
    "\n",
    "The summary table provides an overview of the per-concurrency metrics.\n",
    "- The `P95 LLM Latency` (95th percentile LLM latency) column contains the latency, in seconds, across all LLM invocations. If multiple models are used, the value will trend towards the latency of the model with the highest latency.\n",
    "- The `P95 WF Runtime` (95th percentile workflow runtime) column contains the response time, in seconds, of the workflow and is computed across all runs at the specified concurrency.\n",
    "- The `Total Runtime` columns contains the total time, in seconds, taken to process the entire dataset at a specified concurrency level.\n",
    "\n",
    "```\n",
    "Targets: LLM Latency ≤ 0.0s, Workflow Runtime ≤ 0.0s, Users = 0\n",
    "Test parameters: GPUs = 0\n",
    "Per concurrency results:\n",
    "|   Concurrency |   p95 LLM Latency |   p95 WF Runtime |   Total Runtime |\n",
    "|---------------|-------------------|------------------|-----------------|\n",
    "|             1 |           1.14981 |          4.03488 |         8.06977 |\n",
    "|             2 |           1.3591  |          4.71197 |         9.32298 |\n",
    "|             4 |           1.50682 |          5.67581 |        11.1683  |\n",
    "|             8 |           2.10668 |          7.90895 |        15.6193  |\n",
    "|            16 |           3.30196 |         12.677   |        25.3173  |\n",
    "|            32 |           6.57847 |         24.5307  |        43.9806  |\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots\n",
    "\n",
    "The calculator generates plots to help visualize the concurrency against time metrics.\n",
    "![Simple plot](NeMo-Agent-Toolkit/docs/source/_static/concurrency_vs_p95_simple.png)\n",
    "\n",
    "An enhanced analysis plot is also generated. This plot is described in more detail in the [Slope-based Estimation](#Slope-based-Estimation) section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Output\n",
    "\n",
    "The JSON file contains the per-concurrency metrics you can use for more analysis.\n",
    "Sample output:\n",
    "`calc_runner_output.json`:\n",
    "```bash\n",
    "{\n",
    "  \"gpu_estimates\": {\n",
    "    \"gpu_estimate_by_wf_runtime\": 76.61472307484419,\n",
    "    \"gpu_estimate_by_llm_latency\": null\n",
    "  },\n",
    "  \"per_concurrency_data\": {\n",
    "    \"1\": {\n",
    "      \"gpu_estimates\": {\n",
    "        \"gpu_estimate_by_wf_runtime\": 309.15830421447754,\n",
    "        \"gpu_estimate_by_llm_latency\": null\n",
    "      },\n",
    "      \"out_of_range_runs\": {\n",
    "        \"num_items_greater_than_target_latency\": 0,\n",
    "        \"num_items_greater_than_target_runtime\": 0,\n",
    "        \"workflow_interrupted\": false\n",
    "      },\n",
    "      >>>>>> SNIPPED <<<<<\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "The output is truncated for brevity. For more information, refer to the [CalcRunnerOutput](NeMo-Agent-Toolkit/src/nat/profiler/calc/data_models.py) Pydantic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the Results\n",
    "\n",
    "The sizing calculator provides two GPU count estimates:\n",
    "- `Estimated GPU count (Workflow Runtime)`: Estimated GPU count based on the target workflow runtime.\n",
    "- `Estimated GPU count (LLM Latency)`: Estimated GPU count based on the target LLM latency.\n",
    "\n",
    "You can use a maximum of the two estimates as the final GPU count to accommodate the target users.\n",
    "\n",
    "**Sample output:**\n",
    "```\n",
    "Targets: LLM Latency ≤ 0.0s, Workflow Runtime ≤ 10.0s, Users = 100\n",
    "Test parameters: GPUs = 8\n",
    "Per concurrency results:\n",
    "|   Concurrency |   p95 LLM Latency |   p95 WF Runtime |   Total Runtime |   Runtime OOR |   GPUs (WF Runtime, Rough) |\n",
    "|---------------|-------------------|------------------|-----------------|---------------|----------------------------|\n",
    "|             1 |           1.14981 |          4.03488 |         8.06977 |             0 |                   322.79   |\n",
    "|             2 |           1.3591  |          4.71197 |         9.32298 |             0 |                   188.479  |\n",
    "|             4 |           1.50682 |          5.67581 |        11.1683  |             0 |                   113.516  |\n",
    "|             8 |           2.10668 |          7.90895 |        15.6193  |             0 |                    79.0895 |\n",
    "|            16 |           3.30196 |         12.677   |        25.3173  |            32 |                            |\n",
    "|            32 |           6.57847 |         24.5307  |        43.9806  |            64 |                            |\n",
    "\n",
    "=== GPU ESTIMATES ===\n",
    "Estimated GPU count (Workflow Runtime): 75.4\n",
    "```\n",
    "\n",
    "**Note:**\n",
    "\n",
    "In addition to the slope based estimation, the calculator also provides a rough estimate of the GPU count required for the target user based on the data from each concurrency level. You can use this information to get a quick estimate of the GPU count required for the workflow but is not as accurate as the slope based estimation and is not recommended for production use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slope-based Estimation\n",
    "\n",
    "The sizing calculator uses a **slope-based estimation** approach to determine how your workflow’s performance scales with increasing concurrency. This method helps estimate the number of GPUs required to meet your target user load and response time.\n",
    "\n",
    "**Analysis Plots**\n",
    "\n",
    "The analysis plots, generated by the calculator, offer a visual representation of the concurrency vs. latency and concurrency vs. runtime. The trend line is a linear fit of the concurrency vs. time metrics. The slope of the trend line is used to estimate the GPU count required for the workflow.\n",
    "![Analysis plot output](NeMo-Agent-Toolkit/docs/source/_static/concurrency_vs_p95_analysis.png)\n",
    "\n",
    "**Estimation Process**\n",
    "\n",
    "To estimate the GPU count required for the workflow, the calculator performs the following steps:\n",
    "\n",
    "1. **Linear Fit of Concurrency vs. Time Metrics**\n",
    "   - The calculator runs your workflow at several different concurrency levels.\n",
    "   - For each level, it measures key metrics such as p95 LLM latency and p95 workflow runtime.\n",
    "   - It then fits a straight line (using least squares regression) to the data points, modeling how time metrics change as concurrency increases.\n",
    "\n",
    "2. **Slope and Intercept**\n",
    "   - The **slope** of the fitted line represents how much the time metric (latency or runtime) increases for each additional concurrent user. A slope of 1.0 means that the time metric increases perfectly linearly with the concurrency. A slope greater than 1.0 means that the time metric increases faster than linearly with the concurrency and optimization should be done to reduce the slope.\n",
    "   - The **intercept** represents the baseline time metric when concurrency is zero (theoretical minimum). Note that this is a mathematical extrapolation and may not correspond to actual measurements at concurrency=0. It is indicative of the overhead of the workflow.\n",
    "\n",
    "3. **R² Value**\n",
    "   - The calculator computes the R² (coefficient of determination) to indicate how well the linear model fits your data. An R² value close to 1.0 means a good fit.\n",
    "   - If the R² value is less than 0.7, the calculator will not use the linear fit to estimate the GPU count.\n",
    "\n",
    "4. **Outlier Removal**\n",
    "   - Outliers (data points that deviate significantly from the trend) are automatically detected and removed to ensure a robust fit using the `Interquartile Range` (IQR) method.\n",
    "   - For datasets with fewer than 8 data points, outliers are detected using raw time metric values. For larger datasets, outliers are detected using residuals from the linear fit.\n",
    "\n",
    "5. **Estimating Required Concurrency**\n",
    "   - Using your target time metric (for example, target workflow runtime), the calculator determines the maximum concurrency that can be supported for the `test_gpu_count`, while still meeting the target time. This is the `calculated_concurrency` in the formula below.\n",
    "\n",
    "6. **GPU Count Formula**\n",
    "   - The required GPU count is estimated using the formula:\n",
    "     ```\n",
    "     calculated_concurrency = (target_time_metric - intercept) / slope\n",
    "     gpu_estimate = (target_users / calculated_concurrency) * test_gpu_count\n",
    "     ```\n",
    "   - This formula scales your test results to your target user load, based on the observed scaling behavior.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Suppose your target workflow runtime is 10 seconds, the linear fit gives a slope of 0.6, and an intercept of 3.5. The calculator will compute the concurrency that achieves a 10s runtime:\n",
    "  `(10 - 3.5) / 0.6 ≈ 10.83`\n",
    "If you tested with 8 GPUs and want to support 100 users, the calculator will compute the amount of GPUs needed:\n",
    "  `(100 / 10.83) * 8 ≈ 73.9 GPUs`\n",
    "\n",
    "**Key Points:**\n",
    "- The more concurrency levels you test, the more accurate the estimation.\n",
    "- Outliers and failed runs are excluded from the fit.\n",
    "- The calculator provides both workflow runtime-based and LLM latency-based GPU estimates (if both targets are specified)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Failed Workflows\n",
    "Based on the test setup, you may meet failures as the concurrency value increases. When a workflow fails for an input, the pass stops for that particular concurrency value. The pass is tagged with a `workflow_interrupted` flag in the JSON output. Such concurrencies, with a `workflow_interrupted` flag set to `true`, are not included in the GPU estimate. This information is indicated in the summary table in an `Alerts` column.\n",
    "\n",
    "The following is sample output with alerts:\n",
    "```\n",
    "Targets: LLM Latency ≤ 0.0s, Workflow Runtime ≤ 0.0s, Users = 0\n",
    "Test parameters: GPUs = 0\n",
    "Per concurrency results:\n",
    "Alerts: !W = Workflow interrupted\n",
    "| Alerts |   Concurrency |   p95 LLM Latency |   p95 WF Runtime |   Total Runtime |\n",
    "|--------|---------------|-------------------|------------------|-----------------|\n",
    "|        | 1             | 1.14981           | 4.03488          |         8.06977 |\n",
    "|        | 2             | 1.3591            | 4.71197          |         9.32298 |\n",
    "| !W     | 4             | 1.50682           | 5.67581          |         11.1683 |\n",
    "|        | 8             | 2.10668           | 7.90895          |         15.6193 |\n",
    "|        | 16            | 3.30196           | 12.677           |         25.3173 |\n",
    "|        | 32            | 6.57847           | 24.5307          |         43.9806 |\n",
    "```\n",
    "\n",
    "In this example, the workflow failed at concurrency level 4 (indicated by `!W` in the Alerts column). The time metrics for concurrency 4 are not included in the GPU estimate as they are not reliable and may skew the linear fit used to estimate the GPU count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Workflows\n",
    "\n",
    "### Using a Remote Workflow\n",
    "By default, the calculator runs the workflow locally to gather metrics. You can use the `--endpoint` and `--endpoint_timeout` command line parameters to use a remote workflow for gathering metrics.\n",
    "\n",
    "Start the remote workflow:\n",
    "```bash\n",
    "aiq start fastapi --config_file=$CONFIG_FILE\n",
    "```\n",
    "\n",
    "Run the calculator using the remote endpoint:\n",
    "```bash\n",
    "aiq sizing calc --config_file $CONFIG_FILE --calc_output_dir $CALC_OUTPUT_DIR --concurrencies 1,2,4,8,16,32 --num_passes 2 --endpoint http://localhost:8000\n",
    "```\n",
    "The configuration file used for running the calculator only needs to specify the `eval` section. The `workflow` section is not used by the calculator when running with a remote endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmatic Usage\n",
    "In addition to the command line interface, the sizing calculator can be used programmatically.\n",
    "\n",
    "**Sample code:**\n",
    "```python\n",
    "import asyncio\n",
    "from aiq.profiler.calc.calc_runner import CalcRunner\n",
    "from aiq.profiler.calc.data_models import CalcRunnerConfig\n",
    "from aiq.profiler.calc.data_models import CalcRunnerOutput\n",
    "\n",
    "async def run_calc():\n",
    "    runner_config = CalcRunnerConfig(\n",
    "        config_file=\"config.yml\",\n",
    "        output_dir=\".tmp/calc/\",\n",
    "        concurrencies=[1, 2, 4, 8, 16, 32],\n",
    "        num_passes=2,\n",
    "        test_gpu_count=8,\n",
    "        target_workflow_runtime=10,\n",
    "        target_users=100,\n",
    "    )\n",
    "    runner = CalcRunner(runner_config)\n",
    "    result: CalcRunnerOutput = await runner.run()\n",
    "    # Access GPU estimates and per-concurrency metrics from result\n",
    "    print(result.gpu_estimates)\n",
    "    print(result.per_concurrency_data)\n",
    "\n",
    "# Run the async calc function\n",
    "asyncio.run(run_calc())\n",
    "```\n",
    "\n",
    "{py:class}`~aiq.profiler.calc.data_models.CalcRunnerConfig` is a Pydantic model that contains the configuration for the calculator. It provides fine-grained control over the calculator's behavior.\n",
    "{py:class}`~aiq.profiler.calc.data_models.CalcRunnerOutput` is a Pydantic model that contains the per-concurrency metrics and the GPU count estimates.\n",
    "For more information, refer to the [calculator data models](NeMo-Agent-Toolkit/src/nat/profiler/calc/data_models.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- **Repository**: [NVIDIA NeMo Agent Toolkit](https://github.com/NVIDIA/NeMo-Agent-Toolkit)\n",
    "- **Documentation**: [Full Documentation](https://docs.nvidia.com/aiqtoolkit/latest/index.html)\n",
    "- **Get Started Guide**: [Getting Started](https://docs.nvidia.com/aiqtoolkit/latest/quick-start/installing.html)\n",
    "- **Examples**: Check the `examples/` directory in the cloned repository\n",
    "- **Evaluation Guide**: [Evaluate with NeMo Agent Toolkit](https://docs.nvidia.com/aiqtoolkit/latest/workflows/evaluate.html)\n",
    "- **Troubleshooting**: [Common Issues](https://docs.nvidia.com/aiqtoolkit/latest/troubleshooting.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
