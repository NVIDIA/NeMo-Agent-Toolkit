#!/usr/bin/env python3
"""
üè¶ Banking Data Evaluator

This script evaluates the quality of generated banking data using LLM-as-a-Judge.
It focuses ONLY on evaluation, not data generation.

Requirements:
- NeMo Data Designer microservice deployed locally via docker compose
- NeMo Microservices SDK installed
- Access to the hackathon cluster (https://nmp.aire.nvidia.com)
- Input file: 'banking_agent_evaluation_data.json' (generated by banking_data_generator.py)
"""

import json
import pandas as pd
from typing import Literal
from pydantic import BaseModel, Field

from nemo_microservices import NeMoMicroservices
from nemo_microservices.beta.data_designer import (
    DataDesignerConfigBuilder,
    DataDesignerClient,
)
from nemo_microservices.beta.data_designer.config import columns as C
from nemo_microservices.beta.data_designer.config import params as P


class CustomerQuery(BaseModel):
    """Structure for customer queries"""
    query: str = Field(description="The customer's request or question")
    urgency: Literal["Low", "Medium", "High"] = Field(description="Urgency level of the request")
    complexity: Literal["Simple", "Moderate", "Complex"] = Field(description="Complexity of the request")


class ToolParameter(BaseModel):
    """Structure for tool parameters"""
    name: str = Field(description="Parameter name")
    type: str = Field(description="Parameter type (string, integer, float, boolean)")
    description: str = Field(description="Parameter description")
    required: bool = Field(description="Whether this parameter is required")


class BankingTool(BaseModel):
    """Structure for banking tools"""
    name: str = Field(description="Tool function name")
    description: str = Field(description="What this tool does")
    parameters: list[ToolParameter] = Field(description="List of parameters this tool accepts")


class ToolCall(BaseModel):
    """Structure for tool calls"""
    function_name: str = Field(description="Name of the function to call")
    arguments: dict = Field(description="Arguments to pass to the function")


def initialize_data_designer():
    """Initialize the NeMo Data Designer client and config builder"""
    print("üöÄ Initializing NeMo Data Designer for evaluation...")
    
    # Initialize the NDD client
    ndd = DataDesignerClient(client=NeMoMicroservices(base_url="http://localhost:8000", timeout=600))
    
    # Configure model endpoints for the hackathon cluster - using 70B model for evaluation
    judge_model_endpoint = "https://nim.aire.nvidia.com/v1/"
    judge_model_id = "meta/llama-3.3-70b-instruct"  # Using 70B for evaluation
    judge_model_alias = "llama33-70b"

    # Create model configuration for evaluation only
    model_configs = [
        P.ModelConfig(
            alias=judge_model_alias,
            inference_parameters=P.InferenceParameters(
                max_tokens=512,  # Sufficient for evaluation responses
                temperature=0.3,  # Lower temperature for more consistent evaluation
                top_p=1.0,
            ),
            model=P.Model(api_endpoint=P.ApiEndpoint(
                model_id=judge_model_id,
                url=judge_model_endpoint,
            ), ),
        ),
    ]

    config_builder = DataDesignerConfigBuilder(model_configs=model_configs)
    
    print("‚úÖ Data Designer initialized successfully for evaluation")
    return ndd, config_builder


def load_existing_data(filename='banking_agent_evaluation_data.json'):
    """Load existing data from the JSON file"""
    print(f"üìÇ Loading existing data from {filename}...")
    
    try:
        with open(filename, 'r') as f:
            data = json.load(f)
        
        print(f"‚úÖ Loaded {len(data)} records from {filename}")
        return data
    except FileNotFoundError:
        print(f"‚ùå File {filename} not found!")
        print("üí° Please run 'banking_data_generator.py' first to generate the data.")
        return None
    except Exception as e:
        print(f"‚ùå Error loading data: {e}")
        return None


def create_evaluation_dataset(ndd, data):
    """Create a dataset for evaluation from existing data"""
    print("üîß Creating evaluation dataset...")
    
    # Convert the JSON data to a pandas DataFrame for processing
    records = []
    for record in data:
        # Extract customer profile
        customer_profile = record.get('customer_profile', {})
        customer_name = customer_profile.get('name', 'Unknown')
        customer_age = customer_profile.get('age', 'Unknown')
        monthly_income = customer_profile.get('monthly_income', 'Unknown')
        account_type = customer_profile.get('account_type', 'Unknown')
        banking_experience = customer_profile.get('banking_experience', 'Unknown')
        
        # Extract query
        messages = record.get('messages', [])
        query_text = messages[0].get('content', '') if messages else ''
        
        # Extract metadata
        query_metadata = record.get('query_metadata', {})
        urgency = query_metadata.get('urgency', 'Unknown')
        complexity = query_metadata.get('complexity', 'Unknown')
        
        # Extract tools and tool calls
        tools = record.get('tools', [])
        tool_calls = record.get('tool_calls', [])
        
        records.append({
            'customer_name': customer_name,
            'customer_age': customer_age,
            'monthly_income': monthly_income,
            'account_type': account_type,
            'banking_experience': banking_experience,
            'query_text': query_text,
            'urgency': urgency,
            'complexity': complexity,
            'tools': tools,
            'tool_calls': tool_calls
        })
    
    dataset = pd.DataFrame(records)
    
    # Debug: Print the actual columns to see what we have
    print(f"Dataset columns: {dataset.columns.tolist()}")
    print(f"First record sample:")
    if len(dataset) > 0:
        print(f"  customer_name: {dataset.iloc[0]['customer_name']}")
        print(f"  query_text: {dataset.iloc[0]['query_text']}")
        print(f"  urgency: {dataset.iloc[0]['urgency']}")
    
    print(f"‚úÖ Created evaluation dataset with {len(dataset)} records")
    return dataset


def add_evaluation_columns(config_builder, dataset):
    """Add the evaluation columns to the config builder so they can be referenced"""
    print("üîß Adding evaluation columns to config builder...")
    
    # Add each column from the dataset to the config builder
    # We'll use simple sampler columns for the existing data
    
    # Customer profile columns
    config_builder.add_column(
        C.SamplerColumn(
            name="customer_name",
            type=P.SamplerType.CATEGORY,
            params=P.CategorySamplerParams(
                values=dataset['customer_name'].unique().tolist()
            ),
            description="Customer's name"
        )
    )
    
    config_builder.add_column(
        C.SamplerColumn(
            name="customer_age",
            type=P.SamplerType.CATEGORY,
            params=P.CategorySamplerParams(
                values=dataset['customer_age'].unique().tolist()
            ),
            description="Customer's age"
        )
    )
    
    config_builder.add_column(
        C.SamplerColumn(
            name="monthly_income",
            type=P.SamplerType.CATEGORY,
            params=P.CategorySamplerParams(
                values=dataset['monthly_income'].unique().tolist()
            ),
            description="Customer's monthly income"
        )
    )
    
    config_builder.add_column(
        C.SamplerColumn(
            name="account_type",
            type=P.SamplerType.CATEGORY,
            params=P.CategorySamplerParams(
                values=dataset['account_type'].unique().tolist()
            ),
            description="Type of bank account"
        )
    )
    
    config_builder.add_column(
        C.SamplerColumn(
            name="banking_experience",
            type=P.SamplerType.CATEGORY,
            params=P.CategorySamplerParams(
                values=dataset['banking_experience'].unique().tolist()
            ),
            description="Customer's banking experience level"
        )
    )
    
    # Query columns
    config_builder.add_column(
        C.SamplerColumn(
            name="query_text",
            type=P.SamplerType.CATEGORY,
            params=P.CategorySamplerParams(
                values=dataset['query_text'].unique().tolist()
            ),
            description="Customer's query text"
        )
    )
    
    config_builder.add_column(
        C.SamplerColumn(
            name="urgency",
            type=P.SamplerType.CATEGORY,
            params=P.CategorySamplerParams(
                values=dataset['urgency'].unique().tolist()
            ),
            description="Query urgency level"
        )
    )
    
    config_builder.add_column(
        C.SamplerColumn(
            name="complexity",
            type=P.SamplerType.CATEGORY,
            params=P.CategorySamplerParams(
                values=dataset['complexity'].unique().tolist()
            ),
            description="Query complexity level"
        )
    )
    
    config_builder.validate()
    print("‚úÖ Evaluation columns added to config builder successfully")


def add_quality_assessment(config_builder):
    """Add quality assessment using LLM-as-a-Judge"""
    print("üßê Adding quality assessment...")
    
    # Define quality assessment rubrics
    tool_appropriateness_rubric = P.Rubric(
        name="tool_appropriateness",
        description="The generated tools should be appropriate and necessary for handling the customer query",
        scoring={
            "4": "Tools are perfectly appropriate and cover all necessary functions",
            "3": "Tools are mostly appropriate with minor gaps",
            "2": "Some tools are appropriate but there are gaps or unnecessary tools",
            "1": "Many tools are inappropriate or missing key functions",
            "0": "Tools are completely inappropriate for the query"
        })

    tool_call_accuracy_rubric = P.Rubric(
        name="tool_call_accuracy",
        description="The tool calls should have correct arguments that match the tool parameters",
        scoring={
            "4": "All tool calls have perfectly accurate arguments",
            "3": "Most tool calls have accurate arguments with minor issues",
            "2": "Some tool calls have accurate arguments but there are errors",
            "1": "Many tool calls have incorrect or missing arguments",
            "0": "Tool calls are completely inaccurate or missing"
        })

    query_realism_rubric = P.Rubric(
        name="query_realism",
        description="The customer query should be realistic and appropriate for the customer profile",
        scoring={
            "4": "Query is perfectly realistic and matches customer profile",
            "3": "Query is mostly realistic with minor inconsistencies",
            "2": "Query is somewhat realistic but has some issues",
            "1": "Query has significant unrealistic elements",
            "0": "Query is completely unrealistic"
        })

    # Add quality assessment
    config_builder.add_column(
        C.LLMJudgeColumn(
            name="quality_assessment",
            model_alias="llama33-70b",
            prompt=(
                "Evaluate the quality of the generated banking customer interaction data:\n"
                "Customer Profile: {{ customer_name }}, Age: {{ customer_age }}, "
                "Income: {{ monthly_income }}, Account: {{ account_type }}, "
                "Experience: {{ banking_experience }}\n"
                "Customer Query: {{ query_text }} (Complexity: {{ complexity }}, Urgency: {{ urgency }})\n\n"
                "Evaluate based on the provided rubrics."
            ),
            rubrics=[tool_appropriateness_rubric, tool_call_accuracy_rubric, query_realism_rubric]
        )
    )

    config_builder.validate()
    print("‚úÖ Quality assessment added successfully")


def generate_evaluation_preview(ndd, config_builder):
    """Generate a preview of the evaluation dataset"""
    print("üëÄ Generating evaluation preview...")
    
    try:
        preview = ndd.preview(config_builder, verbose_logging=True)
        print("‚úÖ Evaluation preview generated successfully")
        print(f"Preview dataset shape: {preview.dataset.shape}")
        return preview
    except Exception as e:
        print(f"‚ùå Error generating evaluation preview: {e}")
        return None


def run_full_evaluation(ndd, config_builder, num_records=5):
    """Run the full evaluation on the dataset"""
    print(f"üß¨ Running full evaluation on {num_records} records...")
    
    try:
        results = ndd.create(config_builder, num_records=num_records, wait_until_done=True)
        dataset = results.load_dataset()
        
        print(f"‚úÖ Evaluation completed successfully on {len(dataset)} records")
        print(f"Dataset columns: {dataset.columns.tolist()}")
        
        return dataset, results
    except Exception as e:
        print(f"‚ùå Error running evaluation: {e}")
        return None, None


def extract_quality_scores(dataset):
    """Extract quality scores from the evaluation results"""
    print("üîç Extracting quality scores...")
    
    def extract_json_field(json_str, field_name):
        try:
            if isinstance(json_str, str):
                # Handle Python dict strings (single quotes) by converting to valid JSON
                if json_str.startswith("{") and json_str.endswith("}"):
                    # Convert Python dict string to valid JSON
                    import ast
                    data = ast.literal_eval(json_str)
                else:
                    data = json.loads(json_str)
            else:
                data = json_str
            return data.get(field_name, None)
        except Exception as e:
            print(f"  Parse Error for field '{field_name}': {e}")
            return None

    def extract_score(score_dict):
        """Extract numeric score from score dictionary"""
        if isinstance(score_dict, dict) and 'score' in score_dict:
            try:
                return int(score_dict['score'])
            except (ValueError, TypeError):
                return None
        return None
    
    # Extract quality scores
    dataset['tool_appropriateness_score'] = dataset['quality_assessment'].apply(
        lambda x: extract_score(extract_json_field(x, 'tool_appropriateness')) if x else None
    )
    dataset['tool_call_accuracy_score'] = dataset['quality_assessment'].apply(
        lambda x: extract_score(extract_json_field(x, 'tool_call_accuracy')) if x else None
    )
    dataset['query_realism_score'] = dataset['quality_assessment'].apply(
        lambda x: extract_score(extract_json_field(x, 'query_realism')) if x else None
    )
    
    # Calculate overall quality score
    def calculate_overall_score(row):
        scores = []
        if row['tool_appropriateness_score']:
            scores.append(row['tool_appropriateness_score'])
        if row['tool_call_accuracy_score']:
            scores.append(row['tool_call_accuracy_score'])
        if row['query_realism_score']:
            scores.append(row['query_realism_score'])
        
        if scores:
            return sum(scores) / len(scores)
        return None

    dataset['overall_quality_score'] = dataset.apply(calculate_overall_score, axis=1)
    
    print("‚úÖ Quality scores extracted successfully")
    return dataset


def analyze_evaluation_results(dataset):
    """Analyze the evaluation results"""
    print("üìä Analyzing evaluation results...")
    
    print("=== Evaluation Results Analysis ===")

    # Quality score distribution
    print(f"\nOverall Quality Score Distribution:")
    print(f"Average: {dataset['overall_quality_score'].mean():.2f}")
    print(f"Median: {dataset['overall_quality_score'].median():.2f}")
    print(f"Min: {dataset['overall_quality_score'].min():.2f}")
    print(f"Max: {dataset['overall_quality_score'].max():.2f}")

    # Individual metric scores
    print(f"\nTool Appropriateness Scores:")
    print(f"Average: {dataset['tool_appropriateness_score'].mean():.2f}")
    print(f"Distribution: {dataset['tool_appropriateness_score'].value_counts().sort_index().to_dict()}")

    print(f"\nTool Call Accuracy Scores:")
    print(f"Average: {dataset['tool_call_accuracy_score'].mean():.2f}")
    print(f"Distribution: {dataset['tool_call_accuracy_score'].value_counts().sort_index().to_dict()}")

    print(f"\nQuery Realism Scores:")
    print(f"Average: {dataset['query_realism_score'].mean():.2f}")
    print(f"Distribution: {dataset['query_realism_score'].value_counts().sort_index().to_dict()}")

    # High-quality examples (score >= 3.0)
    high_quality = dataset[dataset['overall_quality_score'] >= 3.0]
    print(f"\nHigh Quality Examples (Score >= 3.0): {len(high_quality)}/{len(dataset)} ({len(high_quality)/len(dataset)*100:.1f}%)")

    if len(high_quality) > 0:
        print("\nSample High-Quality Record:")
        sample = high_quality.iloc[0]
        print(f"Customer: {sample.get('customer_name', '')}")
        print(f"Query: {sample.get('query_text', '')}")
        print(f"Quality Score: {sample.get('overall_quality_score', '')}")

    # Low-quality examples (score < 2.0)
    low_quality = dataset[dataset['overall_quality_score'] < 2.0]
    if len(low_quality) > 0:
        print(f"\nLow Quality Examples (Score < 2.0): {len(low_quality)}/{len(dataset)} ({len(low_quality)/len(dataset)*100:.1f}%)")
        print("\nSample Low-Quality Record:")
        sample = low_quality.iloc[0]
        print(f"Customer: {sample.get('customer_name', '')}")
        print(f"Query: {sample.get('query_text', '')}")
        print(f"Quality Score: {sample.get('overall_quality_score', '')}")


def export_evaluation_results(dataset, filename='banking_evaluation_results.json'):
    """Export the evaluation results to JSON format"""
    print(f"üíæ Exporting evaluation results to {filename}...")
    
    export_data = []
    
    for _, row in dataset.iterrows():
        export_record = {
            "customer_profile": {
                "name": row.get('customer_name', ''),
                "age": row.get('customer_age', ''),
                "monthly_income": row.get('monthly_income', ''),
                "account_type": row.get('account_type', ''),
                "banking_experience": row.get('banking_experience', '')
            },
            "query": {
                "text": row.get('query_text', ''),
                "urgency": row.get('urgency', ''),
                "complexity": row.get('complexity', '')
            },
            "quality_scores": {
                "tool_appropriateness": row.get('tool_appropriateness_score', ''),
                "tool_call_accuracy": row.get('tool_call_accuracy_score', ''),
                "query_realism": row.get('query_realism_score', ''),
                "overall": row.get('overall_quality_score', '')
            },
            "raw_quality_assessment": row.get('quality_assessment', '')
        }
        
        export_data.append(export_record)

    # Save to JSON file
    with open(filename, 'w') as f:
        json.dump(export_data, f, indent=2)

    print(f"‚úÖ Exported {len(export_data)} evaluation records to '{filename}'")
    return export_data


def display_evaluation_summary(dataset, export_data):
    """Display a summary of the evaluation results"""
    print("\nüìã Evaluation Summary:")
    print("=" * 50)
    
    print(f"Total Records Evaluated: {len(dataset)}")
    print(f"Average Overall Quality Score: {dataset['overall_quality_score'].mean():.2f}/4.0")
    
    # Score breakdown
    print(f"\nScore Breakdown:")
    print(f"  Tool Appropriateness: {dataset['tool_appropriateness_score'].mean():.2f}/4.0")
    print(f"  Tool Call Accuracy: {dataset['tool_call_accuracy_score'].mean():.2f}/4.0")
    print(f"  Query Realism: {dataset['query_realism_score'].mean():.2f}/4.0")
    
    # Quality distribution
    high_quality = dataset[dataset['overall_quality_score'] >= 3.0]
    medium_quality = dataset[(dataset['overall_quality_score'] >= 2.0) & (dataset['overall_quality_score'] < 3.0)]
    low_quality = dataset[dataset['overall_quality_score'] < 2.0]
    
    print(f"\nQuality Distribution:")
    print(f"  High Quality (‚â•3.0): {len(high_quality)} ({len(high_quality)/len(dataset)*100:.1f}%)")
    print(f"  Medium Quality (2.0-2.9): {len(medium_quality)} ({len(medium_quality)/len(dataset)*100:.1f}%)")
    print(f"  Low Quality (<2.0): {len(low_quality)} ({len(low_quality)/len(dataset)*100:.1f}%)")
    
    print(f"\nüìÅ Generated Files:")
    print(f"  - banking_evaluation_results.json")
    print(f"  - {len(export_data)} evaluation records")


def main():
    """Main function to run the banking data evaluator"""
    print("üè¶ Banking Data Evaluator")
    print("=" * 40)
    print("üîç This script evaluates data quality ONLY")
    print("üìù Use 'banking_data_generator.py' to generate data first")
    print("=" * 40)
    
    try:
        # Step 1: Load existing data
        data = load_existing_data()
        if data is None:
            return
        
        # Step 2: Initialize Data Designer for evaluation
        ndd, config_builder = initialize_data_designer()
        
        # Step 3: Create evaluation dataset
        dataset = create_evaluation_dataset(ndd, data)
        
        # Step 4: Add the columns to the config builder so they can be referenced
        print("üîß Adding columns to config builder...")
        add_evaluation_columns(config_builder, dataset)
        
        # Step 5: Add quality assessment
        add_quality_assessment(config_builder)
        
        # Step 6: Generate evaluation preview
        preview = generate_evaluation_preview(ndd, config_builder)
        if preview is None:
            print("‚ùå Failed to generate evaluation preview. Exiting.")
            return
        
        # Step 7: Run full evaluation
        evaluation_dataset, results = run_full_evaluation(ndd, config_builder, num_records=len(data))
        if evaluation_dataset is None:
            print("‚ùå Failed to run evaluation. Exiting.")
            return
        
        # Step 8: Extract quality scores
        evaluation_dataset = extract_quality_scores(evaluation_dataset)
        
        # Step 9: Analyze results
        analyze_evaluation_results(evaluation_dataset)
        
        # Step 10: Export results
        export_data = export_evaluation_results(evaluation_dataset)
        
        # Step 11: Display summary
        display_evaluation_summary(evaluation_dataset, export_data)
        
        print("\nüéâ Banking Data Evaluator completed successfully!")
        print("\nüîÑ Next Steps:")
        print("1. Review the evaluation results in 'banking_evaluation_results.json'")
        print("2. Identify areas for improvement in data generation")
        print("3. Use high-quality data for testing your banking AI agent")
        print("4. Iterate on data generation based on evaluation feedback")
        
    except Exception as e:
        print(f"‚ùå Error in main execution: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
